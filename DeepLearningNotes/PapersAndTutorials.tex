% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% P  R  E  A  M  B  L  E
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
\documentclass[11pt]{article}
\usepackage{amsbsy, amsmath, amssymb, authblk}

%\usepackage{array} 
%\usepackage{algorithm2e}
\usepackage{booktabs, bm}
\usepackage[small,labelfont=bf,up,singlelinecheck=false]{caption}
\usepackage{cancel}
\usepackage{comment}
% \usepackage[shortlabels]{enumitem}
%\usepackage{fancyhdr}
%\usepackage[default]{lato}
\usepackage[T1]{fontenc}
\usepackage[bottom]{footmisc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage[utf8]{inputenc}
%	\inputencoding{latin1}
%	\inputencoding{utf8}
%\usepackage{lettrine}
%\usepackage[sc]{mathpazo}
\usepackage{lmodern} % Nice fonts?
%\usepackage{mathrsfs}
\usepackage{mathtools} 
%\usepackage{marvosym} % silly bullet-point symbols (misc symbols)
%\usepackage{microtype}
\usepackage{minitoc}         % left in case it is needed elsewhere
\setcounter{secttocdepth}{5} % idem
\usepackage{etoc} % for toc before each section.
%\usepackage{multicol}
\usepackage{needspace}
\usepackage{paralist}
\usepackage{pifont}
%\usepackage{polynom} 			% typesetting polynomial long division
%\usepackage{setspace}
%	\onehalfspacing 
\usepackage{stmaryrd}  % \llbracket, \rrbracket
\usepackage{tocloft}
\usepackage{xparse} % DeclareDocumentCommand
\usepackage[compact]{titlesec} 		% compact shrinks whitespace around section headings.
\usepackage{ulem} 				% for strikeout \sout command.
%\usepackage{verbatim}

% Muh packagez :)
\usepackage{../Packages/MathCommands}
\usepackage{../Packages/BrandonColors}
\usepackage{../Packages/BrandonBoxes}
\usepackage{../Packages/NoteTaker}

%\usepackage{showframe} % Displays layout frame on all pages
\usepackage{marginnote}
\renewcommand*{\marginfont}{\scriptsize}
\usepackage{listings}

\usepackage{tikz}
\usetikzlibrary{arrows, automata, shapes, snakes, positioning}
\usetikzlibrary{bayesnet}

% hi?
 \renewcommand\dotseq[2]
 {#1^{(1)}, \ldots, #1^{(#2)}}
 \renewcommand\rdotseq[2]
 {#1^{(#2)}, \ldots, #1^{(1)}} % reversed


%\setlength{\parskip}{1pt}
%\setlength{\columnseprule}{0.1pt}
%\setlength{\columnsep}{0.6cm}
%\setlength\tabcolsep{0.1cm}
\renewcommand{\arraystretch}{1.2}

\DeclareDocumentEnvironment{definition}{O{-0.5em} o}{
	\IfNoValueTF{#2}{}{\textbf{#2}}
	\vspace*{#1}
	\begin{quote}
		\itshape\small}
	{\end{quote}}

% author, title, affiliation, date.
\DeclareDocumentCommand{\citepaper}{ m m m m }{
	\vspace{-1em}
	{\footnotesize #1, ``#2'' \textit{#3}, (#4).}
}

% Title
\title{\vspace{-10mm}\fontsize{24pt}{8pt}\selectfont\textbf{Fall 2016 Course Notes}\vspace*{-4mm}}
% Author
\author{Brandon McKinzie}
% Date
\date{}

\begin{document}
\dosecttoc
\tableofcontents

% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Neural Machine Translation}\label{Neural Machine Translation}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------


% ============================================================================================
\lecture{Neural Machine Translation}{Neural Conversation Model}{February 8, 2017}
\vspace{-1em}

\p \blue{Abstract}. This paper presents a simple approach for conversational modeling which uses the sequence to sequence framework. It can be trained end-to-end, meaning fewer hand-crafted rules. The \red{lack of consistency} is a common failure of our model. 

\myspace
\p \blue{Introduction}. Major advantage of the seq2seq model is it requires little feature engineering and domain specificity. Here, the model is tested on chat sessions from an IT helpdesk dataset of conversations, as well as movie subtitles.

\myspace
\p \blue{Related Work}. The authors' approach is based on the following (linked and saved) papers on seq2seq:
\begin{compactitem}
	\item \href{https://arxiv.org/pdf/1306.3584.pdf}{Kalchbrenner \& Blunsom, 2013.}
	\item \href{https://arxiv.org/pdf/1409.3215.pdf}{Sutskever et al., 2014.} (Describes Seq2Seq model)
	\item \href{https://arxiv.org/pdf/1409.0473.pdf}{Bahdanau et al., 2014.}
\end{compactitem}

\myspace
\p \blue{Model}. Succinctly described by the authors:\vspace{-1em}
\begin{quote}
	The model reads the input sequence one token at a time, and predicts the output sequence, also one token at a time. During training, the true output sequence is given to the model, so learning can be done by backpropagation. The model is trained to maximize the cross entropy of the correct sequence given
	its context. During inference, in which the true output sequence is not observed, we simply feed the predicted output token as input to predict the next output. This is a “greedy” inference approach. 
\end{quote}
\marginnote{Example of less greedy approach: \red{beam search}.}[-6em]
\myfig[0.8\textwidth]{Seq2SeqModel.PNG}

\myspace
\p The \green{thought vector} is the hidden state of the model when it receives [as input] the end of sequence symbol $\langle eos \rangle$, because it stores the info of the sentence, or \textit{thought}, ``ABC''. The authors acknowledge that this model will \textit{not} be able to ``solve'' the problem of modeling dialogue due to the objective function not capturing the actual objective achieved through human communication, which is typically longer term and based on exchange of information [rather than next step prediction]\marginnote{Ponder: what \textit{would} be a reasonable objective function \& model for conversation?}[-2em]\footnote{I'd imagine that, in order to model human conversation, one obvious element needed would be a \textit{memory}. Reminds me of DeepMind's DNC. There would need to be some online filtering \& output process to capture the crucial aspects/info to store in memory for later, and also some method of retrieving them when needed later. The method for retrieval would likely be some inference process where, given a sequence of inputs, the probability of them being related to some portion of memory could be trained. This would allow for conversations that stretch arbitrarily back in the past. Also, when storing the memories, I'd imagine a reasonable architecture would be some encoder-decoder for a sparse distributed representation of memory.}.

\myspace
\p \blue{IT Data \& Experiment}.\marginnote{Reminder: Check out \href{https://github.com/farizrahman4u/seq2seq}{this git repo}}
\begin{compactitem}
	\item \textbf{Data Description}: Customers talking to IT support, where typical interactions are 400 words long and turn-taking is clearly signaled. 
	
	\item \textbf{Training Data}: 30M tokens, 3M of which are used as validation. They built a vocabulary of the most common 20K words, and introduced special tokens indicating turn-taking and actor.
	
	\item \textbf{Model}: A single-layer LSTM with 1024 memory cells. 
	
	\item \textbf{Optimization}: SGD with gradient clipping.
	
	\item \textbf{Perplexity}: At convergence, achieved \red{perplexity} of 8, whereas an n-gram model achieved 18.
\end{compactitem}


% ========================================================================================
% ========================================================================================
\lecture{Neural Machine Translation}{NMT By Jointly Learning to Align \& Translate}{February 27, 2017}
% ========================================================================================
% ========================================================================================

\href{https://arxiv.org/abs/1409.0473}{[Bahdanau et. al, 2014]}. The primary motivation for me writing this is to better understand the \green{attention mechanism} in my sequence to sequence chatbot implementation.

\myspace
\p \blue{Abstract}. The authors claim that using a fixed-length vector [in the vanilla encoder-decoder for NMT] is a bottleneck. They propose allowing a model to (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.

\myspace
\p \blue{Learning to Align\footnote{By ``align'' the authors are referring to aligning the source-search to the relevant parts for prediction.} and translate}. 
\begin{itemize}
	\item \textbf{Decoder}. Their encoder defines the conditional output distribution as
	\begin{align}
	p(y_i \mid y_1, \ldots, y_{i - 1}, \vec{x}) &= g(y_{i - 1}, s_i, c_i) \\
	s_i &= f(s_{i - 1}, y_{i - 1}, c_i)
	\end{align}
	where $s_i$ is the RNN [decoder] hidden state at time $i$. 
	\begin{compactitem}
		\item NOTE: $c_i$ is \textit{not} the $i$th element of the standard context vector; rather, it is \textit{itself} a distinct context vector that depends on a sequence of \green{annotations} $(h_1, \ldots, h_{T_x})$. It seems that each annotation $h_i$ is a hidden (encoder) state ``that contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence.''
		
		\item The context vector $c_i$ is computed as follows:
		\begin{align}
		c_i &= \sum_{j = 1}^{T_x} \alpha_{ij} h_j \\
		\alpha_{ij} &= \frac{\exp(e_{ij})}{\sum_{k = 1}^{T_x} \exp(e_{ik})} \\
		e_{ij} &= a(s_{i - 1}, h_j) 
		\end{align}
		where the function $e_{ij}$ is given by an \green{alignment model} which scores how well the inputs around position $j$ and the output at position $i$ match. 
	\end{compactitem}
	
	
	\item \textbf{Encoder}. It's just a bidirectional RNN. What they call ``annotation $h_j$'' is literally just a concatenated vector of $h_j^{forward}$ and $h_j^{backward}$
\end{itemize}


\myspace
\subsub{Detailed Model Architecture}

(Appendix A). Explained with the TensorFlow user in mind.\\

\myspace
\p \blue{Decoder Internals}. It's just a GRU. However, it will be helpful to detail how we format the inputs (given we now have attention). Wherever we'd usually pass the previous decoder state $s_{i - 1}$, we now pass a \textit{concatenated} state, $[s_{i - 1},~ c_i]$, that also contains the $i$th context vector. Below I go over the flow of information from GRU input to output:
\begin{compactenum}
	\item \textbf{Notation}: $y_t$ is the loop-embedded \underline{output} of the decoder (prediction) at time $t$, $s_t$ is the internal hidden state of the decoder at time $t$, and $c_t$ is the context vector at time $t$. $\tilde s_t$ is the proposed/proposal state at time $t$.
	\item \textbf{Gates}:
	\begin{align}
	z_t &= \sigma\left( W_z y_{t - 1} + U_z [s_{t - 1},~ c_t] \right)
	\qquad \mgreen{\text{[update gate]}} \\
	r_t &= \sigma\left( W_r y_{t - 1} + U_r [s_{t - 1},~ c_t] \right)
	\qquad \mgreen{\text{[reset gate]}} \\
	\end{align}
	
	\item \textbf{Proposal state}:
	\begin{align}
	\tilde s_t &= \tanh\left( W y_{t - 1} + U [r_t \circ s_{t - 1}, ~ c_t]  \right)
	\end{align}
	
	\item \textbf{Hidden state}:
	\begin{align}
	s_t &= (1 - z_t) \circ s_{t - 1} + z_t \circ \tilde s_t
	\end{align}
	
\end{compactenum}

\myspace 
\p \blue{Alignment Model}. All equations enumerated below are for some timestep $t$ during the decoding process.
\begin{compactenum}
	\item \textbf{Score}: For all $j \in [0, L_{enc}-1]$ where $L_{enc}$ is the number of words in the encoder sequence, compute:
	\begin{align}
	a_j = a(s_{t - 1}, h_j) &= v_a^T \tanh\left( W_a s_{t - 1} + U_a h_j \right)
	\end{align}
	
	\item \textbf{Alignments}: Feed the unnormalized alignments (scores) through a softmax so they represent a valid probability distribution.
	\begin{align}
	a_j \leftarrow \frac{e^{a_j}}{\sum_{k = 0}^{L_{enc}-1} e^{a_k}}
	\end{align}
	
	\item \textbf{Context}: The context vector input for our decoder at this timestep:
	\begin{align}
	c = \sum_{j = 1}^{L_{enc}} a_j h_j
	\end{align}
\end{compactenum}

\myspace
\p \blue{Decoder Outputs}. All below is for some timestep $t$ during the decoding process. To find the probability of some (one-hot) word $y$ [at timestep $t$]:
\begin{align}
\Pr\left(y \mid s, c \right) &\propto e^{y^T W_o u} \\
u &= \left[\max\{ \tilde u_{2j - 1}, \tilde u_{2j} \}\right]_{j = 1,\ldots, \ell}^T \\
\tilde u &= U_o [s_{t - 1}, ~ c] + V_o y_{t - 1}
\end{align}
\red{N.B.}: From reading other (and more recent) papers, these last few equations do not appear to be the way it is usually done (thank the lord). See Luong's work for a much better approach.





% ==============================================================================
% ==============================================================================
\lecture{Neural Machine Translation}{Using Large Vocabularies for NMT}{March 11, 2017}
% ==============================================================================
% ==============================================================================

\p Paper information:
\begin{compactitem}[-]
	\item Full title: On Using Very Large Target Vocabulary for Neural Machine Translation. 
	\item Authors: Jean, Cho, Memisevic, Bengio.
	\item Date: 18 Mar 2015. 
	\item \href{https://arxiv.org/abs/1412.2007}{[arXiv link]}
\end{compactitem}


\myspace
\p \blue{NMT Overview}. Typical implementation is encoder-decoder network. Notation for inputs \& encoder:
\begin{align}
x &= (x_1, \ldots, x_T)
\qquad \mgreen{\text{[source sentence]}} \\
h &= (h_1, \ldots, h_T)
\qquad \mgreen{\text{[encoder state seq]}} \\
h_t &= f(x_t, h_{t - 1}) \label{h}
\end{align}
where $f$ is the function defined by the \textit{cell state} (e.g. GRU/LSTM/etc.). Then the decoder generates the output sequence $y$, and with probability given below:\marginnote{The functions $q$, $g$, and $r$ are just placeholders -- ``some function of [inputs].''}[2em]
\begin{align}
y &= (y_1, \ldots, y_T') \qquad \mgreen{[y_i \in \mathbb{Z}]} \\
\Pr[y_t \mid y_{<t}, ~ x] &\propto e^{q(y_{t-1},~ z_t, ~ c_t)}  \label{y}\\
z_t &= g(y_{t - 1}, z_{t - 1}, c_t) 
\qquad \mred{\text{[decoder hidden?]}} \label{z} \\
c_t &= r(z_{t - 1}, h_1, \ldots, h_T) 
\qquad \mred{\text{[decoder inp?]}} \label{c}
\end{align}
As usual, model is jointly trained to maximize the conditional log-likelihood of correct translation. For $N$ training sample pairs $(x^n, y^n)$, and denoting the length of the $n$-th target sentence as $T_n$, this can be written as,
\begin{align}
\theta^* &= \argmax_\theta \sum_{n = 1}^{N} \sum_{t = 1}^{T_n}
\log\left(\Pr[y_t^n \mid y_{<t}^n, ~ x^n]  \right)
\end{align}

\myspace
\p \blue{Model Details}. Above is the general structure. Here I'll summarize the specific model chosen by the authors. 
\begin{compactitem}
	\item \textbf{Encoder}. Bi-directional, which just means $h_t = \left[h_t^{backward};~h_t^{forward} \right]$. The chosen cell state (the function $f$) is GRU.
	
	\item \textbf{Decoder}. At each timestep, computes the following:
	\begin{compactitem}[$\rightarrow$]
		\item \green{Context vector $c_t$}. \marginnote{$a$ is a standard single-hidden-layer NN.}[7em]
		\begin{align}
		c_t &= \sum_{i = 1}^{T} \alpha_i h_i \\
		\alpha_t &= \dfrac{e^{a(h_t, z_{t - 1})}}{\sum_k e^{a(h_k, z_{t - 1}})} 
		\end{align}
		\item \green{Decoder hidden state $z_t$}. Also a GRU cell. Computed based on the previous hidden state $z_{t - 1}$, the previously generated symbol $y_{t - 1}$, and also the computed context vector $c_t$. 
	\end{compactitem}
	
	\item \textbf{Next-word probability}. They model equation ~\ref{y} as\footnote{Note: The formula for $Z$ is correct. Notice that the only part of the RHS of Pr($y_t$) with a $t$ is as the subscript of $w$. To be clear, $w_k$ is a full word vector and the sum is over all words in the output \textit{vocabulary}, the index $k$ has absolutely nothing to do with timestep. They use the word target but make sure not to misinterpret that as somehow meaning target words in the sentence or something.} ,\marginnote{Reminder: $y_i$ is an integer token, while $\rvec[i]{w}$ is the target vector of length vocab size}
	\begin{align}
	\Pr[y_t \mid y_{<t}, ~ x]  &= \frac{1}{Z} e^{\rvec[t]{w}^T \phi(y_{t - 1}, z_t, c_t) + b_t} \\
	Z &= \sum_{k:~y_k\in V}    e^{\rvec[k]{w}^T \phi(y_{t - 1}, z_t, c_t) + b_k}
	\end{align}
	where $\phi$ is affine transformation followed by a nonlinear activation, $\rvec[t]{w}$ and $b_t$ are the \green{target word vector} and bias. $V$ is the set of all target \textit{vocabulary}. 
\end{compactitem}

\myspace
\p \blue{Approximate Learning Approach}. Main idea:
\vspace{-1em}
\begin{center}
	\begin{quote}
		{\footnotesize ``In order to avoid the growing complexity of computing the normalization constant, we propose here to use only a small subset $V'$ of the target vocabulary at each update.''}
	\end{quote}
\end{center}
Consider the gradient of the log-likelihood\footnote{\red{NOTE TO SELF}: After long and careful consideration, I'm concluding that the authors made a typo when defining $\mathcal{E}(y_j)$, which they choose to subscript all parts of the RHS with $j$, but that is in direct contradiction with a step-by-step derivation, which is why I have written it the way it is. I'm pretty sure my version is right, but I know you'll have to re-derive it yourself next time you see this. And you'll somehow prove me wrong. Actually, after reading on further, I doubt you'll prove me wrong. Challenge accepted, me. Have fun!}, written in terms of the energy $\mathcal{E}$. 
\begin{align}
\nabla \log\left(\Pr[y_t \mid y_{<t}, ~ x]  \right)
&= \nabla \mathcal{E}(y_t) 
- \sum_{k:~y_k\in V}\Pr[y_k \mid y_{<t}, ~ x]\nabla\mathcal{E}(y_k)\\
\mathcal{E}(y_j) &= \rvec[j]{w}^T \phi(y_{t - 1}, z_t, c_t) + b_j
\end{align}


\myspace
\p The crux of the approach is interpreting the second term as $\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right]$, where $P$ denotes $Pr(y \mid y_{<t}, x)$. They approximate this expectation by taking it over a \underline{subset} $V'$ of the predefined proposal \underline{distribution} $Q$. So $Q$ is a p.d.f. over the possible $y_i$, and we sample \textit{from} $Q$ to generate the elements of the subset $V'$. 
\graybox{
	\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right] 
	&\approx \sum_{k:~y_k\in V'} \dfrac{\omega_k}{
		\sum_{k':y_{k'}\in V'} \omega_{k'} } \nabla\mathcal{E}(y_k)\\
	\omega_k &= e^{\mathcal{E}(y_k) - \log Q(y_k)}
}

Here is some math I did that was illuminating to me; I'm not sure why the authors didn't point out these relationships. 
\begin{align}
&\omega_k = \frac{e^{\mathcal{E}(y_k)}}{Q(y_k)} 
\quad\text{thus}\quad
p(y_k \mid y_{<t}, x) = \omega_k \frac{Q(y_k)}{Z} \\
\rightarrow~~  &e^{\mathcal{E}(y_k)} = Z \cdot p(y_k \mid y_{<t}, x) 
= Q(y_k) \cdot \omega_k
\end{align}


\redbox[Now \textit{check this out}]{
	Below are the exact and approximate formulas for $\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right]$ written in a \sout{seductive} suggestive manner. Pay careful attention to subscripts and primes.

	\begin{align}
	\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right] 
	&=  \sum_{k:~y_k\in V} \dfrac{ \omega_k \cdot Q(y_k) }{ 
		\sum_{k':y_{k'}\in V} \omega_{k'} \cdot Q(y_{k'}) }  
	\nabla\mathcal{E}(y_k)\\
	\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right] 
	&=  \sum_{k:~y_k\in V'} \dfrac{ \omega_k }{  \sum_{k':y_{k'}\in V'} \omega_{k'} }  \nabla\mathcal{E}(y_k)
	\end{align}
	They're almost the same! It's much easier to see why when written this way. I interpret the difference as follows: in the exact case, we explicitly attach the probabilities $Q(y_k)$ and sum over all values in $V$. In the second case, by sampling a subset $V'$ from $Q$, we have encoded these probabilities implicitly as the relative frequency of elements $y_k$ in $V'$
}


\myspace
\p \blue{How to do in practice (very important)}. 
\begin{center}
	\begin{quote}
		``In practice, we partition the training corpus and define a subset $V'$ of the target vocabulary for each partition prior to training. Before training begins, we sequentially examine each target sentence in the training corpus and accumulate
		unique target words until the number of unique target words reaches the predefined threshold $\tau$. The accumulated vocabulary will be used for this partition of the corpus during training. We repeat this until the end of the training set is reached. Let us refer to the subset of target words used for the i-th partition by $V'_i$.
	\end{quote}
\end{center}







% ============================================================================================
% ============================================================================================
\lecture{Neural Machine Translation}{Candidate Sampling -- TensorFlow}{March 19, 2017}
% ============================================================================================
% ============================================================================================

{\scriptsize \href{https://www.tensorflow.org/extras/candidate_sampling.pdf}{[Link to article]}}

\myspace
\blue{What is Candidate Sampling} The goal is to learn a compatibility function $F(x, y)$ which says something about the compatibility of a class $y$ with a context $x$. Candidate sampling: for each training example $(x_i, y_i)$, only need to evaluate $F(x, y)$ for a small set of classes $\{C_i\} \subset \{L\}$, where $\{L\}$ is the set of all possible classes (vocab size number of elements). We represent $F(x, y)$ as a \textit{layer that is trained by back-prop from/within the loss function}. 

\myspace
\p \blue{C.S. for Sampled Softmax}. I'll further narrow this down to my use case of having exactly 1 target class (word) at a given time. Any other classes are referred to as \green{negative} classes (for that example). 

\myspace
\p \blue{Sampling algorithm}. For each training example $(x_i, y_i)$, do:
\begin{compactitem}
	\item Sample the subset $S_i \subset L$. How? By sampling from $Q(y|x)$ which gives the probability of any particular $y$ being included in $S_i$. 
	
	\item Create the set of \green{candidates}, which is just $C_i := S_i \cup y_i$. 
\end{compactitem}

\myspace
\p \blue{Training task}. We are given this set $C_i$ and want to find out which element of $C_i$ is the target class $y_i$. In other words, we want the posterior probability that any of the $y$ in $C_i$ are the target class, given what we know about $C_i$ and $x_i$. We can evaluate and rearrange as usual with Bayes' rule to get:
\graybox{
	\Pr\left(y^{true}_i = y \mid C_i, ~ x_i \right) 
	&= 
	\dfrac{\Pr\left(y^{true}_i = y \mid x_i \right) 
		\cdot \Pr\left( C_i \mid y^{true}_i = y, ~ x_i \right)
	}{
		\Pr\left(C_i \mid x_i\right) 
	} \\
	%
	&= 
	\frac{\Pr\left(y\mid x_i\right)}{Q\left(y\mid x_i\right)} 
	\cdot 
	\frac{1}{K(x_i, C_i)}  \label{candform}
}
where they've just defined 
\begin{align}
K(x_i, C_i) \triangleq \frac{ \Pr\left(C_i \mid x_i \right) }{
	\prod_{y'\in C_i} Q(y' \mid x_i)
	\prod_{y'\in(L - C_i)} \left( 1 - Q(y' \mid x_i)\right)
}
\end{align}

\myspace
\p \blue{Clarifications}. 
\begin{itemize}
	\item The learning function $F(x, y)$ is the \textit{input} to our softmax. \underline{It is our neural network}, excluding the softmax function. 
	
	\item After training our network, it should have learned the general form
	\begin{align}
	F(x, y) = \log( \Pr( y \mid x) )  + 
	K(x) \label{Fxy}
	\end{align}
	which is the general form because
	\begin{align}
	\text{Softmax}(\log(\Pr(y \mid x)) + K(x)) 
	&= 
	\frac{ e^{\log(\Pr(y \mid x)) + K(x)}   }{
		\sum_{y'} e^{\log(\Pr(y' \mid x) + K(x) }} \\
	&= \Pr(y \mid x)	
	\end{align}
	Note that I've been a little sloppy here, since $\Pr(y \mid x)$ up until the last line actually represented the (possibly) unnormalized/\textit{relative} probabilities. 
	
	\item {\footnotesize \red{[MAIN TAKEAWAY]}}. Time to bring it all together. Notice that we've only trained $F(x, y)$ to include \textit{part} of what's needed to compute the probability of any $y$ being the target given $x_i$ \underline{and $C_i$} \textellipsis equation ~\ref{Fxy} doesn't take into account $C_i$ at all! Luckily we know the form of the full equation because it just the log of equation ~\ref{candform}. We can easily satisfy that by subtracting $\log(Q(y\mid x))$ from $F(x, y)$ right before feeding into the softmax. \\
	
	\p\textbf{TL;DR}. Train network to learn $F(x, y)$ before softmax, but instead of feeding $F(x, y)$ to softmax directly, feed 
	\graybox{
		\text{Softmax Input: } F(x, y) - \log(Q(y\mid x))
	}
	instead. That's it.
\end{itemize}




% ==========================================================================
\lecture{Neural Machine Translation}{Attention Terminology}{April 04, 2017}
%===========================================================================

Generally useful info. Seems like there are a few notations floating around, and here I'll attempt to set the record straight. The order of notes here will loosely correspond with the order that they're encountered going from encoder output to decoder output.\\

\myspace
\p \blue{Jargon}. The people in the attention business \textit{love} obscure names for things that don't need names at all. Terminology:
\begin{compactitem}
	\item \textbf{Attentions keys/values}: Encoder output sequence.
	\item \textbf{Query}: Decoder [cell] state. Typically the most recent one.
	\item \textbf{Scores}: Values of $e_{ij}$. For the Bahdanau version, in code this would be computed via
	\begin{align}
	e_i &= v^T \tanh(\text{FC}(s_{i - 1}) + \text{FC}(h))
	\end{align}
	where we'd have FC be tf.layers.fully\_connected with num\_outputs equal to our attention size (up to us). Note that $v$ is a vector.
	\item \textbf{Alignments}: output of the softmax layer on the attention scores. 
	\item \textbf{Memory}: The $\alpha$ matrix in the equation $c_i = \sum_{j = 1}^{T_x} \alpha_{ij} h_j$.
\end{compactitem}

\myspace
\p When someone lazily calls some layer output the ``attention'', they are usually referring to the layer \textit{just after} the linear combination/map of encoder hidden states. You'll often see this as some vague function of the previous decoder state, context vector, and possibly even decoder output (after project), like $f(s_{i - 1}, y_{i - 1}, c_i)$. In 99.9\% of cases, this function is just a fully connected layer (if even needed) to map back to the state size for decoder input. That is it.


\p \blue{From encoder to decoder}. The path of information flow from encoder outputs to decoder inputs, a non-trivial process that isn't given the \textit{attention} (heh) it deserves\footnote{For some reason, the literature favors explaining the path ``backwards'', starting with the highly abstracted ``decoder inputs as a weighted sum of encoder states'' and then breaking down what the weights are. Unfortunately, the weights are computed via a multi-stage process so that becomes very confusing very quick.}
\begin{enumerate}
	\item \textbf{Encoder outputs}. Tensor of shape \purple{[batch size, sequence length, state size]}. The state is typically some RNNCell state.
	\begin{compactitem}
		\item Note: TensorFlow's AttenntionMechanism classes will actually convert this to \purple{[batch size, $L_{enc}$, attention size]}, and refer to it as the ``memory''. It is also what is returned when calling myAttentionMech.values. 
	\end{compactitem}
	
	
	\item \textbf{Compute the scores}. The attention scores are the computation described by Luong/Bahdanau techniques. They both take an inner product of sorts on \textit{copies} of the encoder outputs and decoder previous state (query). The main choices are:
	\begin{align}
	\text{score}(\vec[t]{h}, \vec[s]{\bar h}) 
	&= \begin{cases}
	\vec[t]{h}^T \vec[s]{\bar h} & \mgreen{dot} \\
	\vec[t]{h}^T \matr[a]{W} \vec[s]{\bar h} & \mgreen{general} \\
	\vec[a]{v}^T \tanh\left(\matr[a]{W} [\vec[t]{h}; ~ \vec[s]{\bar h}] \right) & \mgreen{concat} 
	\end{cases}
	\end{align}
	where the shapes are as follows (for single timestep during decoding process):\marginnote{\green{Synonyms:\\ - scores \\
			- unnormalized alignments}}[-5em]
	\begin{compactitem}
		\item $\vec[s]{\bar h}$: \purple{[batch size, 1, state size]}
		\item $\vec[t]{h}$: \purple{[batch size, 1, state size]}
		\item $\matr[a]{W}$: \purple{[batch size, state size, state size]}
		\item $\text{score}(\vec[t]{h}, \vec[s]{\bar h})$: \purple{[batch size]}
	\end{compactitem}
	
	\item \textbf{Softmax the scores}. In the vast majority of cases, the attention scores are next fed through a softmax to convert them into a valid probability distribution. Most papers will call this some vague probability function, when in reality they are using softmaxonly.\marginnote{ \green{Synonyms:\\
			- softmax outputs \\
			- attention dist. \\
			- alignments}}[-3.5em]
	\begin{align}
	\vec[t]{a}(s) &= \text{align}(\vec[t]{h}, \vec[s]{\bar h}) \\
	&= \frac{\exp(\text{score}(\vec[t]{h}, \vec[s]{\bar h}) )}{
		\sum_{s'} \exp(\text{score}(\vec[t]{h}, \vec[s']{\bar h}) )
	}
	\end{align}
	where the alignment vector $\vec[t]{a}$ has shape \purple{[batch size, $L_{enc}$]}
	
	\item \textbf{Compute the context vector}. The inner product of the softmax outputs and the raw encoder outputs.\marginnote{\green{Synonyms:\\ 
			- context vector\\
			- attention}}[-1em]
	This will have shape \purple{[batch size, attention size]} in TensorFlow, where attention size is from the constructor for your AttentionMechanism.
	
	\item \textbf{Combine context vector and decoder output}: Typically with a concat. \textit{The result is what people mean when they say ``attention''}. Luong et al. denotes this as $\vec[t]{\tilde h}$, the decoder output at timestep $t$. This is what TensorFlow means by ``Luong-style mechanisms output the attention.'' And yes, these are used (at least for Luong) to compute the prediction:
	\graybox{
		\vec[t]{\tilde h} &= \tanh\left( \matr[c]{W}\left[
		\vec[t]{c}, ~ \vec[t]{h}
		\right]   \right) \\
		p( y_t \mid y_{<t}, x  ) &= \text{softmax}(
		\matr[s]{W}  \vec[t]{\tilde h}
		)
	}
\end{enumerate}





























% ==============================================================================
% ==============================================================================
\lecture{Neural Machine Translation}{Effective Approaches to Attention-Based NMT}{May 11, 2017}
% ==============================================================================
% ==============================================================================

\href{https://arxiv.org/abs/1508.04025}{[Luong et. al, 2015]}

\p \blue{Attention-Based Models}. For attention especially, the devil is in the details, so I'm going to go into somewhat excruciating detail here to ensure no ambiguities remain. For both global and local attention, the following information holds true:
\begin{compactitem}
	\item ``At each time step t in the decoding phase, both approaches first take as input the hidden state $\vec[t]{h}$ at the top layer of a stacking LSTM.'' 
	\item Then, they derive [with different methods] a context vector $\vec[t]{c}$ to capture source-side info.
	\item Given $\vec[t]{h}$ and $\vec[t]{c}$, they both compute the \green{attentional hidden state} as:
	\graybox{
		\vec[t]{\tilde h} &= \tanh\left(  \matr[c]{W} [\vec[t]{c};~ \vec[t]{h}]   \right)
	}
	
	\item Finally, the predictive distribution (decoder outputs) is given by feeding this through a softmax:
	\begin{align}
	p(y_t \mid y_{<t}, x) &= \text{softmax}\left(  \matr[s]{W} \vec[t]{\tilde h}   \right)
	\end{align}
\end{compactitem}

\myspace
\p \blue{Global Attention}. Now I'll describe in detail the processes involved in $\vec[t]{h} \rightarrow  \vec[t]{a} \rightarrow \vec[t]{c} \rightarrow   \vec[t]{\tilde h}$.
\begin{compactenum}
	\item $\vec[t]{h}$: Compute the hidden state $\vec[t]{h}$ in the normal way (not obvious if you've read e.g. Bahdanau's work...)
	
	\item $\vec[t]{a}$: 
	\begin{compactenum}
		\item Compute the \green{scores} between $\vec[t]{h}$ and each source $\vec[s]{\bar h}$, where our options are:
		\graybox{
			\text{score}(\vec[t]{h}, \vec[s]{\bar h}) 
			&= \begin{cases}
				\vec[t]{h}^T \vec[s]{\bar h} & \mgreen{dot} \\
				\vec[t]{h}^T \matr[a]{W} \vec[s]{\bar h} & \mgreen{general} \\
				\vec[a]{v}^T \tanh\left(\matr[a]{W} [\vec[t]{h}; ~ \vec[s]{\bar h}] \right) & \mgreen{concat} 
			\end{cases}
		}
		
		\item Compute the \green{alignment vector} $\vec[t]{a}$ of length $L_{enc}$ (number of words in the encoder sequence):
		\begin{align}
		\vec[t]{a}(s) &= \text{align}(\vec[t]{h}, \vec[s]{\bar h}) \\
		&= \frac{\exp(\text{score}(\vec[t]{h}, \vec[s]{\bar h}) )}{
			\sum_{s'} \exp(\text{score}(\vec[t]{h}, \vec[s']{\bar h}) )
		}
		\end{align}
	\end{compactenum}
	
	\item $\vec[t]{c}$: The weighted average over all source (encoder) hidden states\footnote{NOTE: Right after mentioning the context vector, the authors have the following cryptic footnote that may be useful to ponder: \textit{For short sentences, we only use the top part of
			$a_t$ and for long sentences, we ignore words near the end}.}:
	\begin{align}
	\vec[t]{c} &= \sum_{i = 1}^{L_{enc}} \vec[t]{a}(i) \vec[i]{\bar h}
	\end{align}
	
	\item $\vec[t]{\tilde h}$: For convenience, I'll copy the equation for $\vec[t]{\tilde h}$ again here:
	\begin{align}
	\vec[t]{\tilde h} &= \tanh\left(  \matr[c]{W} [\vec[t]{c};~ \vec[t]{h}]   \right)
	\end{align}
\end{compactenum}

\myspace
\p \blue{Input-Feeding Approach}. A copy of each output $\vec[t]{\tilde h}$ is sent forward and concatenated with the inputs for the next timestep, i.e. the inputs go from $\vec[t+1]{x}$ to $[\vec[t]{\tilde h}; \vec[t+1]{x}]$. 








% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Conditional Random Fields}\label{Conditional Random Fields}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------

% ============================================================================================
\lecture{Conditional Random Fields}{Conditional Random Fields}{July 30, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Lafferty et al., ``Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,'' (2001).}

\p \blue{Introduction}. CRFs offer improvements to HMMs, MEMMs, and other discriminative Markov models. MEMMs and other non-generative models share a weakness called the \green{label bias problem}: the transitions leaving a given
state compete only against each other, rather than against all other transitions in the model. The key difference between CRFs and MEMMs is that a CRF has a single exponential model for the joint probability of the \underline{entire sequence of labels} given the observation sequence.

\myspace
\p \blue{The Label Bias Problem}. Recall that MEMMs are run left-to-right. One way of interpreting such a model is to consider how the probabilities (of state sequences) are distributed as we continue through the sequence of observations. The issue with MEMMs is that there's nothing we can do if, somewhere along the way, we observe something that makes one of these state paths extremely likely/unlikely; we can't redistribute the probability mass amongst the various allowed paths. The CRF solution:
\vspace{-1em}
\begin{quote}
	{\small \textit{Account for whole state sequences at once by letting some transitions ``vote''
			more strongly than others depending on the corresponding observations. This implies that score mass will not be conserved, but instead individual transitions can ``amplify'' or
			``dampen'' the mass they receive.}}
\end{quote}

\myspace
\p \blue{Conditional Random Fields}. Here we formalize the model and notation. Let $\rvec{X}$ be a random variable over data sequences to be labeled (e.g. over all words/sentences), and let $\rvec{Y}$ the random variable over corresponding label sequences\footnote{We assume all components $\rvec[i]{Y}$ can only take on values in some finite label set $\mathcal{Y}$.}. Formal definition: 
\vspace{-1em}
\begin{quote}
	{\small \textit{Let $G = (V, E)$ be a graph such that $Y = (Y_v)_{v \in V}$, so that Y is indexed by the vertices of G. Then $(X, Y)$ is a CRF if, when conditioned on $X$, the random variables $Y_v$ obey the Markov property with respect to the graph:
			\begin{align}
			\Prob{Y_v | X, Y_w , w \ne v} =  \Prob{Y_v | X, Y_w , w \sim v}
			\end{align}
			where $w \sim v$ means that $w$ and $v$ are neighbors in G.}}
\end{quote}
All this means is a CRF is a random field (discrete set of random-valued points in a space) where all points (i.e. globally) are conditioned on $\rvec{X}$. If the graph $G = (V, E)$ of $\rvec{Y}$ is a tree, its cliques\footnote{A clique is a subset of vertices in an undirected graph such that every two distinct vertices in the clique are adjacent} are the edges and vertices. Take note that $\rvec{X}$ is not a member of the vertices in $G$. $G$ only contains vertices corresponding to elements of $\rvec{Y}$. Accordingly, when the authors refer to cases where $G$ is a ``chain'', remember that they just mean the $\rvec{Y}$ vertex sequence.  \\

\p By the fundamental theorem of random fields:
\graybox{
	p_{\theta}(\rvec{y} \mid \rvec{x})
	&\propto \exp\left(
	\sum_{e \in E, k} \lambda_k f_k(e, \rvec{y}|_e, \rvec{x}) +
	\sum_{v \in V, k} \mu_k g_k(v, \rvec{y}|_v, \rvec{x})
	\right)
}
where $\rvec{y}|_S$ is the set of components of $\rvec{y}$ associated with the vertices in subgraph $S$. We assume the $K$ feature [functions] $f_k$ and $g_k$ are given and fixed. Note that $f_k$ are the feature functions over \textit{transitions} $y_{t-1}$ to $y_t$, and $g_k$ are the feature functions over \textit{states} $y_t$ and $x_t$. Our estimation problem is thus to determine parameters $\theta = (\lambda_1, \lambda_2, \ldots; \mu_1, \mu_2, \ldots)$ from the labeled training data.

\myspace
\p \blue{Linear-Chain CRF}. Let $|\mathcal Y|$ denote the number of possible labels. At each position $t$ in the observation sequence $\vec x$, we define the $|\mathcal Y| \times |\mathcal Y|$ matrix random variable $\matr[t]{M}(\vec x)$
\begin{align}
\matr[t]{M}(y', y, \mid \vec x) &= \exp(\matr[t]{\Lambda}(y', y \mid x)) \\
\matr[t]{\Lambda}(y', y \mid x) &=
\sum_k \lambda_k f_k(y', y, \vec x) + 
\sum_k \mu_k g_k(y, \vec x)
\end{align}
where $y_{t-1} := y'$ and $y_t := y$. We can see that the individual elements correspond to specific values of $e$ and $v$ in the double-summations of $p_{\theta}(\vec y \mid \vec x)$ above. Then the normalization (partition function) $Z_{\theta}(\vec x)$ is the $(y_0, y_{T+1})$ entry (the fixed boundary states) of the product:
\begin{align}
Z_{\theta}(\vec x) &= \left[ \prod_{t = 1}^{T + 1} \matr[t]{M}(\vec x)  \right]_{y_0, y_{T + 1}}
\end{align} 
which includes all possible sequences $\vec y$ that start with the fixed $y_0$ and end with the fixed $y_{T + 1}$. Now we can write the conditional probability as a function of just these matrices:
\graybox{
	p_{\theta}(\vec y \mid \vec x)
	&= \dfrac{
		\prod_{t = 1}^{T + 1} \matr[t]{M}(y_{t - 1}, y_t \mid \vec x)	
	}{
		\left[ \prod_{t = 1}^{T + 1} \matr[t]{M}(\vec x)  \right]_{y_0, y_{T + 1}}
	}	
}

\myspace
\p \blue{Parameter Estimation} (for linear-chain CRFs). For each $t$ in $[0, T + 1]$, define the \green{forward vectors} $\alpha_t(\vec x)$ with base case $\alpha_0(y \mid \vec x) = 1$ if $y = y_0$, else 0. Similarly, define the \green{backward vectors} $\beta_t(\vec x)$ with base case $\beta_{T + 1}(y \mid \vec x) = 1$ if $y = y_{T+1}$ else 0\footnote{Remember that $y_0$ and $y_{T + 1}$ are their own fixed symbolic constants representing a fixed start/stop state.}. Their recurrence relations are
\begin{align}
\alpha_t(\vec x) &=  \alpha_{t - 1}(\vec x) \matr[t]{M}(\vec x) \\
\beta_t(\vec x)^T &= \matr[t+1]{M}(\vec x) \beta_{t + 1}(\vec x)
\end{align}



% ============================================================================================
\lecture{Conditional Random Fields}{An Introduction to Conditional Random Fields}{April 30, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Sutton et al., ``An Introduction to Conditional Random Fields,'' (2012).}



\p \blue{Graphical Modeling} (2.1). Some notation. Denote factors as $\psi_a(\vec[a]{y})$ where $1 \le a \le A$ and $A$ is the total number factors. $\vec[a]{y}$ is an assignment to the subset $Y_a \subseteq Y$ of variables associated with $\psi_a$. The value returned by $\psi_a$ is a non-negative scalar that can be thought of as a measure of how compatible the values $\vec[a]{y}$ are with each other. Given a collection of subsets $\{Y_a\}_{a=1}^{A}$ of $Y$, an \green{undirected graphical model} is the set of all distributions that can be written as
\begin{align}
p(y) &= \inv{Z} \prod_{a=1}^A \psi_a(\vec[a]{y}) \\
Z &= \sum_{\vec{y}} \prod_{a=1}^A \psi_a(\vec[a]{y})
\end{align}
for any choice of \green{factors} $\mathcal{F} = \{\psi_a\}$ that have $\psi_a(\vec[a]{y}) \ge 0$ for all $\vec[a]{y}$. The sum for the \green{partition function}, $Z$, is over all possible assignments $\vec{y}$ of the variables $Y$. We'll use the term \green{\textit{random field}} to refer to a particular distribution among those defined by an undirected model\footnote{i.e. a particular set of factors.}. \\

\p We can represent the factorization with a \green{factor graph}: a bipartite graph $G = (V, F, E)$ in which one set of nodes $V = \{1, 2, \ldots, |Y| \}$ indexes the RVs in the model, and the set of nodes $F= \{1, 2, \ldots, A\}$ indexes the factors. A connection between a variable node $Y_s$ for $s \in V$ to some factor node $\psi_a$ for $a \in F$ means that $Y_s$ is one of the arguments of $\psi_a$. It is common to draw the factor nodes as squares, and the variable nodes as circles.


\myspace
\p \blue{Generative versus Discriminative Models} (2.2). Naive Bayes is generative, while logistic regression (a.k.a maximum entropy) is discriminative. Recall that Naive Bayes and logistic are defined as, respectively,
\begin{align}
p(y, \vec{x}) &= p(y) \prod_{k = 1}^{K} p(x_k \mid y) \\
p(y \mid \vec{x}) &= \inv{Z(\vec{x})} \exp \left( \sum_{k = 1}^K \theta_{k} f_k(y, \vec{x})  \right)
\end{align}
where the $f_k$ in the definition of logistic regression denote the feature functions. We could set them, for example, as $f_{y', j} (y, \vec{x}) = 1_{y' = y} x_j$. \\

An example generative model for sequence prediction is the \green{HMM}. Recall that an HMM defines
\begin{align}
p(\vec{y}, \vec{x}) &= \prod_{t = 1}^{T} p(y_t \mid y_{t-1}) p(x_t \mid y_t) 
\end{align}
where we are using the dummy notation of assuming an initial-initial state $y_0$ clamped to 0 and begins every state sequence, so we can write the initial state distribution as $p(y_1 \mid y_0)$. \\

\p We see that the generative models, like naive Bayes and the HMM, define a family of joint distributions that factorizes as $p(y, x) = p(y)p(x \mid y)$. Discriminative models, like logistic regression, define a family of conditional distributions $p(y \mid x)$. The main conceptual difference here is that a conditional distribution $p(y \mid x)$ doesn't include a model of $p(x)$. The principal advantage of discriminative modeling is that it's better suited to include rich, overlapping features. Discriminative models like CRFs make conditional independence assumptions both (1) among $y$ and (2) about how the $y$ can depend on $x$, but do \textit{not} make conditional independence assumptions among $x$.  \\

\p The difference between NB and LR is due \textit{only} to the fact that NB is generative and LR is discriminative. Any LR classifier can be converted into a NB classifier with the same decision boundary, and vice versa. In other words, NB defines the same family as LR, if we interpret NB generatively as 
\begin{align}
p(y, \vec{x}) &= \dfrac{
	\exp \left( \sum_k \theta_k f_k(y, \vec{x}) \right) 
}{
	\sum_{\widetilde{y}, \widetilde{x} } \exp \left( \sum_k \theta_k f_k(\widetilde y, \widetilde x)  \right)
}
\end{align}
and train it to maximize the conditional likelihood. Similarly, if the LR model is interpreted as above, and trained to maximize the joint likelihood, then we recover the same classifier as NB.

\myfig[0.7\textwidth]{figs/intro_crf_2_4.png}


\myspace\Needspace{2\baselineskip}
\p \blue{Linear-Chain CRFs} (2.3). Key point: the conditional distribution $p(\vec{y} \mid \vec{x})$ that follows from the joint distribution $p(\vec{y}, \vec{x})$ of an HMM is in fact a CRF with a particular choice of feature functions. First, we rewrite the HMM joint in a form that's more amenable to generalization:\marginnote{HMM joint distribution}[3em]
\begin{align}
p(\vec{y}, \vec{x}) &= \inv{Z} 
\prod_{t =1}^{T} \exp \left( 
\sum_{i,j \in S} \theta_{i,j} \ind_{  \{ y_t=i, y_{t-1}=j  \} }   
+ \sum_{i \in S, o \in O} \mu_{o,i} \ind_{ \{  y_t=i, x_t=o \}  }
\right) \\
&= \inv{Z} \prod_{t = 1}^{T} \exp \left(
\sum_{k = 1}^{K} \theta_k f_k(y_t, y_{t - 1}, x_t)
\right)
\end{align}
and the latter provides the more compact notation\footnote{Note how we collapsed the summations over $i,j$ and $i,o$ to simply $k$. This is purely notational. Each value $k$ can be mapped to/from a unique $i,j$ or $i,o$ in the first version. Also note that, necessarily, each feature function $f_k$ in the latter version maps to a specific indicator function $\ind$ in the first.}. We can use Bayes rule to then write $p(\vec{y} \mid \vec{x})$, which would give us a particular kind of linear-chain CRF that only includes features for the current word's identity. The general definition of linear-chain CRFs is given below:
\vspace{-0.7em}
\begin{quote}
	{\small\itshape
		Let $Y, X$ be random vectors, $\theta = \{\theta_k\} \in \R^K$ be a parameter vector, and $\mathcal{F} = \{f_k(y, y', \vec[t]{x})\}_{k = 1}^{K}$ be a set of real-valued feature functions. Then a \green{linear-chain conditional random field} is a distribution $p(\vec{y} \mid \vec{x})$ that takes the form:}
	\graybox{
		p(\vec{y} \mid \vec{x} ) &= \inv{Z(\vec x)} 
		\prod_{t = 1}^{T} \exp \left(
		\sum_k	\theta_k f_k(y_t, y_{t - 1}, \vec[t]{x}) \right) \\
		Z(\vec{x}) &= \sum_{\vec y} \prod_{t = 1}^{T} \exp \left(
		\sum_k	\theta_k f_k(y_t, y_{t - 1}, \vec[t]{x}) \right) 
	}
\end{quote}
Notice that a linear chain CRF can be described as a factor graph over $\vec{x}$ and $\vec{y}$, where each local function (factor) $\psi_t$ has the special log-linear form:
\begin{align}
\psi_t(y_t, y_{t-1}, x_t) &= \exp \left(
\sum_k \theta_k f_k(y_t, y_{t - 1}, x_t)
\right)
\end{align}


\myspace
\p \blue{General CRFs} (2.4). Let $G$ be a factor graph over $X$ and $Y$. Then $(X, Y)$ is a conditional random field if for any value $\vec{x}$ of $X$, the distribution $p(\vec{y} \mid \vec{x})$ factorizes according to $G$. If $F = \{\psi_a\}$ is the set of $A$ factors in $G$, then the conditional distribution for a CRF is
\begin{align}
p(\vec{y} \mid \vec{x}) &= \inv{Z(\vec{x})} \prod_{a = 1}^{A} \psi_a(\vec[a]{y}, \vec[a]{x})
\end{align}
It is often useful to require that the factors be log-linear over a prespecified set of feature functions, which allows us to write the conditional distribution as
\begin{align}
p(\vec{y} \mid \vec{x}) &= \inv{Z(\vec{x})} \prod_{\psi_a \in F}
\exp\left( \sum_{k =1}^{K(a)}  \theta_{a,k}f_{a,k}(\vec[a]{y}, \vec[a]{x} ) \right)
\end{align}
In addition, most models rely extensively on \textbf{parameter tying}\footnote{Note how, for CRFs, the actual parameters $\theta$ are tightly coupled with the feature functions $f$.}. To denote this, we partition the factors of $G$ into $\mathcal{C} = \{C_1, C_2, \ldots, C_P\}$, where each $C_p$ is a \green{clique template}: a set of factors sharing a set of feature functions $\{f_{p,k}(\vec[c]{x}, \vec[c]{y})\}_{k=1}^{K(p)}$ and a corresponding set of parameters $\vec[p]{\theta} \in \R^{K(p)}$. A CRF that uses clique templates can be written as 
\begin{align}
p(\vec{y} \mid \vec{x}) &= \inv{Z(\vec x)} \prod_{C_p \in \mathcal{C}} \prod_{\psi_c \in C_p} \psi_c(\vec[c]{x}, \vec[c]{y}; \vec[p]{\theta}) \\
&= \inv{Z(\vec x)} \prod_{C_p \in \mathcal{C}} \prod_{c \in C_p}
\exp\bigg\{ \sum_{k = 1}^{K(p)} \theta_{p,k}f_{p,k}(\vec[c]{x}, \vec[c]{y}) \bigg\}
\end{align}
In a linear-chain CRF, typically one uses one clique template $C_0 = \{\psi_t\}_{t=1}^{T}$. Again, each factor in a given template \textit{shares the same feature functions and parameters}, so the previous sentence means that we reuse the set of features and parameters for each timestep. 

\myspace
\p \blue{Feature engineering} (2.5). 
\begin{compactitem}
	\item \textbf{Label-observation features}. When our label variables are discrete, the features $f_{p,k}$ of a clique template $C_p$ are ordinarily chosen to have a particular form:
	\begin{align}
	f_{p,k}(\vec[c]{y}, \vec[c]{x}) = 1_{\{ \vec[c]{y} = \vec[c]{\widetilde y} \}} q_{p,k}(\vec[c]{x})
	\end{align}
	and we refer to the functions $q_{p,k}(\vec[c]{x})$ as \textit{observation functions}.
	
	\item \textbf{Unsupported features}. Many observation-label pairs may never occur in our training data (e.g. having the word ``with'' being associated with label ``CITY''). Such features are called unsupported features, and can be useful since often their weights will be driven negative, which can help prevent the model from making predictions in the future that are far from what was seen in the training data.
	
	\item \textbf{Edge-Observation and Node-Observation features}: the two most common types of label-observation features. Edge-observation features are for the transition factors, while node-observation features are the form introduced for label-observation features above.
	\begin{align}
	\mtgreen{[edge-obs]} \quad f(y_t, y_{t - 1}, \vec[t]{x}) &= q_m(\vec[t]{x})\ind_{y_t=y,y_{t-1}=y'} \quad (\forall y, y' in \mathcal{Y}, \forall m) \\
	\mtgreen{[node-obs]} \quad f(y_t, y_{t - 1}, \vec[t]{x}) &= \ind_{y_t=y,y_{t-1}=y'} \quad (\forall y, y' in \mathcal{Y})
	\end{align}
	and both use the same $f(y_t, \vec[t]{x}) = q_m(\vec[t]{x}) \ind_{y_t = y}$ $(\forall y, \in \mathcal{Y}, \forall m)$. Recall that $m$ is the index into our set of observation features\footnote{In CRFSuite, the observation features are all the attributes we define, and any features that use both label and observation are defined within CRFSuite itself.}.
	
	\item \textbf{Feature Induction}. The model begins with a number of base features, and the training procedure adds conjunctions of those features.
\end{compactitem}


\myspace\Needspace{20\baselineskip}
\subsub{Inference (Sec. 4)}
\myspace

\p There are two inference problems that arise:
\begin{compactenum}
	\item Wanting to predict the labels of a new input $\vec{x}$ using the most likely labeling $\vec{y}^* = \argmax_{\vec y} p(\vec{y} \mid \vec{x})$. 
	
	\item Computing marginal distributions (during parameter estimation, for example) such as node marginals $p(y_t \mid \vec x)$ and edge marginals $p(y_t, y_{t -1} \mid \vec x)$. 
\end{compactenum}
For linear-chain CRFs, the \green{forward-backward algorithm} is used for computing marginals, and the \green{Viterbi algorithm} for computing the most probable assignment. We'll first derive these for the case of HMMs, and then generalize to the linear-chain CRF case. 

\myspace
\p \blue{Forward-backward algorithm (HMMs)}. An efficient technique for computing marginals. We begin by writing out $p(\vec x)$, and using the distributive law to convert the sum of products to a product of sums\marginnote{We define the $\psi_t$ as	$p(y_t \mid y_{t-1}) p(x_t \mid y_t)$}[3em]:
\begin{align}
p(\vec x) &= \sum_{\vec y} p(\vec x, \vec y) \\
&= \sum_{\vec y} \prod_{t = 1}^{T} \psi_t(y_t, y_{t - 1}, x_t) \\
&= \sum_{y_1} p(y_1)p(x_1 \mid y_1) \sum_{y_2} p(y_2 \mid y_1) p(x_2 \mid y_2) \sum_{y_3} \cdots \sum_{y_T} p(y_T \mid y_{T-1}) p(x_T \mid y_{T}) \\
&= \sum_{y_1} \psi_1(y_1, x_1) \sum_{y_2} \cdots \sum_{y_T} \psi_T(y_T, y_{T - 1}, x_T) \\
&= \sum_T \sum_{T - 1} \psi_T(y_T, y_{t - 1}, x_T) \sum_{y_{T - 2}} \cdots \sum_{y_1} \psi_1(y_1, x_1) \label{crf-4-3}
\end{align}
We see that we can save an exponential amount of work by caching the inner sums as we go. Let $M$ denote the number of possible states for the $y$ variables. We define a set of $T$ \green{forward variables} $\alpha_t$, each of which is a vector of size $M$:
\begin{align}
\alpha_t(j) &\triangleq p(\slice[t]{x}, y_t = j) \label{alpha-1} \\
&= \sum_{\slice[t-1]{y}} p(\slice[t]{x}, \slice[t-1]{y}, y_t = j) \label{alpha-2} \\
&= \sum_{\slice[t-1]{y}} \psi_t(j, y_{t - 1}, x_t) \prod_{t' = 1}^{t' - 1} \psi_{t'}(y_{t'}, y_{t - 1}, x_{t'}) \\
&= \sum_{i \in S} \psi_t(j, i, x_t) \sum_{\slice[t-2]{y}} \psi_{t-1}(y_{t -1}, y_{t -2}, x_{t -1}) \prod_{t' = 1}^{t - 2} \psi_{t'}(y_{t'}, y_{t - 1}, x_{t'}) \\
&= \sum_{i \in S} \psi_t(j, i, x_t) \alpha_{t - 1}(i)
\end{align}
where $S$ is the set of M possible states. By recognizing that $p(\vec x) = \sum_{j \in S}\sum_{\slice[t-1]{y}} p(\slice[t]{x}, \slice[t-1]{y}, j)$, we can rewrite $p(\vec x)$ as
\begin{align}
p(\vec x) &= \sum_{j \in S} \alpha_T(j) \label{alpha-3}
\end{align}

Notice how in the step from equation \ref{alpha-1} to \ref{alpha-2}, we marginalized over all possible $y$ subsequences that could've been aligned with $\slice[t]{x}$. We will repeat this pattern to derive the backward recursion for $\beta_t$, which is the same idea except now we go from $T$ backward until some $t$ (instead of going from 1 \textit{forward} until some $t$).\marginnote{We initialize $\beta_T(i) = 1$.}[4em]
\begin{align}
\beta_t(i) &\triangleq p(\slice[T][t+1]{x} \mid y_t = i) \\
&= \sum_{\slice[T][t+1]{y}} \psi_{t + 1}(y_{t + 1}, i, x_{t + 1}) \prod_{t' = t+2}^{T} \psi_{t'}(y_{t'}, y_{t'-1}, x_{t'}) \\
&= \sum_{y_{t + 1}} \psi_{t + 1}(y_{t + 1}, i, x_{t + 1}) \beta_{t+1}(y_{t + 1})
\end{align}
Similar to how we obtained equation \ref{alpha-3}, we can rewrite $p(\vec x)$ in terms of the $\beta$:
\begin{align}
p(\vec x) &= \beta_0(y_0) \triangleq \sum_{y_1} \psi_1(y_1,  y_0, x_1) \beta_1(y_1) \label{beta-1}
\end{align}
We can then combine the definition for $\alpha$ and $\beta$ to compute marginals of the form $p(y_{t - 1}, y_t \mid \vec{x})$:
\graybox{
	p(y_{t - 1}, y_t \mid \vec{x}) &=
	\inv{p(\vec x)} \alpha_{t -1}(y_{t-1}) \psi_t(y_t, y_{t-1}, x_t) \beta_t(y_t) \label{fw-bw-marginals}	\\
	\text{where}\quad p(\vec x) &=  \sum_{j \in S} \alpha_T(j)  =  \beta_0(y_0) 
}
In summary, the forward-backward algorithm consists of the following steps:
\begin{compactenum}
	\item Compute $\alpha_t$ for all $t$ using equation \ref{alpha-3}. 
	\item Compute $\beta_t$ for all $t$ using equation \ref{beta-1}. 
	\item Return the marginal distributions computed from equation \ref{fw-bw-marginals}. 
\end{compactenum}

\myspace
\p \blue{Viterbi algorithm (HMMs)}. For computing $\vec{y}^* = \argmax_{\vec y} p(\vec y \mid \vec x)$. The derivation is nearly the same as how we derived the forward-backward algorithm, except now we've replaced the summations in equation \ref{crf-4-3} with maximization. The analog of $\alpha$ for viterbi are defined as:
\begin{align}
\delta_t(j) &= \max_{\slice[t-1]{y}} \psi_t(j, y_{t -1}, x_t) \prod_{t' = 1}^{t - 1} \psi_{t'}(y_{t'}, y_{t' - 1}, x_{t'}) \\
&= \max_{i \in S} \psi_t(j, i, x_t) \delta_{t - 1}(i)
\end{align}
and the maximizing assignment is computed by a backwards recursion,
\graybox{
	y_T^* &= \argmax_{i \in S} \delta_T(i) \\
	y_t^* &= \argmax_{i \in S} \psi_t(y_{t + 1}^*, i, x_{t + 1}) \delta_t(i) \text{ for } t < T
}
Computing the recursions for $\delta_t$ and $y_t^*$ together is the \textit{Viterbi algorithm}. 


\myspace
\p \blue{Forward-backward and Viterbi for linear-chain CRF}. Generalizing to the linear-chain CRF, where now
\begin{align}
p(\vec y \mid \vec x) &= \inv{Z(\vec x)} \prod_{t = 1}^{T} \psi_t(y_t, y_{t - 1}, x_t) \\
\text{where}\quad \psi_t(y_t, y_{t - 1}, x_t) &= \exp \left\{    
\sum_k \theta_k f_k(y_t, y_{t -1}, x_t) \right\}
\end{align}
and the results for the forward-backward algorithm become
\graybox{
	p(y_{t - 1}, y_t \mid \vec{x}) &=
	\inv{Z(\vec x)} \alpha_{t -1}(y_{t-1}) \psi_t(y_t, y_{t-1}, x_t) \beta_t(y_t)	\\
	p(y_t \mid \vec x) &= \inv{Z(\vec x)} \alpha_t(y_t) \beta_t(y_t) \\
	\text{where}\quad 
	Z(\vec x) &=  \sum_{j \in S} \alpha_T(j)  =  \beta_0(y_0) 
}
Note that the interpretation is also slightly different. The $\alpha$, $\beta$, and $\delta$ variables should only be interpreted with the factorization formulas, and \textit{not} as probabilities. Specifically, use
\begin{align}
\alpha_t(j) 
&= \sum_{\slice[t-1]{y}} \exp \left\{\sum_k \theta_k f_k(j, y_{t -1}, x_t) \right\} 
\prod_{t' = 1}^{t' - 1} \exp \left\{\sum_k \theta_k f_k(y_{t'}, y_{t' -1}, x_{t'}) \right\}  \\
\beta_t(i) &= \sum_{\slice[T][t+1]{y}}  \exp \left\{\sum_k \theta_k f_k(y_{t+1}, i, x_{t+1}) \right\} \prod_{t' = t+2}^{T}  \exp \left\{\sum_k \theta_k f_k(y_{t'}, y_{t' -1}, x_{t'}) \right\}  \\
\delta_t(j) &= \max_{\slice[t-1]{y}}  \exp \left\{\sum_k \theta_k f_k(j, y_{t-1}, x_t) \right\} \prod_{t' = 1}^{t - 1}  \exp \left\{\sum_k \theta_k f_k(y_{t'}, y_{t' -1}, x_{t'}) \right\} 
\end{align}


\myspace 
\p \blue{Markov Chain Monte Carlo} (MCMC). The two most popular classes of approximate inference algorithms are \green{Monte Carlo} algorithms and \green{variational} algorithms. In what follows, we drop the CRF-specific notation and refer to the more general joint distribution
\begin{align}
p(\vec y) = Z^{-1} \prod_{a \in F} \psi_a(\vec[a]{y})
\end{align}
that factorizes according to some factor graph $G = (V, F)$. MCMC methods construct a Markov chain whose state space is the same as that of $Y$, and sample from this chain to approximate, e.g., the expectation of some function $f(\vec y)$ over the distribution $p(\vec y)$. MCMC algorithms aren't commonly applied in the context of CRFs, since parameter estimation by maximum likelihood requires calculating marginals many times. 


\myspace\Needspace{20\baselineskip}
\subsub{Parameter Estimation (Sec. 5)}
\myspace

\p \blue{Maximum Likelihood for Linear-Chain CRFs}. Since we're modeling the conditional distribution with CRFs, we use the \green{conditional log likelihood} $\ell(\vec{\theta})$ with l2-regularization:
\begin{align}
\ell(\vec{\theta}) &= \sum_{i = 1}^{N} \log p(\vec{y}^{(i)} \mid \vec{x}^{(i)} ; \vec{\theta}) - \inv{2\sigma^2}\sum_{k = 1}^{K} \theta_k^2 \\
&= \sum_{i = 1}^{N}\sum_{t = 1}^{T}\sum_{k = 1}^{K} \theta_k f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)}) - \sum_{i = 1}^{N} \log Z(\vec{x}^{(i)})  - \inv{2\sigma^2}\sum_{k = 1}^{K} \theta_k^2
\end{align}
with regularization parameter $1/2\sigma^2$. The partial derivatives are 
\begin{align}
\pderiv{\ell}{\theta_k} &= \sum_{i,t} f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)})
- \sum_{i,t,y,y'} f_k(y, y', \vec[t]{x}^{(i)}) p(y, y' \mid \vec{x}^{(i)})
- \frac{\theta_k}{\sigma^2}
\end{align}
and the derivation for the partial derivative of $\log(Z(x))$ is
\begin{align}
\pderiv{\log Z(x)}{\theta_k} 
&= \inv{Z(x)} \pderiv{}{\theta_k} \left[ \sum_{\slice[T]{y}} \prod_{t = 1}^{T} \exp\left\{ \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}  \right] 
\\
&= \inv{Z(x)} \sum_{\slice[T]{y}} \pderiv{}{\theta_k}  \exp\left\{ \sum_t \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}
\\
&= \inv{Z(x)} \sum_{\slice[T]{y}} \sum_t f_k(y_t, y_{t-1}, x_t)  \exp\left\{ \sum_t \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}  
\\
&= \sum_t \sum_{y_t} \sum_{y_{t-1}} f_k(y_t, y_{t-1}, x_t) \left[ \sum_{\slice[t-2]{y}} \sum_{\slice[T][t+1]{y}} \inv{Z(x)}  \exp\left\{ \sum_t \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}  \right] 
\\
&= \sum_t \sum_{y} \sum_{y'} f_k(y, y', x_t) p(y_t=y, y_{t-1}=y' \mid x)
\end{align}

We can rewrite this in the form of expectations. For now, let $\widetilde{p}(\vec y, \vec x)$ denote the \textit{empirical distribution}, and let $\hat{p}(\vec y \mid \vec x; \theta)\widetilde{p}(\vec x)$ denote the \textit{model distribution}.
\begin{align}
\pderiv{\ell}{\theta_k}
&= \E[\vec{x},\vec{y} \sim \widetilde{p}(\vec{y},\vec{x})]{\sum_t f_k(y_t, y_{t-1}, x_t)}
- \E[\vec{x}, \vec y \sim \hat{p}(\vec y, \vec x)]{\sum_t f_k(y_t, y_{t-1}, x_t)}
\end{align}

\begin{algorithm}[Procedure: Training Linear-Chain CRFs]
	Here I summarize the main steps involved during a parameter update.\\
	
	\textbf{Inference}. We need to compute the log probabilities for each instance in the dataset, under the current parameters. We will need them when evaluating $Z(\vec x)$ and the marginals $p(y, y' \mid \vec x)$ when computing gradients.
	\begin{compactenum}
		\item Initialize $\alpha_1(j) = \exp\{ \sum_k \theta_k f_k(j, y_0, x_1) \}$ ($y_0$ is the fixed initial state) and $\beta_T(i) = 1$. 
		
		\item For all $t$, compute:
		\begin{align}
		\alpha_t(j) &= \sum_{i \in S} \exp\left\{ \sum_k \theta_k f_k(j, i, x_t)\right\}  \alpha_{t - 1}(i)  \\
		\beta_t(j) &= \sum_{i \in S}   \exp\left\{ \sum_k \theta_k f_k(i, j, x_{t+1})\right\} \beta_{t+1}(i)
		\end{align}
		
		\item Compute the marginals:
		\begin{align}
		p(y_t, y_{t-1}, \mid \vec{x}) &=
		\inv{Z(\vec x)} \alpha_{t -1}(y_{t-1}) \psi_t(y_t, y_{t-1}, x_t) \beta_t(y_t)	\\
		p(y_t \mid \vec x) &= \inv{Z(\vec x)} \alpha_t(y_t) \beta_t(y_t) \\
		\text{where}\quad 
		Z(\vec x) &=  \sum_{j \in S} \alpha_T(j)  =  \beta_0(y_0) 
		\end{align}
	\end{compactenum}
	
	\textbf{Gradients}. For each parameter $\theta_k$, compute:
	\begin{align}
	\pderiv{\ell}{\theta_k} &= \sum_{i,t} f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)})
	- \sum_{i,t,y,y'} f_k(y, y', \vec[t]{x}^{(i)}) p(y, y' \mid \vec{x}^{(i)})
	- \frac{\theta_k}{\sigma^2}
	\end{align}
\end{algorithm}


\myspace \p \blue{CRF with latent variables}. Now we have additional latent variables $\vec z$:
\begin{align}
p(\vec y, \vec z \mid \vec x) &= \inv{Z(\vec x)} \prod_t \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x}) 
\end{align}
Since we observe $\vec y$ during training, what if we instead treat this as a CRF with both $\vec x$ \textit{and} $\vec y$ observed?
\begin{align}
p(\vec z \mid \vec y, \vec x) 
&= \inv{Z(\vec y, \vec x)} \prod_t \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x}) \\
Z(\vec y, \vec x) 
&= \sum_{\vec z} \prod_t  \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x})
\end{align}
We can use the same inference algorithms as usual to compute $Z(\vec y, \vec x)$, and the key result is that we can now write
\graybox{
	p(\vec y \mid \vec x)
	&= \inv{Z(\vec x)} \sum_{\vec z} \prod_t  \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x})
	= \frac{Z(\vec y, \vec x)}{Z(\vec x)}
}
Finally, we can write the gradient as\footnote{This uses the trick $$ \deriv{f}{\theta} = f(\theta) \deriv{\log f}{\theta} $$}
\begin{align}
\pderiv{\ell}{\theta_k}
&= \sum_{\vec z} p(\vec z \mid \vec y, \vec x) \pderiv{}{\theta_k}\left[ \log p(\vec y, \vec z \mid \vec x) \right]\\
&= \sum_t \sum_{\vec[t]{z}} \left[ p(\vec[t]{z} \mid \vec y, \vec x) f_k(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x})
-  \sum_{y, y'} p(\vec[t]{z}, y, y' \mid \vec[t]{x}) f_k(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x}) \right]
\end{align}
where I've assumed there are no connections between any $\vec[t]{z}$ and $\vec[t' \ne t]{z}$.



\myspace
\p \blue{Stochastic Gradient Methods}. Now we compute gradients for a single example, and for a linear-chain CRF:
\begin{align}
\pderiv{\ell_i}{\theta_k} 
&= \sum_{t} f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)})
- \sum_{t,y,y'} f_k(y, y', \vec[t]{x}^{(i)}) p(y, y' \mid \vec{x}^{(i)})
- \frac{\theta_k}{N \sigma^2}
\end{align}
which corresponds to parameter update (remember: we are using the LL, not the negative LL):
\begin{align}
\theta^{(m)}
&= \theta^{(m-1)} + \alpha_m \nabla \ell_i(\theta^{(m-1)})
\end{align}
where $m$ denotes this is the $m$th update of the training process.

\myspace\Needspace{20\baselineskip}
\subsub{Related Work and Future Directions (Sec. 6)}
\myspace

\p \blue{MEMMs}. Maximum-entropy Markov models. Essentially a Markov model in which the transition probabilities are given by logistic regression. Formally, a MEMM is defined by
\begin{align}
p_{MEMM}(\vec y \mid \vec x) &= \prod_{t = 1}^{T} p(y_t \mid y_{t -1}, \vec x) \\
p(y_t \mid y_{t-1}, \vec x) &= \frac{ \exp\bigg\{  \sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, \vec[t]{x}) \bigg\} }{ Z_t(y_{t-1}, \vec x) } \\
Z_t(y_{t-1}, \vec x) &= \sum_{y'} \exp\bigg\{  \sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, \vec[t]{x}) \bigg\}
\end{align}
which has some important differences compared to the linear-chain CRF. Notice how maximum-likelihood training of MEMMs does \textit{not} require performance inference over full output sequences $\vec y$, because $Z_t$ is a simple sum over the labels at a single position. MEMMs, however, suffer from \textit{label bias}, while CRFs do not.

\myspace
\p \blue{Bayesian CRFs}. Instead of predicting the optimal labeling $y_{ML}^*$ for input sequence $x$ with maximum likelihood (ML), we can instead use a fully Bayesian (B) approach, both of which are shown below for comparison:
\begin{align}
y_{ML}^* &\leftarrow arg\,max_y p(y \mid x; \hat{\theta}) \\
y_{B}^* &\leftarrow arg\,max_y \mathbb{E}_{\theta \sim p(\theta \mid x)}(p(y \mid x; \theta))
\end{align}
Unfortunately, computing the exact integral (the expectation) is usually intractable, and we must resort to approximate methods like MCMC.  







% ============================================================================================
\lecture{Conditional Random Fields}{Hidden-Unit Conditional Random Fields}{June 30, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Maaten et al., ``Hidden-Unit Conditional Random Fields,'' (2011).}

\p \blue{Introduction}. Three key advantages of CRFs over HMMs:
\begin{compactenum}
	\item CRFs don't assume that the observations are conditionally independent given the hidden (or target if linear-chain CRF) states. 
	
	\item CRFs don't suffer from the label bias problems of models that do local probability normalization\footnote{See the introduction in my CRF notes for recap of label-bias.}. 
	
	\item For certain choices of factors, the negative conditional L.L. is convex. 
\end{compactenum}
The hidden-unit CRF (HUCRF), similar to discriminative restricted Boltzmann machines (RBMs), has binary stochastic hidden units that are conditionally independent given the data and the label sequence. By exploiting the conditional independence properties, we can efficiently compute:
\begin{compactenum}
	\item The exact gradient of the C.L.L. 
	
	\item The most likely label sequence. 
	
	\item The marginal distributions over label sequences. 
\end{compactenum}

\myspace
\p \blue{Hidden-Unit CRFs}. At each time step $t$, the HUCRF employs $H$ binary stochastic hidden units $\vec[t]{z}$. It models the conditional distribution as 
\graybox{
	p(\vec y \mid \vec x) &= \inv{\matr{Z}(\vec x)} \sum_{\vec z} \exp\left( E( \vec x, \matr z, \vec y ) \right) \\
	\begin{split}
		E(\vec x, \vec z, \vec y) &= 
		\sum_{t = 2}^T \big[ \vec[t - 1]{y}^T \matr{A} \vec[t]{y} \big] + 
		\sum_{t = 1}^T \big[ \vec[t]{x}^T \matr{W} \vec[t]{z} + 
		\vec[t]{z}^T \matr{V} \vec[t]{y} + 
		\vec{b}^T \vec[t]{z} + \vec{c}^T \vec[t]{y}  \big] \\
		&~~+ \vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau}
	\end{split}
}
Since the hidden units are conditionally independent given the data and labels, the hidden units can be marginalized out one-by-one. This, along with the nice property that the hidden units have binary elements, allows us to write $p(\vec y \mid \vec x)$ without writing any $\vec[t]{z}$ explicitly, as shown below:
\begin{align}
\begin{split}
p(\vec y \mid \vec x) 
= \frac{ \exp\{\vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau} \} }{ Z(\vec x) }
&\prod_{t=1}^{T}\bigg[ 
\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}           \\
&\quad \prod_{h = 1}^{H} \sum_{z_h \in \{0, 1\}} \exp\{ z_h b_h + z_h \vec[h]{w}^T \vec[t]{x} + z_h \vec[h]{v}^T \vec[t]{y}  \}	\bigg]
\end{split}\\
\begin{split}
= \frac{ \exp\{\vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau} \} }{ Z(\vec x) }
&\prod_{t=1}^{T}\bigg[ 
\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}           \\
&\quad \prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)	\bigg]
\end{split}
\end{align}
For inference, we'll need an algorithm for computing the marginals $p(y_t \mid \vec x)$ and $p(y_t, y_{t - 1} \mid \vec x)$. The equations are essentially the same as the forward-backward formulas for the linear-chain CRF, but with summations over $\vec z$:
\graybox{
	p(y_t, y_{t-1} \mid \vec x) &\propto \alpha_{t - 1}(y_{t - 1}) \bigg[ \sum_{\vec[t]{z}} \Psi_t(\vec[t]{x}, \vec[t]{z}, y_t, y_{t - 1}) \bigg] \beta_{t}(y_t)	
	\\
	p(y_t \mid \vec x) &\propto \alpha_t(y_t) \beta_t(y_t) \\
	\\
	\alpha_t(j) &= \sum_{i \in \mathcal{Y}} \sum_{\vec[t]{z}} \Psi_t(\vec[t]{x}, \vec[t]{z}, j, i) \alpha_{t - 1}(i)
	\\
	\beta_t(j) &= \sum_{i \in \mathcal{Y}} \sum_{\vec[t+1]{z}} \Psi_{t+1}(\vec[t+1]{x}, \vec[t+1]{z}, i, j) \beta_{t+1}(i)
}


\myspace 
\p \blue{Training}. The conditional log likelihood for a single example $(\vec x, \vec y)$ is (bias and initial-state terms omitted) 
\begin{align}
\mathcal{L} &= \log p(\vec y \mid \vec x) \\
&= \sum_{t = 1}^{T} \log \left( 
\sum_{\vec[t]{z}} \Psi_t\left( \vec[t]{x}, \vec[t]{z}, y_{t-1}, y_t)  \right)
\right) - \log Z(\vec x) \\
\text{where}\qquad \Psi_t &:= \exp\{  y_{t-1} \matr{A} y_t + \vec[t]{x}^T \matr{W} \vec[t]{z} + \vec[t]{z}^T \matr{V} y_t  \}
\end{align}
Let $\Upsilon = \{\matr W, \matr V, \vec b, \vec c, \}$ be the set of model parameters. The gradient w.r.t. the data-dependent\footnote{The data-dependent parameters are each individual element of the elements of $\Upsilon$. ``Data'' here means $(\vec x, \vec y)$. Notice that $\Upsilon$ does not include $\matr A$, $\vec{\pi}$, or $\vec{\tau}$.} parameters $v \in \Upsilon$ is given by
\begin{align}
\pderiv{\mathcal L}{v} 
&= \sum_{t=1}^{T} \left[
\sum_{k \in \mathcal{Y}} \left(
\left(  \ind_{y_t = k}   -  p(y_t = k \mid \vec x) \right) \sum_{h = 1}^{H} \sigma(o_{hk}(\vec[t]{x})) \pderiv{o_{hk}(\vec[t]{x})}{ v }
\right)
\right] 
\\
\text{where}\qquad
o_{hk}(\vec[t]{x}) &= b_h + c_k + V_{hk} + \vec[h]{w}^T \vec[t]{x}
\end{align}
Unfortunately, the negative CLL is \textbf{non-convex}, and so we are only guaranteed to converge to a \textit{local} maximum of the CLL. 


\subsub{Detailed Derivations}

Unfortunately, the paper leaves out a lot of details regarding derivations and implementations. I'm going to work through them here. First, a recap of the main equations, and with all biases/initial states included. Not leaving anything out\footnote{The equation for $p(\vec y \mid \vec x)$ from the paper, and thus here, is technically incorrect. The term $\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}$ should not be included in the product over $t$ for $t=1$.}
\begin{small}
	\begin{align}
	p(\vec y \mid \vec x) 
	&= \frac{\exp\left\{ \vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau}  \right\}}{Z(\vec x)} \prod_{t=1}^{T}\bigg[ 
	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}  
	\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)	\bigg] \\
	NLL 
	&= - \sum_{i=1}^{N} \log p(\vec{y}^{(i)} \mid \vec{x}^{(i)}) \\
	&= - \sum_{i=1}^{N} \left[ \sum_{t = 1}^{T} \log \left( \sum_{\vec[t]{z}} \psi_t(\vec[t]{x}, \vec[t]{z}, \vec[t-1]{y}, \vec[t]{y}) \right) 
	- \log Z(\vec{x}^{(i)}) \right] 
	\end{align}
\end{small}

%\begin{comment}
The above formula for $p(\vec y \mid \vec x)$ implies something that will be very useful:
\begin{align}
\sum_{\vec[t]{z}} \psi_t(\vec[t]{x}, \vec[t]{z}, \vec[t-1]{y}, \vec[t]{y}) 
&= 	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}  
\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)
\end{align}
Using the generalization of the product rule for derivatives over N products, we can derive that
\begin{align}
\pderiv{}{v} \prod_h (1 + \exp\{ o(h) \}) &= \left[ \prod_h (1 + \exp\{ o(h) \})\right] \left[ \sum_h \sigma\left(o\left(h\right)\right) \pderiv{o(h)}{v} \right]
\end{align}
Which means the derivatives of $\sum_z \psi$ for the data-dependent params $v_{dat}$ and transition params $v_{tr}$, are:
\begin{align}
\pderiv{\sum_{\vec[t]{z}} \psi_t}{v_{dat}} 
&=  \left[ \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right]  \sum_{\vec[t]{z}} \psi_t \\
\pderiv{\sum_{\vec[t]{z}} \psi_t}{v_{tr}}
&= \left[ \pderiv{}{v_{tr}}  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right] \sum_{\vec[t]{z}} \psi_t 
\end{align}
which also conveniently means that
\begin{align}
\pderiv{}{v_{dat}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
&=  \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \\
\pderiv{}{v_{tr}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
&=  \pderiv{}{v_{tr}}  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} 
\end{align}
% \end{comment}

I'll now proceed to derive the gradients of negative (conditional) log-likelihood for the main parameters. We can save some time by getting the base formula for any of the gradients with respect to a specific single parameter $v$:
\begin{small}
	\begin{align}
	\pderiv{NLL}{v} 
	&=  - \sum_{i=1}^{N} \left[ \sum_{i = 1}^{T} \pderiv{}{v} \log \left( \sum_{\vec[t]{z}} \psi_t(\vec[t]{x}^{(i)}, \vec[t]{z}, \vec[t-1]{y}^{(i)}, \vec[t]{y}^{(i)}) \right) 
	- \pderiv{}{v} \log Z(\vec{x}^{(i)}) \right] \\
	\pderiv{\log Z(\vec{x}^{(i)})}{v}
	&= \inv{Z(\vec{x}^{(i)} )} \pderiv{}{v} \sum_{\slice[T]{y}} \widetilde{p}(\vec[1]{y},\ldots,\vec[T]{y} \mid \vec{x}^{(i)}) \\
	\pderiv{}{v}  \widetilde{p}(\vec[1]{y},\ldots,\vec[T]{y} \mid \vec{x}^{(i)}) 
	&= \pderiv{}{v} \prod_t \sum_{\vec[t]{z}} \psi_t \\
	&= \left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
	\sum_t \frac{\pderiv{}{v} \sum_{\vec[t]{z}}  \psi_t }{ \sum_{\vec[t]{z}} \psi_t   }
	\right]
	\end{align}
\end{small}
where I've done some regrouping on the last line to be more gradient-friendly.

\begin{example}[Data-dependent parameters] 
	
	All params $v$ that are not transition params. 
	
	\begin{align}
	\pderiv{NLL}{v_{dat}} 
	&=  - \sum_{i=1}^{N} \left[ \sum_{t = 1}^{T} \pderiv{}{v_{dat}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
	- \pderiv{}{v} \log Z(\vec{x}^{(i)}) \right] \\
	&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
	- \inv{Z(\vec{x}^{(i)})} \sum_{\slice[T]{y}}   
	\left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
	\sum_t \frac{\pderiv{}{v} \sum_{\vec[t]{z}}  \psi_t }{ \sum_{\vec[t]{z}} \psi_t   }
	\right]
	\right] \\ 
	&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
	- \inv{Z(\vec{x}^{(i)})} \sum_{\slice[T]{y}}   
	\left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
	\sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
	\right]
	\right] \\
	&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
	- \sum_t \sum_y \sum_{y'} \left[\sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right] \xi_{t,y,y'}
	\right] \\
	&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
	- \sum_t \sum_y \left[\sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right] \gamma_{t,y}
	\right] \\
	&= - \sum_{i=1}^{N} \left[ \sum_t \left( \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
	- \sum_y \left[\sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right] \gamma_{t,y}
	\right) \right] \\
	&= - \sum_{i=1}^{N} \left[ \sum_t \sum_y \left(    \left(
	\ind_{y_t = y} -  \gamma_{t,y} \right) 
	\sum_h   \sigma\left(o(h, y_t)\right) \pderiv{o(h, y_t)}{v_{dat}} \right) \right] 
	\end{align}
	NOTE: Although I haven't thoroughly checked the last few steps, they are required to be true in order to match the paper's results.
\end{example}



\begin{example}[Transition parameters]
	\begin{align}
	\pderiv{NLL}{v_{tr}} 
	&=  - \sum_{i=1}^{N} \left[ \sum_{t = 1}^{T} \pderiv{}{v_{tr}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
	- \pderiv{}{v_{tr}} \log Z(\vec{x}^{(i)}) \right] \\
	&= -\sum_i^N \left[ 
	\sum_t \pderiv{}{v_{tr}} \left[ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right]
	- \pderiv{}{v_{tr}} \log Z(\vec{x}^{(i)}) 
	\right]\\
	&= - \sum_{i=1}^{N} \left[ \sum_t  \pderiv{}{v_{tr}} \left[ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right]
	- \inv{Z(\vec{x}^{(i)})} \sum_{\slice[T]{y}}   
	\left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
	\sum_t  \pderiv{}{v_{tr}} \left[  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right]
	\right]
	\right] \\
	&= - \sum_{i=1}^{N} \left[ \sum_t \sum_y \left(    \left(
	\ind_{y_t = y} -  \gamma_{t,y} \right) 
	\pderiv{}{v_{tr}} \left[  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right] 
	\right)\right] 
	\end{align}
\end{example}



\begin{example}[Boundary parameters]
	\begin{align}
	\pderiv{NLL}{\pi_\ell} 
	&=  - \sum_{i=1}^{N} \left[ \ind_{y_1 = \ell} - \gamma_{1, \ell} \right] \\
	\pderiv{NLL}{\tau_\ell} 
	&=  - \sum_{i=1}^{N} \left[ \ind_{y_T = \ell} - \gamma_{T, \ell} \right]
	\end{align}
\end{example}


The results of each of the boxes above are summarized below, for the case of $N=1$ to save space.
\graybox{
	\pderiv{NLL}{v_{dat}} 
	&= -\sum_t \sum_y \left( \left(
	\ind_{y_t = y} -  \gamma_{t,y} \right) 
	\sum_h   \sigma\left(o(h, y)\right) \pderiv{o(h, y)}{v_{dat}} \right) \\
	\pderiv{NLL}{v_{tr}} 
	&= -\sum_t \sum_y \left(    \left(
	\ind_{y_t = y} -  \gamma_{t,y} \right) 
	\pderiv{}{v_{tr}} \left[  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right] 
	\right) \\
	\pderiv{NLL}{\pi_\ell} 
	&=  - \sum_{i=1}^{N} \left[ \ind_{y_1 = \ell} - \gamma_{1, \ell} \right] \\
	\pderiv{NLL}{\tau_\ell} 
	&=  - \sum_{i=1}^{N} \left[ \ind_{y_T = \ell} - \gamma_{T, \ell} \right]
}

Now I'll further go through and show how the equations simplify for each type of data-dependent parameter.
\begin{align}
\pderiv{NLL}{W_{c,h}}
&= -\sum_t \sum_y \left( \left(
\ind_{y_t = y} -  \gamma_{t,y} \right) 
\sum_{h'}   \sigma\left(o(h', y)\right) \pderiv{}{W_{c,h}} \left( b_{h'} + c_y + V_{h',y} + \vec[h']{w}^T \vec[t]{x}  \right) \right) \\
&= -\sum_t \sum_y \left( \left(
\ind_{y_t = y} -  \gamma_{t,y} \right) 
\sum_{h'}  \sigma\left(o(h, y)\right) \ind_{h=h'}\ind_{c \in \vec[t]{x}} \right) \\
&= -\sum_t \sum_y \left( 
\ind_{y_t = y} -  \gamma_{t,y} \right) 
\sigma\left(o(h, y)\right) \ind_{c \in \vec[t]{x}}  \\
\pderiv{NLL}{V_{h,y}} 
&= -\sum_t \left( 
\ind_{y_t = y} -  \gamma_{t,y} \right) 
\sigma\left(o(h, y)\right)   \\
\pderiv{NLL}{b_h}
&= -\sum_t \sum_y \left( 
\ind_{y_t = y} -  \gamma_{t,y} \right) 
\sigma\left(o(h, y)\right)  \\
\pderiv{NLL}{c_y}
&= -\sum_t  \left( 
\ind_{y_t = y} -  \gamma_{t,y} \right) 
\sum_h \sigma\left(o(h, y)\right)   \\
\end{align}


\myspace
\p \blue{Alternative Approach}. The above was a bit more cumbersome than needed. I'll now derive it in an easier way. 

\begin{small}
	\begin{align}
	p(\vec y \mid \vec x) 
	&= \frac{\exp\left\{ \vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau}  \right\}}{Z(\vec x)} \prod_{t=1}^{T}\bigg[ 
	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}  
	\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)	\bigg] \\
	&= \frac{\exp\{ I + T \}}{Z(\vec x)} \prod_{t=1}^{T}
	\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)
	\\
	NLL 
	&= - \sum_{i=1}^{N} \log p(\vec{y}^{(i)} \mid \vec{x}^{(i)}) \\
	&= - \sum_{i=1}^{N} \left[ 
	I + T + 
	\sum_{t = 1}^{T} \sum_{h=1}^{H} \left[ \log \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right) \right]
	- \log Z(\vec{x}^{(i)})
	\right]
	\end{align}
\end{small}

Now, focusing on the regular log-likelihood for a single example, we have
\begin{align}
\pderiv{\mathcal{L}_i}{v}
&= \pderiv{}{v} \log p(\vec{y}^{(i)}, \vec{x}^{(i)}) \\
&= \pderiv{}{v} \left[ 
I + T + 
\sum_{t, h} \log\left( 1 + \exp\left\{
b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y} 
\right\}\right) - 
\log Z(\vec{x}^{(i)})
\right] \\
\pderiv{\log Z(\vec{x}^{(i)})}{v} 
&= \inv{Z(\vec{x}^{(i)})}   \sum_{\slice[T]{y}} \pderiv{}{v} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)}) \\
&= \inv{Z(\vec{x}^{(i)})}   \sum_{\slice[T]{y}} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)}) \pderiv{}{v} \log\widetilde{p}(\vec{y} \mid \vec{x}^{(i)})
\end{align}
as our base formula for partial derivatives. 

\begin{example}[Transition parameters]
	\begin{align}
	\pderiv{\mathcal{L}_i}{A_{i,j}} 
	&= \pderiv{}{A_{i,j}} \left[ 
	I + T + 
	\sum_{t, h} \log\left( 1 + \exp\left\{
	b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y} 
	\right\}\right) - 
	\log Z(\vec{x}^{(i)})
	\right] \\
	&= \pderiv{}{A_{i,j}} \left[ I + T \right]
	- \sum_{\slice[T]{y}}  p(\vec{y} \mid \vec{x}^{(i)}) \pderiv{}{A_{i,j}} \left[ 
	y_1^T \pi + y_T^T \tau + \sum_t y_{t-1}Ay_t \right] \\
	&= \sum_t \ind_{y_{t-1}^{(i)}=i} \ind_{y_{t}^{(i)}=j}
	- \sum_{\slice[T]{y}} \inv{Z(\vec{x}^{(i)})} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)}) \sum_{t=1}^{T} \ind_{y_{t-1}=i} \ind_{y_{t}=j} \\
	&= \sum_t \ind_{y_{t-1}^{(i)}=i} \ind_{y_{t}^{(i)}=j}
	- \sum_t \sum_{y_t} \sum_{y_{t-1}}   \ind_{y_{t-1}=i} \ind_{y_{t}=j} \sum_{ \slice[t-2][1]{y} } \sum_{  \slice[T][t+1]{y}} \inv{Z(\vec{x}^{(i)})} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)})
	\end{align}
\end{example}




% ============================================================================================
\lecture{Conditional Random Fields}{Pre-training of Hidden-Unit CRFs}{June 30, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Kim et al., ``Pre-training of Hidden-Unit CRFs,'' (2018).}


\p \blue{Model Definition}. The Hidden-Unit CRF (HUCRF) accepts the usual observation sequence $\vec x = x_1, \ldots, x_n$, and associated label sequence $\vec y = y_1, \ldots y_n$ for training. The HUCRF also has a hidden layer of binary-valued $\vec z = z_1 \ldots z_n$. It defines the joint probability
\graybox{
	p_{\theta, \gamma}(\vec y, \vec z \mid \vec x) &= \dfrac{
		\exp\big( \vec{\theta}^T \Phi(\vec x, \vec z) + \vec{\gamma}^T \Psi(\vec z, \vec y)  \big)
	}{
		\sum_{\vec{z}', \vec{y}' \in \mathcal{Y}(\vec x, \vec{z}')} 
		\exp\big( \vec{\theta}^T \Phi(\vec x, \vec{z}') + \vec{\gamma}^T \Psi(\vec{z}', \vec{y}')  \big)
	}
}
where
\begin{compactitem}
	\item $\mathcal{Y}(\vec x, \vec z)$ is the set of all possible label sequences for $\vec x$ and $\vec z$. 
	
	\item $\Phi(\vec x, \vec z) = \jnsum \phi(x, j, z_j)$
	
	\item $\Psi(\vec z, \vec y) = \jnsum \psi(z_j, y_{j-1}, y_j)$. 
\end{compactitem}
Also note that we model $(z_i \perp z_{j \ne i} \mid \vec{x}, \vec{y})$. 

\myspace 
\p \blue{Pre-training HUCRFs}. Since the objective for HUCRFs is non-convex, we should choose a better initialization method than random initialization. This is where pre-training comes in, a simple 2-step approach:
\begin{compactenum}
	\item Cluster observed tokens from $M$ unlabeled sequences and treat the clusters as labels to train an intermediate HUCRF. Let $C(u^{(i)})$ be the sequence of cluster assignments/labels for the unlabeled sequence $u^{(i)}$. We compute:
	\begin{align}
	(\theta_1, \gamma_1) &\approx \argmax_{\theta, \gamma} \sum_{i = 1}^{M} \log p_{\theta, \gamma}(C(u^{(i)}) \mid u^{(i)} )
	\end{align}
	
	\item Train a final model on the labeled data $\{  (x^{(i)}, y^{(i)}  ) \}_{i = 1}^{N}$, using $\theta_1$ as an initialization point:
	\begin{align}
	(\theta_2, \gamma_2) &\approx \argmax_{\theta, \gamma} \sum_{i=1}^{N} \log p_{\theta, \gamma}(y^{(i)} \mid x^{(i)})
	\end{align}
\end{compactenum}
Note that pre-training only defines the initialization for $\theta$, the parameters between $\vec x$ and $\vec z$. We still train $\gamma$, the parameters from $\vec{z}$ to $\vec y$, from scratch. 

\myspace
\p \blue{Canonical Correlation Analysis} (CCA). A general technique that we will need to understand as a prerequisite for the multi-sense clustering approach (defined in the next section). Given $n$ samples of the form $(x^{(i)}, y^{(i)})$, where each $x^{(i)} \in \{0, 1\}^d$ and $y^{(i)} \in \{0, 1\}^{d'}$, CCA returns \textit{projection matrices} $A \in \R^{d \times k}$ and $B \in \R^{d' \times k}$ that we can use to project the samples to $k$ dimensions:
\begin{align}
x &\longrightarrow A^T x \\
y &\longrightarrow B^T y 
\end{align}
The CCA algorithm is outlined below.
\begin{example}[Algorithm: CCA]
	\begin{compactenum}
		\item Calculate $\matr{D} \in \R^{d \times d'}$, $\vec u \in \R^d$, and $\vec v \in \R^{d'}$ as follows:
		\begin{align}
		D_{i,j} &= \sum_{l = 1}^{n} \ind_{x_i^{(l)} = 1}  \ind_{y_j^{(l)} = 1} \\
		u_i &= \sum_{l = 1}^{n}  \ind_{x_i^{(l)} = 1} \\
		v_i &= \sum_{l = 1}^{n}  \ind_{y_i^{(l)} = 1}
		\end{align}
		
		\item Define $\hat{\Omega} = \text{diag}(\vec u)^{-1/2} ~ \matr{D} ~ \text{diag}(\vec v)^{-1/2}$. 
		
		\item Calculate rank-$k$ SVD $\hat{\Omega}$. Let $\matr U \in \R^{d \times k}$ and $\matr V \in \R^{d' \times k}$ contain the left and right, respectively, singular vectors for the largest $k$ singular values. 
		
		\item Return $A = \text{diag}(\vec u)^{-1/2} \matr U$ and $B = \text{diag}(\vec v)^{-1/2} \matr V$. 
	\end{compactenum}
\end{example}

\myspace
\p \blue{Multi-sense clustering}. For each word type, use CCA to create a set of context embeddings corresponding to all occurrences of that word type. Then, cluster these embeddings with $k$-means. Set the number of word senses $k$ to the number of label types occurring in the labeled data.\\



% ============================================================================================
\lecture{Conditional Random Fields}{Neural Conditional Random Fields}{July 08, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Do and Artieres, ``Neural Conditional Random Fields,'' (2010).}


\p \blue{Neural CRFs}. Essentially, we feed the input sequence $\vec x$ through a feed-forward network whose output layer has a linear activation function. The output layer is then connected with the target variable sequence $\rvec Y$. In other words, instead of feeding instances $\vec x$ of the observation variables $\rvec X$, we feed the hidden layer activations of the NN. This results in the conditional probability\marginnote{We can set $\Phi_c = \Phi$ for a shared-weights approach}[2em]
\graybox{
	p(\vec y \mid \vec x) &\propto \prod_{c \in C} e^{-E_c(\vec x, \vec[c]{y}, \vec w) }
	= \prod_{c \in C} e^{  \langle \vec[c]{w}^{\vec[c]{y}}, \vec[c]{\Phi}(\vec x, \vec[NN]{w})   \rangle   }
}
where
\begin{compactitem}
	\item $\vec[NN]{w}$ are the weights for the NN. 
	\item $\vec[c]{w}^{\vec[c]{y}}$ are the weights (for clique $c$) for the CRF. 
	\item $ \vec[c]{\Phi}(\vec x, \vec[NN]{w}) $ is the output of the NN. It symbolizes the high-level feature representation of the input $\vec x$ at clique $c$ computed by the NN. 
\end{compactitem}
The authors refer to the linear output layer (containing the CRF weights) as the \textit{energy outputs}. For the sake of writing this in more familiar notation for the linear-chain CRF case, here is the above equation translated for the case where  each clique corresponds to a timestep $t$ of the input sequence and is either a label-label clique or a state-label clique. 
\begin{align}
p(\vec y \mid \vec x) &= \inv{Z(x)} \prod_t^T \exp\left\{ - E_t(\vec x, \vec[t]{y}, \vec[t-1]{y}, \vec w) \right\} \\
&= \inv{Z(x)} \prod_t^T \exp\left\{ -E_{loc}(\vec x, t, y_t, \vec w) - E_{trans}(\vec x, t, y_{t-1}, y_t, \vec w)  \right\} \\
\end{align}
where the authors are using a blanket $\vec w$ to denote all model parameters\footnote{Also note that the authors allow for utilizing the input sequence $\vec x$ in the transition energy function, $E_{trans}$, although usually we implement $E_{trans}$ using only $y_{t-1}$ and $y_{t}$.}.


\myspace
\p \blue{Initialization \& Fine-Tuning}. The hidden layers of the NN are initialized layer-by-layer in an unsupervised manner using RBMs. It's important to note that the hidden layers of the NN consist of binary units. Then, using the pre-trained hidden layers, the CRF layer is initialized by training it in the usual way, and keeping the pretrained NN weights fixed. \\

Next, fine-tuning is used to learn all parameters globally.
\begin{align}
\pderiv{L(\vec w)}{\vec w} 
&= \inv{n}\sum_{i=1}^n \pderiv{L_i(\vec w)}{\vec w} \\
\pderiv{L_i(\vec w)}{\vec w} 
&= \pderiv{L_i(\vec w)}{\matr{E}(\vec{x}^{(i)})} \pderiv{\matr{E}(\vec{x}^{(i)})}{\vec w} \\
\left[ \matr{E}(\vec{x}^{(i)}) \right]_t	
&= E_{loc}(\vec x, t, y_t, \vec w) + E_{trans}(\vec x, t, y_{t-1}, y_t, \vec w) \\
\end{align}
where $\pderiv{\matr[i]{E}}{\vec w}$ is the Jacobian matrix of the NN outputs for input sequence $\vec{x}^{(i)}$ w.r.t. weights $\vec w$. By setting $\pderiv{L_i(\vec w)}{\matr[i]{E}}$ as backprop errors of the NN output units, we can backpropagate and get $\pderiv{L_i(\vec w)}{\vec w}$ using the chain rule over the hidden layers.




% ============================================================================================
\lecture{Conditional Random Fields}{Bidirectional LSTM-CRF Models for Sequence Tagging}{July 08, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Huang et al., ``Bidirectional LSTM-CRF Models for Sequence Tagging,'' (2015).}

\p \blue{BI-LSTM-CRF Networks}. Consider the matrix of scores $f_{\theta}(\vec x)$ for input sentence $\vec x$. The element $\left[ f_{\theta} \right]_{\ell, t}$ gives the score for label $\ell$ with the $t$-th word. This is output by the LSTM network parameterized by $\theta$. We let $[A]_{i,j}$ denote the transition score from label $i$ to label $j$ within the CRF. The total set of parameters is denoted $\widetilde{\theta} = \theta \cup \matr{A}$. The total score for input sentence $\vec x$ and predicted label sequence $y$ is then 
\begin{align}
s(\vec x, \vec y, \widetilde{\theta}) &= \sum_t^T \left(   A_{y_{t-1}, y_t} + \left[ f_{\theta}  \right]_{y_t, t}  \right)
\end{align}


\myspace
\p \blue{Features}. The authors incorporate 3 distinct types of input features:
\begin{compactitem}
	\item \textbf{Spelling features}. Various standard lexical/syntactical features. One-hot encoded as usual.
	\item \textbf{Context features}. Unigram, bigram, and sometimes tri-gram features. One-hot encoded.
	\item \textbf{Word embeddings}. Distinct from the word features. Use a pretrained embedding for each word. 
\end{compactitem}
Although it's not entirely clear, it appears they concatenate all of the aforementioned features together as input to the BI-LSTM. This necessarily means they are learning an embedding for the one-hot encoded spelling and word features. They also add direct connections from the input to the CRF for the spelling and word features. \\

EDIT: they may actually replace the one-hot encoded word features with the word embeddings. Unclear.



% ============================================================================================
\lecture{Conditional Random Fields}{On Herding and the Perceptron Cycling Theorem}{July 31, 2018}
% ============================================================================================

\vspace{-1em}
{\footnotesize Gelfand et al., ``On Herding and the Perceptron Cycling Theorem,'' (2010).}


\p \blue{Introduction}. Begin with the familiar learning rule of Rosenblatt's perceptron, after some \textit{incorrect} prediction $\hat{y_i} = \text{sgn}(\vec{w}^T \vec[i]{x})$,
\begin{align}
\vec{w} \leftarrow \vec{w} + \vec[i]{x}(y_i - \hat{y_i})
\end{align}
which has the effect that a subsequent prediction on $\vec[i]{x}$ will (before taking the sign) be $||\vec[i]{x}||_2^2$ closer to the correct side of the hyperplane. The \green{perceptron cycling theorem} (PCT) states that if the data is \textit{not} linearly separable, the weights will still remain bounded and won't diverge to infinity. \textbf{This paper shows that the PCT implies that certain moments are conserved on average}. Formally, their result says that, for some $N$ number of iterations over samples selected from training data (with replacement)\footnote{Unclear whether this is only for samples that corresponded to an update, or just all samples during training.},
\graybox{
	\bigg|\bigg| 
	\inv{N} \sum_{i}^{N} \vec[i]{x} y_i - \inv{N} \sum_i^N \vec[i]{x}\hat{y_i}
	\bigg|\bigg|	
	\sim 
	\mathcal{O}\left(\inv{N}\right)
}
where it's important to remember that $\hat{y_i}$ here is the prediction for $\vec[i]{x}$ when it was encountered at that training iteration. This result shows that perceptron learning generate predictions that correlate with the input attributes the same way as the true labels do, and [the correlations] converge to the sample mean with a rate of 1/N. This also hints at why averaged perceptron algorithms (using the average of weights across training) makes sense, as opposed to just selected the best weights. This paper also shows that \textit{supervised perceptron algorithms and unsupervised herding algorithms can all be derived from the PCT}. \\

Below are some theorems that will be used throughout the paper. Let $\{\vec[t]{w}\}$ be a sequence of vectors $\vec[t]{w} \in \R^{D}$, each generated according to iterative updates $\vec[t+1]{w} = \vec[t]{w} + \vec[t]{v}$, where $\vec[t]{v}$ is an element of a \textit{finite} set $\matr V$, and the norm of $\vec[t]{v}$ is bounded: $\max ||\vec[t]{v}|| = R < \infty$. 
\begin{definition}
	\green{PCT}: $\forall t \ge 0$: If $\vec[t]{w}^T \vec[t]{v} \le 0$, $\exists M > 0$ s.t. $||\vec[t]{w} - \vec[0]{w}|| < M$. \\
	\green{Convergence Thm}: If PCT holds, then $||\tfrac{1}{T} \sum_{t=1}^T \vec[t]{v} || \sim \mathcal{O}{\tfrac{1}{T}}$. 
\end{definition}


\myspace
\p \blue{Herding}. Consider a fully observed Markov Random Field (MRF) over $m$ variables, each of which can take on an integer value in the range $[1, K]$. In herding, our energy function and weight updates for observation $\vec x$ (over all $m$ variables in $\mathcal X$),
\graybox{
	E(\vec x) 
	&= - \vec{w}^T \phi(\vec x) \\
	\vec[t+1]{w} 
	&= \vec[t]{w} + \bar{\phi} - \phi(\vec[t]{x}^*) \\ 
	\text{where}\qquad \bar{\phi} 
	&= \E[\vec{x}^{(i)} \sim p_{data}]{\phi(\vec{x}^{(i)})} \\
	\text{and}\qquad
	\vec[t]{x}^* &= \argmax_{\vec x} \vec[t]{w}^T \phi(\vec x)
}
What if we consider more complicated features that depend on the weights $\vec w$? This situation may arise in e.g. models with hidden units $\vec z$, where our feature function would take the form $\phi(\vec x, \vec z)$, and we always select $\vec z$ via
\begin{align}
\vec{z}(\vec x, \vec w)
&= \argmax_{\vec{z}'} \vec{w}^T \phi(\vec x, \vec{z}')
\end{align}
and therefore our feature function $\phi$ depends on weights $\vec w$ through $\vec z$. In this case, our herding update terms from above take the form
\begin{align}
\bar{\phi}
&= \E[\vec{x}^{(i)} \sim p_{data}]{\phi(\vec{x}^{(i)}, \vec{z}(\vec{x}^{(i)}, \vec{w})  )} \\
\vec[t]{x}^*, \vec[t]{z}^*
&= \argmax_{\vec x, \vec z} \vec[t]{w}^T \phi(\vec x, \vec z)
\end{align}

\myspace
\p \blue{Conditional Herding}. Main contribution of this paper. It's basically identical to regular herding, but now we decompose $\vec x$ into inputs and outputs $(\vec x, \vec y)$ for interpreting in a discriminative setting. In the paper, they express $\vec{w}^T \phi(\vec x, \vec y, \vec z)$ identically as a discriminative RBM. The parameter update for mini-batch $\mathcal{D}_t$ is given by 
\graybox{
	\vec[t+1]{w} 
	&= \vec[t]{w} + \frac{\vec\eta}{|\mathcal{D}_t|}	
	\sum_{(\vec{x}^{(i)}, \vec{y}^{(i)}) \in \mathcal{D}_t} \left(
	\phi(\vec{x}^{(i)}, \vec{y}^{(i)}, \vec{z}) -
	\phi(\vec{x}^{(i)}, \vec{y}^*, \vec{z}^*)
	\right)
}













% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Structured Prediction}\label{Structured Prediction}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------




% ============================================================================================
\lecture{Structured Prediction}{Joint Event Extraction via RNNs}{Oct 31, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Nguyen, Cho, and Grishman, ``Joint Event Extraction via Recurrent Neural Networks,'' (2016).}

\p \blue{Event Extraction Task}. Automatic Context Extraction (ACE) evaluation. Terminology:
\begin{compactitem}
	\item \green{Event}: something that happens or leads to some change of state.
	\item \green{Mention}: phrase or sentence in which an event occurs, including one trigger and an arbitrary number of arguments.
	\item \green{Trigger}: main word that most clearly expresses an event occurrence. 
	\item \green{Argument}: an entity mention, temporal expression, or value that serves as a participant/attribute with a specific role in an event mention.
\end{compactitem}

Example:\marginnote{\red{TRIGGER}\\\green{ARGUMENT}}[3em]
\begin{quote}
	In Baghdad, a \green{cameraman} \red{died}\{\textit{Die}\} when an American tank \red{fired}\{\textit{Attack}\} on the Palestine hotel.
\end{quote}
Each event subtype has its own set of roles to be filled by the event arguments. For example, the roles for the \textit{Die} event subtype include \textit{Place}, \textit{Victim}, and \textit{Time}. 

\myspace
\p \blue{Model}. 
\begin{compactitem}[-]
	\item \textbf{Sentence Encoding}. Let $w_i$ denote the $i$th token in a sentence. It is transformed into a real-valued vector $x_i$, defined as
	\begin{align}
	x_i := \left[ \text{GloVe}(w_i);~ \text{Embed}(\text{EntityType}(w_i));~ \text{DepVec}(w_i)    \right]
	\end{align}
	where ``Embed'' is an embedding we learn, and ``DepVec'' is the binary vector whose dimensions correspond to the possible relations between words in the dependency trees. 
	
	\item \textbf{RNN}. Bidirectional LSTM on the inputs $x_i$. 
	
	\item \textbf{Prediction}. Binary memory vector $G_i^{trg}$ for triggers; binary memory matrices $G_i^{arg}$ and $G_i^{arg/trg}$ for arguments (at each timestep i). At each time step $i$, do the following in order:
	\begin{compactenum}
		\item \underline{Predict trigger} $t_i$ for $w_i$. First compute the feature representation vector $R_i^{trig}$, defined as:
		\begin{align}
		R_i^{trig} &:= \left[ h_i;~ L_i^{trg};~ G_{i-1}^{trg} \right]
		\end{align}
		where $h_i$ is the RNN output, $L_i^{trg}$ is the local context vector for $w_i$, and $G_{i-1}^{trg}$ is the memory vector from the previous step. $L_i^{trg} := [\text{GloVe}(w_{i-d}); \ldots; \text{GloVe}(w_{i + d})]$ for some predefined window size $d$. This is then fed to a fully-connected layer with softmax activation, $\vec{F}^{trg}$, to compute the probability over possible trigger subtypes:
		\begin{align}
		P_{i;t}^{trg} &:= F_t^{trg}(R_i^{trg})
		\end{align}
		As usual, the predicted trigger type for $w_i$ is computed as $t_i = \argmax_t \left( P_{i;t}^{trg} \right)$. If $w_i$ is not a trigger, $t_i$ should predict ``\textit{Other}.''
		
		\item \underline{Argument role predictions}, $a_{i1}, \ldots, a_{ik}$, for all of the [already known] entity mentions in the sentence, $e_1, \ldots, e_k$ with respect to $w_i$. $a_{ij}$ denotes the argument role of $e_j$ with respect to [the predicted trigger of] $w_i$. If NOT($w_i$ is trigger AND $e_j$ is one of its arguments), then $a_{ij}$ is set to \textit{Other}. For example, if $w_i$ was the word ``died'' from our example sentence, we'd hope that its predicted trigger would be $t_i = Die$, and that \textbf{the entity associated with ``cameraman'' would get a predicted argument role of \textit{Victim}.}
		
		\lstset{language=Python}
		\begin{lstlisting}
		def getArgumentRoles(triggerType=t, entities=e):
		k = len(e)
		if isOther(t):
		return [Other] * k 
		else:
		for e_j in e:
		
		\end{lstlisting}
		
		\begin{align}
		R_{ij}^{arg} &:= \left[ h_i;~ h_{i_j};~ L_{ij}^{arg};~ B_{ij};~ G_{i-1}^{arg}[j]; G_{i-1}^{arg/trg}[j] \right]
		\end{align}
		
		\item \underline{Update memory}. TO BE CONTINUED...(moving onto another paper because this model is getting a \textit{bit} too contrived for my tastes. Also not a fan of the reliance on a dependency parse.)
	\end{compactenum}
\end{compactitem}


\begin{comment}
% ============================================================================================
\lecture{Miscellaneous}{Zero-Shot Learning for Event Extraction}{Oct 31, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize L. Huang, H. Ji, K. Cho, and C. Voss, ``Zero-Shot Learning for Event Extraction,'' (2017).}
\end{comment}



% ============================================================================================
\lecture{Structured Prediction}{Event Extraction via Bidi-LSTM Tensor NNs}{Oct 31, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Y. Chen, S. Liu, S. He, K. Liu, and J. Zhao, ``Event Extraction via Bidirectional Long Short-Term Memory Tensor Neural Networks.''}

\p \blue{Overview}. The task/goal is the event extraction task as defined in \textit{Automatic Content Extraction} (ACE). Specifically, given a text document, our goal is to do the following in order \textit{for each sentence}:
\begin{compactenum}
	\item Identify any event triggers in the sentence.
	\item If triggers found, predict their subtype. For example, given the trigger ``fired,'' we may classify it as having the \textit{Attack} subtype.
	\item If triggers found, identify their candidate argument(s). ACE defines an event argument as ``an entity mention, temporal expression, or value that is involved in an event.''
	\item For each candidate argument, predict its role: ``the relationship between an argument to the event in which it participates.''
\end{compactenum}

\myspace
\p \blue{Context-aware Word Representation}. Use pre-trained word embeddings for the input word tokens, the predicted trigger, and the candidate argument. Note: \textit{we assume we already have predictions for the event trigger $t$ and are doing a pass for one of (possibly many) candidate arguments $a$.}


\begin{compactenum}
	\item Embed each word in the sentence with pre-trained embeddings. Denote the embedding for $i$th word as $e(w_i)$. 
	
	\item Feed each $e(w_i)$ through a bidirectional LSTM. Denote the $i$th output of the forward LSTM as $c_l(w_{i+1})$ and the output of the backward LSTM at the same time step as $c_r(w_{i - 1})$. As usual, they take the general functional form:
	\begin{align}
	c_l(w_i) &= \overrightarrow{LSTM}\left(c_l(w_{i - 1}), e(w_{i-1}) \right) \\
	c_r(w_i) &= \overleftarrow{LSTM}\left(c_r(w_{i + 1}), e(w_{i+1}) \right) \\
	\end{align}
	
	\item Concatenate $e(w_i)$, $c_l(w_i)$, $c_r(w_i)$ together along with the embedding of the candidate argument $e(a)$ and predicted trigger $e(t)$. Also include the relative distance of $w_i$ to $t$ or (??) $a$, denoted as $pi$ for position information, and the embedding of the predicted event type $pe$ of the trigger. Denote this massive concatenation result as $x_i$:
	\begin{align}
	x_i := c_l(w_i) \oplus e(w_i) \oplus c_r(w_i) \oplus pi \oplus pe \oplus e(a) \oplus e(t) \label{fat-concat}
	\end{align}
	
\end{compactenum}

\myspace
\p \blue{Dynamic Multi-Pooling}. This is easiest shown by example. Continue with our example sentence:
\begin{quote}
	In California, \green{Peterson} was arrested for the \red{murder} of his wife and unborn son.
\end{quote}
where the colors are given for \textit{this specific case where murder is our predicted trigger and we are considering the candidate argument Peterson}\footnote{Yes, arrested could be another predicted trigger, but the network considers each possibility at separate times/locations in the architecture.}. Given our $n$ outputs from the previous stage, $y^{(1)} \in \R^{n \times m}$, where $n$ is the length of the sentence and $m$ is the size of that huge concatenation given in equation \ref{fat-concat}. We split our sentence by trigger and candidate argument, then (confusingly) redefine our notation as\marginnote{Peterson is the 3rd word, and murder is the 8th word.}[3em]
\begin{align}
{y}^{(1)}_{1j} &\leftarrow \begin{bmatrix} y^{(1)}_{1j} & y^{(1)}_{2j} \end{bmatrix} \\
{y}^{(1)}_{2j} &\leftarrow \begin{bmatrix} y^{(1)}_{3j} & \cdots & y^{(1)}_{7j} \end{bmatrix} \\
{y}^{(1)}_{3j} &\leftarrow \begin{bmatrix} y^{(1)}_{8j} & \cdots & y^{(1)}_{nj} \end{bmatrix} 
\end{align}
where it's important to see that, for some $1 \le j \le m$, each new $y^{(1)}_{ij}$ is a \textit{vector} of length equal to the number of words in segment $i$. Finally, the dynamic multi-pooling layer, $y^{(2)}$, can be expressed as 
\graybox{
	y^{(2)}_{i,j} &:= \max \left( y^{(1)}_{i,j}\right) \qquad 1 \le i \le 3, ~ 1 \le j \le m
}
where the max is taken over each of the aforementioned vectors, leaving us with $ 3m $ values total. These are concatenated to form $y^{(2)} \in \R^{3m}$. 


\myspace
\p \blue{Output}. To predict of each argument role [for the given argument candidate], $y^{(2)}$ is fed through a dense softmax layer,
\begin{align}
O &= W_2 y^{(2)} + b_2
\end{align}
where $W_2 \in \R^{n_1 \times 3m}$ and $n_1$ is the number of possible argument roles (including "None"). The authors also use dropout on $y^{(2)}$.



% ============================================================================================
\lecture{Structured Prediction}{Reasoning with Neural Tensor Networks}{Nov 2, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Socher et al., ``Reasoning with Neural Tensor Networks for Knowledge Base Completion''}

\p \blue{Overview}. Reasoning over relationships between two entities. Goal: predict the likely truth of additional facts based on existing facts in the KB. This paper contributes (1) the new NTN and (2) a new way to represent entities in KBs. Each relation is associated with a distinct model. Inputs to a given relation's model are pairs of database entities, and the outputs score how likely the pair has the relationship.

\myspace
\p \blue{Neural Tensor Networks for Relation Classification}. Let $e_1, e_2 \in \R^d$ be the vector representations of the two entities, and let $R$ denote the relation (and thus model) of interest. The NTN computes a score of how likely it is that $e_1$ and $e_2$ are related by $R$ via:\marginnote{$$ W_R^{[1:k]} \in \R^{d \times d \times k}$$ $$  V_R \in \R^{k \times 2d}$$}[1em]
\graybox{
	g(e_1, R, e_2) &= u_R^T \tanh\left( 
	e_1^T W_R^{[1:k]} e_2 + V_R \begin{bmatrix} e_1 \\ e_2 \end{bmatrix} + b_R
	\right)
}
where the bilinear tensor product $e_1^T W_R^{[1:k]} e_2$ results in a vector $h \in \R^k$ with each entry computed by one slice $i = 1,\ldots,k$ of the tensor.
\begin{quote}
	{\itshape Intuitively, we can see each slice of the tensor as being responsible for one type of entity pair or instantiation
		of a relation\textellipsis Another way to interpret each tensor slice is that it mediates the relationship between the two entity vectors differently.}
\end{quote}

\myspace
\p \blue{Training Objective and Derivatives}. All models are trained with \green{contrastive max-margin objective functions} and minimize the following objective:
\graybox{
	J(\matr{\Omega}) &= \sum_{i = 1}^{N} \sum_{c = 1}^{C} \max\left( 0,
	1 - g\left( T^{(i)} \right) + g\left( T_c^{(i)} \right) \right) + \lambda ||\matr{\Omega}||_2^2
}
where $c$ is for ``corrupted'' samples, $T_c^{(i)} := \left(e_1^{(i)}, R^{(i)}, e_c^{(i)}\right)$. Notice that this function is minimized when the difference, $g\left( T^{(i)} \right) - g\left( T_c^{(i)} \right)$, is maximized. The authors used minibatched \green{L-BFGS} for optimization.



% ============================================================================================
\lecture{Structured Prediction}{Language to Logical Form with Neural Attention}{Nov 6, 2017}
% ============================================================================================


\vspace{-1em}
{\footnotesize Dong and Lapata, ``Language to Logical Form with Neural Attention,'' (2016)}

\p \blue{Sequence-to-Tree Model}. Variant of Seq2Seq that is more faithful to the compositional nature of meaning representations. It's schematic is shown below. The authors define a ``nonterminal'' $<n>$ token which indicates [the root of] a subtree.

\myfig[0.5\textwidth]{Seq2Tree.png}

where the author's have employed ``parent-feeding'': for a given subtree (logical form), at each timestep, the hidden vector of the parent nonterminal is concatenated with the inputs and fed into the LSTM (best understood via above illustration). 

\begin{quote}
	{\itshape After encoding input $q$, the hierarchical tree decoder generates tokens at depth 1 of the subtree corresponding to parts of logical form $a$. If the predicted token is $<n>$, decode the sequence by conditioning on the nonterminal's hidden vector. This process terminates when no more nonterminals are emitted. }
\end{quote}

\Needspace{10\baselineskip}
Also note that the output posterior probability over the encoded input $q$ is the product of subtree posteriors. For example, consider the decoding example in the figure below:
\myfig[0.4\textwidth]{Seq2TreeExample.png}

We would compute the output posterior as:
\begin{align}
p(a \mid q) &= p(y_1 y_2 y_3 y_4 \mid q) p(y_5 y_6 \mid y_{\le 3},q)
\end{align}
The model is trained by minimizing log-likelihood over the training data, using RMSProp for optimization. At inference time, greedy search or beam search is used to predict the most probable output sequence.








% ============================================================================================
\lecture{Structured Prediction}{Seq2SQL: Generating Structured Queries from NL using RL}{Nov 6, 2017}
% ============================================================================================


\vspace{-1em}
{\footnotesize Zhong, Xiong, and Socher, ``Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning''}

\p \blue{Overview}. Deep neural network for translating natural language questions to corresponding SQL queries. Outperforms state-of-the-art semantic parser.

\myspace
\p \blue{Seq2Tree and Pointer Baseline}. Baseline model is the Seq2Tree model from the previous note on Dong \& Lapata's (2016) paper. Authors here argue their output space is unnecessarily large, and employ the idea of pointer networks with augmented inputs. The input sequence is the concatenation of (1) the column names, (2) the limited vocabulary of the SQL language such as \texttt{SELECT}, \texttt{COUNT}, etc., and (3) the question\marginnote{$$ x_j^c \in \R^{T_j}$$}[2em].
\begin{align}
x &:= \left[
<col>;~
x_1^c; x_2^c; \ldots; x_N^c;~
<sql>;~
x^s;~
<question>;~
x^q
\right]
\end{align}
where we also insert special (``sentinel'') tokens to demarcate the boundaries of each section. The pointer network can then produce the SQL query by selecting exclusively from the input. Let $g_s$ denote the $s$th decoder [hidden] state, and $y_s$ denote the output (index/pointer to input query token).
\graybox{
	\mtgreen{[ptr net]} ~
	y_s = \argmax_t \left( \alpha_s^{ptr} \right) \quad \text{where} \quad
	\alpha_{s,t}^{ptr} = w^{ptr} \cdot \tanh\left( U^{ptr} g_s + V^{ptr} h_t \right) \label{seq2sql-baseline}
}

\myspace
\p \blue{Seq2SQL}.
\begin{enumerate}
	\item \textbf{Aggregation Classifier}. Our goal here is to predict which aggregation operation to use out of \texttt{COUNT}, \texttt{MIN}, \texttt{MAX}, \texttt{NULL}, etc. This is done by projecting the attention-weighted average of encoder states, $\kappa^{agg}$, to $\R^{C}$ where $C$ denotes the number of unique aforementioned aggregation operations. The sequence of computations is summarized as follows:\marginnote{$$W^{agg} \in \R^{C \times T}$$ $$ \beta^{agg} \in \R^{C}$$}[4em]
	\begin{align}
	\alpha_t^{inp} 	&= w^{inp} \cdot h_t^{enc} \label{seq2sql-attn} \\
	\beta^{inp} 	&= \text{softmax}\left( \alpha^{inp} \right) \\
	\kappa^{agg} 	&= \sum_t^T \beta_t^{inp} h_t^{enc} \\
	\alpha^{agg} 	&= W^{agg} \tanh\left( V^{agg} \kappa^{agg} + b^{agg} \right) + c^{agg} \\
	\beta^{agg} 	&= \text{softmax}\left( \alpha^{agg} \right)
	\end{align}
	where $\beta_i^{agg}$ gives the probability for the $i$th aggregation operation. We use cross entropy loss $L^{agg}$ for determining the aggregation operation. Note that this part isn't really a sequence-to-sequence architecture. It's nothing more than an MLP applied to an attention-weighted average of the encoder states.
	
	
	\item \textbf{Get Pointer to Column}. A pointer network is used for identifying which column in the input representation should be used in the query. Recall that $x_{j,t}^c$ denotes the $t$th word in column $j$. We use the last encoder state for a given column's LSTM\footnote{Yes, we encode each column with an LSTM separately.} as its representation; $T_j$ denotes the number of words in the $j$th column.
	\begin{align}
	e_j^c = h_{j,T_j}^c \quad \text{where} \quad 
	h_{j,t}^c = \text{LSTM}\left( \text{emb}(x_{j,t}^c) , h_{j,t-1}^c \right)
	\end{align}
	To construct a representation for the question, compute another input representation $\kappa^{sel}$ using the same architecture (but distinct weights) as for $\kappa^{agg}$. As usual, we compute the scores for each column $j$ via:
	\begin{align}
	\alpha_j^{sel} &= W^{sel} \tanh\left( V^{sel} \kappa^{sel} + V^c e_j^c \right) \\
	\beta^{sel} &= \text{softmax}\left( \alpha^{sel} \right)
	\end{align}
	Similar to the aggregation, we train the \texttt{SELECT} network using cross entropy loss $L^{sel}$. 
	
	
	\item \textbf{WHERE Clause Pointer Decoder}. Recall from equation \ref{seq2sql-baseline} that this is a model with recurrent connections from its \textit{outputs leading back into its inputs}, and thus a common approach is to train it with \green{teacher forcing}\footnote{Teacher forcing is just a name for how we train the decoder portion of a sequence-to-sequence model, wherein we feed the \underline{ground-truth} output $y^{(t)}$ as input at time $t+1$ during training.}. However, since the boolean expressions within a \texttt{WHERE} clause can be swapped around while still yielding the same SQL query, reinforcement learning (instead of cross entropy) is used to \textit{learn a policy to directly optimize the expected correctness of the execution result}. Note that this also implies that we will be sampling from the output distribution at decoding step $s$ to obtain the next input for $s+1$ [instead of teacher forcing].
	
	\Needspace{19\baselineskip}
	\graybox{
		R\left( q(y), q_g \right) &= 
		\begin{cases}
			-2 & \text{if } q(y) \text{ is not a valid SQL query} \\
			-1 & \text{if } q(y) \text{ is a valid SQL query and executes to an incorrect result} \\
			+1 & \text{if } q(y) \text{ is a valid SQL query and executes to the correct result} 
		\end{cases} \\
		L^{whe} &= - \E[y]{ R\left( q(y), q_g \right) } \\
		\nabla L_{\Theta}^{whe}
		&= -\nabla_{\Theta}  \left( \E[ y \sim p_y ]{ R\left( q(y), q_g \right) } \right) \\
		&= - \E[ y \sim p_y ]{ R\left( q(y), q_g \right) \nabla_{\Theta} \sum_t \log p_y ( y_t ; \Theta ) } \\
		&\approx - R\left( q(y), q_g \right) \nabla_{\Theta} \sum_t   \log p_y ( y_t ; \Theta )
	}
	
	where 
	\begin{compactitem}[$\rightarrow$]
		\item $y = [y^1, y^2, \ldots, y^T]$ denotes the sequences of generated tokens in the \texttt{WHERE} clause.
		
		\item $q(y)$ denotes the query generated by the model.
		
		\item $q_g$ denotes the ground truth query corresponding to the question.
	\end{compactitem}
	and the gradient has been approximated in the last line using a single Monte-Carlo sample $y$. 
\end{enumerate}
Finally, the model is trained using gradient descent to minimize $L = L^{agg} + L^{sel} + L^{whe}$. 

\myspace
\p \blue{Speculations for Event Extraction}. I want to explore using this paper's model for the task of event extraction. Below, I've replaced some words (shown in green) from a sentence in the paper in order to formalize this as event extraction.
\begin{quote}
	{
		\itshape Seq2\green{Event} takes as input a \green{sentence} and the \green{possible event types} of an \green{ontology}. It generates the corresponding \green{event annotation}, which, during training, is \green{compared} against an \green{event template}. The result of the \green{comparison} is utilized to train the reinforcement learning algorithm\footnote{Original: Seq2SQL takes as input a question and the columns of a table. It generates the corresponding SQL query, which, during training, is executed against a database. The result of the execution is utilized as the reward to train the reinforcement learning algorithm.}.
	}
\end{quote}




% ============================================================================================
\lecture{Structured Prediction}{SLING: A Framework for Frame Semantic Parsing}{Nov 13, 2017}
% ============================================================================================


\vspace{-1em}
{\footnotesize M. Ringgaard, R. Gupta, F. Pereira, ``SLING: A framework for frame semantic parsing'' (2017)}

\myfig[0.6\textwidth]{sling.png}

\p \blue{Model}. 
\begin{compactitem}
	\item \textbf{Inputs}. [words; affixes; shapes]
	
	\item \textbf{Encoder}. 
	\begin{compactenum}
		\item Embed.
		\item Bidirectional LSTM. 
	\end{compactenum}
	
	\item \textbf{Inputs to TBRU}.
	\begin{compactitem}
		\item BLSTM [forward and backward] hidden state \underline{for the current token in the parser state}.
		
		\item \textbf{Focus}. Hidden layer activations corresponding to the transition steps that evoked/brought into \textit{focus} the top-$k$ frames in the attention buffer.
		
		\item \textbf{Attention}. Recall that we maintain an \green{attention buffer}: an ordered list of frames, where the order represents closeness to center of attention. The attention portion of inputs for the TBRU looks at the top-$k$ frames in the attention buffer, finds the phrases in the text (if any) that evoked them. The activations from the BLSTM for the last token of each of those phrases are included as TBRU inputs\footnote{Okay, how is this attention at all? Seems misleading to call it attention.}
		
		\item \textbf{History}. Hidden layer activations from the previous $k$ steps.
		
		\item \textbf{Roles}. Embeddings of $(s_i, r_i, t_i)$, where the frame at position $s_i$ in the attention buffer has a role (key) $r_i$ with frame at position $t_i$ as its value. Back-off features are added for the source roles $(s_i, r_i)$, target role $(r_i, t_i)$, and unlabeled roles $(s_i, t_i)$. 
	\end{compactitem}
	
	\item \textbf{Decoder} (TBRU). Outputs a softmax over possible transitions (actions). 
\end{compactitem}

\myspace
\p \blue{Transition System}. Below is the list of possible actions. Note that, since the system is trained to predict the correct \textit{frame graph} result, it isn't directly told what order it should take a given set of actions\footnote{This is important to keep in mind, since more than one sequence of actions can result in a given predicted frame graph.}.
\begin{compactitem}
	\item \textbf{SHIFT}. Move to next input token.
	\item \textbf{STOP}. Signal that we've reached end of parse.
	\item \textbf{EVOKE(type, num)}. New frame of \textbf{type} from next \textbf{num} tokens in the input; placed at front of attention buffer.
	\item \textbf{REFER(frame, num)}. New mention from next \textbf{num} tokens, evoking existing \textbf{frame} from attention buffer. Places at front.
	\item \textbf{CONNECT(source-frame, role, target-frame)}. Inserts \textbf{(role, target-frame)} slot into \textbf{source-frame}, move \textbf{source-frame} to front.
	\item \textbf{ASSIGN(source-frame, role, value)}. Same as CONNECT, but with primitive/constant \textbf{value}.
	\item \textbf{EMBED(target-frame, role, type)}. New frame of \textbf{type}, and inserts \textbf{(role, target-frame)} slot. New frame placed to front.
	\item \textbf{ELABORATE(source-frame, role, type)}. New frame of \textbf{type}. Inserts \textbf{(role, new-frame)} slot to \textbf{source-frame}. New frame placed at front.
\end{compactitem}

\myspace
\p \blue{Evaluation}. Need some way of comparing an annotated document with its gold-standard annotation. This is done by constructing a virtual graph where the document is the start node. It is then connected to the spans (which are presumably nodes themselves), and the spans are connected to the frames they evoke. Frames that refer to other frames are given corresponding edges between them. Quality is computed by aligning the golden and predicted graphs and computing precision, recall, and F1. Specifically, these scores are computed separately for spans, frames, frame types, roles linking to other frames (referred to here as just ``roles''), and roles that link to global constants (referred to here as just ``labels''). Results are shown below.
\myfig[0.6\textwidth]{sling_eval.png}


% ============================================================================================
\lecture{Structured Prediction}{DeepWalk: Online Learning of Social Representations}{Nov 17, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize B. Perozzi, R. Al-Rfou, and S. Skiena, ``DeepWalk: Online Learning of Social Representations,'' (2014).}

\p \blue{Problem Definition}. Classifying members of a social network into one or more categories. 
\vspace{-1em}
\begin{quote}
	{\itshape Let $G = (V, E)$, where $V$ are the members of the network, and $E$ be its edges, $E \subseteq (V \times V)$. Given a partially labeled social network $G_L = (V, E, X, Y)$, with attributes $X \in \R^{|V| \times S}$ where $S$ is the size of the feature space for each attribute vector, and $Y \in \R^{|V| \times |\mathcal{Y}|}$, $\mathcal{Y}$ is the set of labels.}
\end{quote}
\vspace{-1em}
In other words, the elements of our training dataset, $(X, Y)$, are the members of the social network, and we want to label each member, represented by a vector in $\R^S$, with one or more of the $|\mathcal{Y}|$ labels. We aim to learn features that capture the graph structure \textit{independent} of the labels' distribution, and to do so in an unsupervised fashion.

\myspace
\p \blue{Learning Social Representations}. We want the representations to be adaptable, community-aware, low-dimensional, and continuous. The authors' method learns representations for vertices from a stream of short random walks, optimized with techniques from language modeling.
\begin{compactitem}
	\item \textbf{Random Walks}. Denote a random walk rooted at vertex $v_i$ as $\mathcal{W}_{v_i}$, where the $k$th visited vertex is chosen at random from the neighbors of the $(k-1)^{th}$ visited vertex, and so on. Motivation for their use here is that they're ``the foundation of a class of \textit{output sensitive} algorithms which use them to compute local community structure information in time sublinear to the size of the input graph.''
	
	\item \textbf{Language Modeling}. Authors present a generalization of language modeling, which traditionally aims to maximize $\Prob{w_n \mid w_0, \ldots, w_{n-1}}$ over all words in a training corpus. The motivation of the generalization is to explore the graph through a stream of short random walks. The walks are thought of as short sentences/phrases in a special language, and we want to estimate the probability of observing vertex $v_i$ given all previous vertices so far in the random walk. Since we want to learn a latent social representation of each vertex, and not simply a probability distribution over node co-occurrences, we condition on the \textit{embeddings} of visited nodes in this latent space (rather than the nodes themselves directly)
	\begin{align}
	\Prob{v_i \mid \Phi(v_1), \Phi(v_2), \ldots, \Phi(v_{i - 1})} 
	\end{align}
	where, in practice, the mapping function $\Phi$ is represented by a $|V| \times d$ matrix of free parameters (an embedding matrix). Since this becomes infeasible to compute as the walk length grows, the authors opt for an approach resembling CBOW: minimizing the NLL of vertices in the the context of a given vertex.
	\begin{align}
	\min_{\Phi}~~ - \log \Prob{v_{i-w}, \ldots, v_{i-1}, v_{i+1}, \ldots, v_{i+w} \mid \Phi(v_i) }
	\end{align}
	Remember that, here, $v_j$ is the $j$th vertex visited in some given random walk. 
\end{compactitem}

\myspace
\p \blue{DeepWalk Algorithm}. Below is a conceptual summary of the procedure, followed by a figure/illustration of the formal algorithm definition.
\begin{compactenum}
	\item \textbf{Inputs}. Graph $G(V, E)$, window size $w$, embedding size $d$, walks per vertex $\gamma$, walk length $t$. 
	
	\item \textbf{Random Walk}. For each vertex $v_i$, compute $\mathcal{W}_{v_i} := RandomWalk(G, v_i, t)$. 
	
	\item \textbf{Updates}. Upon finishing a walk, $\mathcal{W}_{v_i}$, run skipgram on the sequence of walked vertices to update the embedding matrix $\Phi$. 
	
	\item \textbf{Outputs}. The embedding matrix $\Phi \in \R^{|V| \times d}$. 
\end{compactenum}

\myfig[0.6\textwidth]{deepwalk_algo.png}

where $SkipGram(\Phi, W_{v_i}, w)$ performs SGD updates on $\Phi$ to minimize $-\log\Prob{u_k \mid \Phi(v_j)}$ for each visited $v_j$, for each $u_k$ in the ``context'' of $v_j$. Notice that a binary tree $T$ is build from the set of vertices $V$ (line 2) -- this is done as preparation for computing each $\Prob{u_k \mid \Phi(v_j)}$ via a \textit{hierarchical softmax}, to reduce computational burden of its partition function. Finally, a visual overview of the DeepWalk algorithm is shown below.
\myfig[0.8\textwidth]{deepwalk_vis.png}
The authors use this algorithm, combined with a one-vs-rest logistic regression implementation by LibLinear, for various multiclass multilabel classification tasks.  



% ============================================================================================
\lecture{Structured Prediction}{Review of Relational Machine Learning for Knowledge Graphs}{Dec 6, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Nickel, Murphy, Tresp, and Gabrilovich, ``Review of Relational Machine Learning for Knowledge Graphs,'' (2015).}

\p \blue{Introduction}. Paper discusses latent feature models such as tensor factorization and multiway neural networks, and mining observable patterns in the graph. In \green{Statistical Relational Learning (SRL)}, the representation of an object can contain its relationships to other objects. The main goals of SRL include:
\begin{compactitem}
	\item Prediction of missing edges (relationships between entities).
	\item Prediction of properties of nodes.
	\item Clustering nodes based on their connectivity patterns.
\end{compactitem}
We'll be reviewing how SRL techniques can be applied to large-scale \green{knowledge graphs} (KGs), i.e. graph structured knowledge bases (KBs) that store factual information in the form of relationships between entities.

\myspace
\p \blue{Probabilistic Knowledge Graphs}. Let $\mathcal{E} = \{e_1, \ldots, e_{N_e} \}$ be the set of all entities and $\mathcal R = \{r_1, \ldots, r_{N_r}\}$ be the set of all relation types in a KG. We model each \textit{possible} triple $x_{ijk} = (e_i, r_k, e_j)$ as a binary random variable $y_{ijk} \in \{0, 1\}$ that indicates its existence. The full tensor $\matr Y \in \{0, 1\}^{N_e \times N_e \times N_r}$ is called the \textit{adjacency tensor}, where each possible realization of $\matr Y$ can be interpreted as a possible world.\\

\p Clearly, $\matr Y$ will be large and sparse in most applications. Ideally, a relational model for large-scale KGs should scale at most linearly with the data size, i.e., linearly in the number of entities $N_e$, linearly in the number of relations $N_r$, and linearly in the number of \textit{observed} triples $|\mathcal D| = N_d$.

\myspace
\p \blue{Types of SRL Modesls}. The presence or absence of certain triples in relational data is correlated with (i.e. predictive of) the presence or absence of certain other triples. In other words, the random variables $y_{ijk}$ are correlated with each other. There are three main ways to model these correlations:
\begin{compactenum}
	\item \textbf{Latent feature models}: Assume all $y_{ijk}$ are conditionally independent given latent features associated with the subject, object and relation type and additional parameters.
	
	\item \textbf{Graph feature models}: Assume all $y_{ijk}$ are conditionally independent given observed graph features and additional parameters.
	
	\item \textbf{Markov Random Fields}: Assume all $y_{ijk}$ have local interactions.
\end{compactenum}
The first two model classes predict the existence of a triple $x_{ijk}$ via a score function $f(x_{ijk}; \Theta)$ which represents the model's confidence that a triple exists given the parameters $\Theta$. The conditional independence assumptions can be written as
\graybox{
	\Prob{\matr Y \mid \mathcal D, \Theta}
	&= \prod_{i=1}^{N_e} \prod_{j=1}^{N_e} \prod_{k=1}^{N_r} \text{Ber}\left(
	y_{ijk} \mid \sigma\left(  f(x_{ijk}; \Theta)  \right)  \right)
}
where $\text{Ber}$ is the Bernoulli distribution\footnote{Notation used:\begin{align}
	\text{Ber}(y\mid p) = \begin{cases}
	p & \text{if } y = 1 \\
	1 - p & \text{if } y = 0
	\end{cases}
	\end{align}}. Such models will be referred to as \textit{probabilistic models}. We will also discuss \textit{score-based models}, which optimize $f(\cdot)$ via maximizing the margin between existing and non-existing triples.

\myspace
\p \blue{Latent Feature Models}. We assume the variables $y_{ijk}$ are conditionally independent given a set of global latent features and parameters. All LFMs explain triples (observable facts) via latent features of entities\footnote{It appears that ``latent'' is being used here synonymously with "not directly observed in the data".}. One task of all LFMs is to infer these [latent] features automatically from the data.
\begin{itemize}
	\item \textbf{RESCAL}: a bilinear model. Models the score of a triple $x_{ijk}$ as 
	\begin{align}
	f_{ijk}^{RESCAL}
	&:= \vec[i]{e}^T \matr[k]{W} \vec[j]{e} = \sum_{a = 1}^{H_e} \sum_{b = 1}^{H_e} w_{abk} e_{ia} e_{jb}
	\end{align}
	where the entity vectors $\vec[i]{e} \in \R^{H_e}$ and $H_e$ denotes the number of latent features in the model. The parameters of the model are $\Theta = \{ \{  \vec[i]{e} \}_{i=1}^{N_e}, \{ \matr[k]{W} \}_{k = 1}^{N_r} \}$. Note that entities have the same latent representation regardless of whether they occur as subjects or objects in a relationship (shared representation), \textit{thus allowing the model to capture global dependencies in the data}. We can make a connection to \green{tensor factorization} methods by seeing that the equation above can be written compactly as 
	\begin{align}
	\matr[k]{F} &= \matr{E} \matr[k]{W} \matr{E}^T
	\end{align}
	where $\matr[k]{F} \in \R^{N_e \times N_e}$ is the matrix holding all scores for the $k$-th relation, and the $i$th row of $\matr{E} \in \R^{N_e \times H_e}$ holds the latent representation of $\vec[i]{e}$.
	
	\item \textbf{Multi-layer perceptrons}. We can rewrite RESCAL as
	\begin{align}
	f_{ijk}^{RESCAL} &:= \vec[k]{w}^T \vec[i,j]{\phi}^{RESCAL} \\
	\vec[i,j]{\phi}^{RESCAL} &:= \vec[j]{e} \otimes \vec[i]{e}
	\end{align}
	where $\vec[k]{w} = \text{vec}(\matr[k]{W})$ (vector of size $H_e^2$ obtained by stacking columns of $\matr[k]{W}$). The authors extend this to what they call the E-MLP (E for entity) model:
	\begin{align}
	f_{ijk}^{E-MLP} &:= \vec[k]{w}^T \vec{g}(\vec[ijk]{h}^a) \\
	\vec[ijk]{h}^a &:= \matr[k]{A}^T \vec[ij]{\phi}^{E-MLP} \\
	\vec[ij]{\phi}^{E-MLP} &:= [\vec[i]{e}; \vec[j]{e}]
	\end{align}
\end{itemize}


\myspace
\p \blue{Graph Feature Models}. Here we assume that the existence of an edge can be predicted by extracting features from the observed edges in the graph. In contrast to LFMs, this kind of reasoning explains triples directly from the observed triples in the KG.
\begin{compactitem}[-]
	\item \textbf{Similarity measures for uni-relational data}. Link prediction in graphs that consist only of a single relation (e.g. (Bob, isFriendOf, Sally) for a social network). Various \green{similarity indices} have been proposed to measure similarity of entities, of which there are three main classes:
	\begin{compactenum}
		\item \textbf{Local} similarity indices: Common Neighbors, Adamic-Adar index, Preferential Attachment derive entity similarities from number of common neighbors.
		
		\item \textbf{Global} similarity indices: Katz index, Leicht-Holme-Newman index (ensembles of all paths bw entities). Hitting Time, Commute Time, PageRank (random walks). 
		
		\item \textbf{Quasi-local} similarity indices: Local Katz, Local Random Walks.
	\end{compactenum}
	
	\item \textbf{Path Ranking Algorithm} (\green{PRA}): extends the idea of using random walks of bounded lengths for predicting links in multi-relational KGs. Let $\pi_L(i,j,k,t)$ denote a path of length $L$ of the form  $e_i \xrightarrow{r_1} e_2 \xrightarrow{r_2} e_3 \cdots \xrightarrow{r_L} e_j$, where $t$ represents the sequence of edge types $t = (r_1, r_2, \ldots, r_L)$. We also require there to be a direct arc $e_i \xrightarrow{r_k} e_j$, representing the existence of a relationship of type $k$ from $e_i$ to $e_j$. Let $\Pi_L(i,j,k)$ represent the \underline{set} of all such paths of length $L$, ranging over path types $t$. \\
	
	\p We can compute the probability of following a given path by assuming that at each step we follow an outgoing link uniformly at random. Let $\Prob{\pi_L(i,j,k,t)}$ be the probability of the path with type $t$. \textit{The key idea in PRA is to use these path probabilities as features for predicting the probabilities of missing edges}. More precisely, the feature vector and score function (logistic regression) are as follows:
	\graybox{
		\vec[ijk]{\phi}^{PRA} &= [\Prob{\pi}: \pi \in \Pi_L(i,j,k)] \\
		f_{ijk}^{PRA} &:= \vec[k]{w}^T \vec[ijk]{\phi}^{PRA}	
	}
\end{compactitem}


\red{TODO}: Finish...


% ============================================================================================
\lecture{Structured Prediction}{Fast Top-K Search in Knowledge Graphs}{Dec 6, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize S. Yang, F. Han, Y. Wu, X. Yan, ``Fast Top-K Search in Knowledge Graphs.''}

\p \blue{Introduction}. Task: Given a knowledge graph $G$, scoring function $F$, and a graph query $Q$, top-k subgraph search over $G$ returns $k$ answers with the highest matching scores. An example is searching for movie makers (directors) worked with ``Brad'' and have won awards, illustrated below:
\myfig[0.4\textwidth]{TopKQuery.png}

Clearly, it would be extremely inefficient to enumerate all possible matches and then rank them.

\myspace
\p \blue{Preliminaries/Terminology}.
\begin{compactitem}
	\item \textbf{Queries}. A query [graph] is defined as $Q = (V_Q, E_Q)$. Each query node in $Q$ provides information/constraints about an entity, and an edge between two nodes specifies the relationship or the connectivity constraint posed on the two nodes. $Q*$ denotes a star-shaped query, which is basically a graph that looks like a star (central node with tree-like structure radially outward). 
	
	\item \textbf{Subgraph Matching}. Given a graph query $Q$ and a knowledge graph $G$, a \underline{match} $\phi(Q)$ of $Q$ in $G$ is a subgraph of $G$, specified by a one-to-one matching function $\phi$. It maps each node $u$ (edge $e=(u, u')$) in $Q$ to a node match $\phi(u)$ (edge match $\phi(e)=(\phi(u), \phi(u'))$) in $\phi(Q)$. \\
	
	\p The matching score between query $Q$ and its match $\phi(Q)$ is
	\begin{align}
	F(\phi(Q)) &= \sum_{v \in V_Q} F_V(v, \phi(v)) + \sum_{e \in E_Q} F_E(e, \phi(e)) \\
	F_V(v, \phi(v)) &= \sum_i \alpha_i f_i(v, \phi(v)) \\
	F_E(e, \phi(e)) &= \sum_j \beta_j f_j(e, \phi(e))
	\end{align}
\end{compactitem}

\myspace
\p \blue{Star-Based Top-K Matching}. 
\begin{compactenum}
	\item \textbf{Query decomposition}: Given query $Q$, STAR decomposes $Q$ to a set of star queries $\mathcal Q$. A star query contains a pivot node and a set of leaves as its neighbors in $Q$. 
	
	\item \textbf{Star querying engine}: Generate a set of top matches for each star query $\mathcal Q$. 
	
	\item \textbf{Top-k rank join}. The top matches for multiple star queries are collected and joined to produce top-k complete matches of $Q$. 
\end{compactenum}









% ============================================================================================
\lecture{Structured Prediction}{Dynamic Recurrent Acyclic Graphical Neural Networks (DRAGNN)}{Jan 19, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Kong et al., ``DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks,'' (2017).}

\p \blue{Transition Systems}. Define a transition system $\mathcal{T} \triangleq \{ \mathcal{S}, \mathcal{A}, t \}$, where
\begin{compactitem}
	\item $\mathcal{S} = \mathcal{S}(x)$ is a set of states, where that set depends on the input sequence $x$.
	\item A special start state $s^{\dagger} \in \mathcal{S}(x)$.
	\item A set of allowed decisions $\mathcal{A}(s, x) ~ \forall s \in \mathcal{S}(x)$.
	\item A transition function $t(s, d, x)$ returning a new state $s'$ for any decision $d \in \mathcal{A}(s, x)$.  
\end{compactitem}
The authors then define a \green{complete structure} as a sequence of state/decision pairs $(s_1, d_1)\ldots(s_n, d_n)$ such that $s_1 = s^{\dagger}$, $d_i \in \mathcal{A}(s_i)$ for $i = 1, \ldots, n$ and $s_{i+1} = t(s_i, d_i)$, where $n = n(x)$ is the number of decisions for input x\footnote{The authors state that we are only concerned with complete structures that have the same number of decisions $n(x)$ for the same input x.}. We'll use transition systems to map inputs $x$ into a sequence of output symbols $d_1, \ldots, d_n$. 

\myspace
\p \blue{Transition Based Recurrent Networks}. When combining transition systems with recurrent networks, we will refer to them as \green{Transition Based Recurrent Units (TBRU)}, which consist of:
\begin{compactitem}
	\item Transition system $\mathcal{T}$. 
	\item Input function $\vec{m}(s): \mathcal{S} \mapsto \R^K$ that maps states to some fixed-size vector representation (e.g. an embedding lookup operation). 
	\item Recurrence function $\vec{r}(s) : \mathcal{S} \mapsto \mathbb{P}\{ 1, \ldots, i - 1 \}$ that maps states to a set of previous time steps, where $\mathbb{P}$ is the power set. Note that $|\vec{r}(s)|$ may vary with $s$. We use $\vec{r}$ to specify state-dependent recurrent links in the unrolled computation graph. 
	\item The RNN cell $\vec[s]{h} \leftarrow \mathbf{RNN}(\vec{m}(s), \{ \vec[i]{h} \mid i \in \vec{r}(s) \})$. 
\end{compactitem}

\myspace
\p \blue{Example: Sequential tagging RNN}. Let $\vec{x} = \{ x_1, \ldots, x_n \}$ be a sequence of input tokens. Let the $i$th output, $d_i$, be a tag from some predefined set of tags $\mathcal{A}$. Then our model can be defined as:
\begin{compactitem}
	\item Transition system: $\mathcal{T} = \{~  s_i\hspace{-0.5ex}=\hspace{-0.5ex}\mathcal{S}(x_i)\hspace{-0.5ex}=\hspace{-0.5ex}\{1,\ldots,d_{i-1} \}, ~~ \mathcal{A}, ~~ t(s_i, d_i, x_i)\hspace{-0.5ex}=\hspace{-0.5ex}s_{i+1}\hspace{-0.5ex}=s_i\hspace{-0.5ex}+\hspace{-0.5ex}\{d_i\}  ~ \}$. 
	\item Input function:  $\vec{m}(s_i) = embed(x_i)$.
	\item Recurrence function: $\vec{r}(s_i) = \{i - 1\}$ to connect the network to the previous state. 
	\item RNN cell: $\vec[i]{h} \leftarrow LSTM(\vec{m}(s_i) \mid \vec{r}(s_i)=\{i-1\}  )$. 
\end{compactitem}

\myspace
\p \blue{Example: Parsey McParseface}. 
\begin{compactitem}
	\item Transition system: the \green{arc-standard} transition system, defined in image below\footnote{Image taken from ``Transition-Based Parsing'' by Joakim Nivre. Note that ``right-headed'' means ``goes from left to right'' or ``headed \textit{to} the right''.}. 
	\myfig[0.6\textwidth]{figs/arc_standard_alg.png}
	
	so the state contains all words and partially built trees (stack) as well as unseen words (buffer). 
	
	\item Input function: $\vec{m}(s_i)$ is the concatenation of 52 feature embeddings extracted from tokens based on their positions in the stack and the buffer.
	\item Recurrence function: $\vec{r}(s_i)$ is empty, as this is a feed-forward network. 
	\item RNN cell: a feed-forward MLP (so not an RNN...). 
\end{compactitem}

\myspace
\p \blue{Inference with TBRUs}. To predict the output sequence $\{d_1, \ldots, d_n\}$ given input sequence $\vec{x} = \{x_1, \ldots, x_n\}$, do:
\begin{compactenum}
	\item Initialize $s_1 = s^{\dagger}$. 
	\item For $i = 1, \ldots, n$:
	\begin{compactenum}
		\item Compute $\vec[i]{h} = \mathbf{RNN}(\vec{m}(s_i), \{  \vec[j]{h} \mid j \in \vec{r}(s_i)    \}  )$. 
		
		\item Update transition state:
		\begin{align}
		d_i &\leftarrow \argmax_{d \in \mathcal{A}(s_i)} \vec[d]{w}^T \vec[i]{h} \\
		s_{i+1} &\leftarrow t(s_i, d_i)
		\end{align}
		\red{NOTE}: This defines a locally normalized training procedure, whereas Andor et al. of Syntaxnet clearly conclude that their globally normalized model is the preferred choice.
	\end{compactenum}
\end{compactenum}


\myspace
\p \blue{Combining multiple TBRUs}. We connect multiple TBRUs with different transition systems via $\vec{r}(s)$. 
\begin{compactenum}
	\item We execute a list of $T$ TBRU components sequentially, so that each TBRU advances a global step counter.
	\item \textit{Each} transition state, $s^{\tau}$, from the $\tau$'th component has access the \textit{terminal} states from every prior transition system, and the recurrence function $\vec{r}(s^{\tau})$ for any given component can pull hidden activations from every prior one as well.
\end{compactenum}

\myspace
\p \blue{Example: Multi-task bi-directional tagging}. Say we want to do both POS and NER tagging (indices start at 1). 
\begin{compactitem}
	\item Left-to-right: $\mathcal{T} = $ \textit{shift-only}, $\vec{m}(s_i) = \vec[i]{x}$, $\vec{r}(s_i) = \{i - 1\}$. 
	\item Right-to-left: $\mathcal{T} =$ \textit{shift-only}, $\vec{m}(s_{n+i}) = \vec[(n-i)+1]{x}$, $\vec{r}(s_{n+i}) = \{n + i - 1\}$. 
	\item POS Tagger: $\mathcal{T}_{POS} =$ \textit{tagger}, $\vec{m}(s_{2n+i}) = \{\}$, $\vec{r}(s_{2n+i}) = \{i, (2n - i) + 1\}$
	\item NER Tagger: $\mathcal{T}_{NER} =$ \textit{tagger}, $\vec{m}(s_{3n+i}) = \{\}$, $\vec{r}(s_{3n+i}) = \{i, (2n - i) + 1, 2n + i \}$
\end{compactitem}
which illustrates the most important aspect of the TBRU:
\vspace{-0.8em}
\begin{quote}
	{\itshape 
		A TBRU can serve as both an encoder for downstream tasks and a decoder for its own task simultaneously.
	}
\end{quote}
For this example, the POS Tagger served as both an encoder for the NER task as well as a decoder for the POS task.

\myspace
\p \blue{Training a DRAGNN}. Assume training data consists of examples $\vec{x}$ along with gold decision sequences for a given TBRU in the DRAGNN. Given decisions $d_1, \ldots, d_N$ from prior components $1, \ldots, T-1$, the log-likelihood for training the $T$'th TBRU along its gold decision sequence $d_{N+1}^{\star}, \ldots, d_{N+n}^{\star}$ is then:
\graybox{
	L(\vec{x}, ~ d_{N + 1 : N+n}^{\star} ; ~ \theta )	
	&= \sum_{i} \log \Prob{
		d_{N+i}^{\star} \mid d_{1:N}, ~ d_{N+1:N+i-1}^{\star}; ~ \theta
	}
}
During training, the entire input sequence is unrolled and \green{backpropagation through structure} is used for gradient computation.

\myspace\Needspace{18\baselineskip}
\subsub{More Detail: Arc-Standard Transition System}

The arc-standard transition system is mentioned a lot, but with little detail. Here I'll synthesize what I find from external resources. The literature defines the states in a transition system slightly differently than the DRAGNN paper. Here we'll define them as a \green{configuration} $c = (\Sigma, B, A)$ triplet, where
\begin{compactitem}
	\item $\Sigma$ is the \textbf{stack} of tokens in $x$ that we've [partially] processed.
	\item $B$ is the \textbf{buffer} of remaining tokens in $x$ that we need to process. 
	\item $A$ is a set of \textbf{arcs} $(w_i, w_j, \ell)$ that link $w_i$ to $w_j$, and label the arc/link as $\ell$. 
\end{compactitem}
So, in the arc-standard transition system figure presented with Parsey McParseface earlier, 
\begin{compactitem}
	\item SHIFT just means ``move the head element of the buffer to the tail element of the buffer''. 
	
	\item Left-arc just means ``make a link \textit{from} the tail element of the stack \textit{to} the element before it. Remove the pointed-to element from the stack.''
	
	\item Right-arc just means ``make a link \textit{from} the element before the tail element in the stack \textit{to} the tail element. Remove the pointed-to element from the stack.''
\end{compactitem}





% ============================================================================================
\lecture{Structured Prediction}{Joint Extraction of Events and Entities within a Document Context}{April 26, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize B. Yang and T. Mitchell, ``Joint Extraction of Events and Entities within a Document Context,'' (2016).}

\p \blue{Introduction}. Two main reasons that state-of-the-art event extraction systems have difficulties: 
\begin{compactenum}
	\item They extract events and entities in separate stages. 
	\item They extract events independently from each individual sentence, ignoring the rest of the document. 
\end{compactenum}
This paper proposes an approach that simultaneously extracts events and entities within a document context. They do this by first decomposing the problem into 3 tractable subproblems:
\begin{compactenum}
	\item Learning the dependencies between a single event [trigger] and all of its potential arguments.
	\item Learning the co-occurrence relations between events across the document.
	\item Learning for entity extraction.
\end{compactenum} 
and then combine these learned models into a joint optimization framework. 

\myspace
\p \blue{Learning Within-Event Structures}. For now, assume we have some document $x$, a set of candidate event triggers $\mathcal{T}$, and a set of candidate entities $\mathcal{N}$. Denote the set of entity candidates that are potential arguments for trigger candidate $i$ as $\mathcal{N}_i$. The joint distribution over the possible trigger types, roles, and entities for those roles, is given by\marginnote{All $f_i$ also depend on $i, x$. In addition, all $f_i$ except $f_1$ depend on the current $j$ in the summation.}[2em]
\graybox{
	&\Prob[\vec{\theta}]{ t_i, \vec[i]{r}, \vec{a} \mid i, \mathcal{N}_i, x }
	\propto \\
	&\exp\left( \vec[1]{\theta}^T \mgreen{f_1}(t_i) +
	\sum_{j \in \mathcal{N}_i} \left[ \vec[2]{\theta}^T \mgreen{f_2}(r_{ij}) +
	\vec[3]{\theta}^T f_3 (t_i, r_{ij}) +
	\vec[4]{\theta}^T \mgreen{f_4} (a_j) + 
	\vec[5]{\theta}^T f_5 (r_{ij}, a_j)\right] \right) 
}

where each $f_i$ is a feature function, and I've colored the unary feature functions green. The unary features are tabulated in Table 1 of the original paper. They use simple indicator functions $1_{t,r}$ and $1_{r, a}$ for the pairwise features. They train using maximum-likelihood estimates with L2 regularization:
\begin{align}
\vec{\theta}^* &= \argmax_{\vec\theta} \mathcal{L}(\vec\theta) - \lambda ||\vec\theta||_2^2 \\
\mathcal{L}(\vec\theta) &= \sum_i \log \left( \Prob[\vec{\theta}]{ t_i, \vec[i]{r}, \vec{a} \mid i, \mathcal{N}_i, x } \right)
\end{align}
and use \green{L-BFGS} to optimize the training objective. 

\myspace
\p \blue{Learning Event-Event Relations}. A pairwise model of event-event relations in a document. Training data consists of all pair of trigger candidates that co-occur in the same sentence or are connected by a co-referent subject/object if they're in different sentences. Given a trigger candidate pair $(i, i')$, we estimate the probabilities for their event types $(t_i, t_{i'})$ as
\graybox{
	\Prob[\phi]{t_i, t_{i'} \mid x, i, i'} \propto \exp \left( 
	\phi^T g(t_i, t_{i'}, x, i, i')
	\right)
}
where $g$ is a feature function that depends on the trigger candidate pair and their context. In addition to re-using the trigger features in Table 1 of the paper, they also introduce relational trigger features:
\begin{compactenum}
	\item whether they're connected by a conjunction dependency relation
	\item whether they share a subject or an object
	\item whether they have the same head word lemma
	\item whether they share a semantic frame based on FrameNet. 
\end{compactenum}
As before, they using L-BFGS to compute the maximum-likelihood estimates of the parameters $\phi$. 


\myspace
\p \blue{Entity Extraction}. Trained a standard linear-chain \green{CRF} using the BIO scheme. Their CRF features:
\begin{compactenum}
	\item current words and POS tags
	\item context words in a window of size 2
	\item word type such as all-capitalized, is-capitalized, all-digits
	\item Gazetteer-based entity type if the current word matches an entry in the gazetteers collected from Wikipedia. 
	\item pre-trained word2vec embeddings for each word
\end{compactenum}

\myspace
\p \blue{Joint Inference}. Allows information flow among the 3 local models and finds globally-optimal assignments of all variables. Define the following objective:
\graybox{
	\max_{ \vec t, \vec r, \vec a} \sum_{i \in \mathcal{T}} E(t_i, \vec[i]{r}, \vec{a})
	+ \sum_{i, i' \in \mathcal{T}} R(t_i, t_{i'})
	+ \sum_{j \in \mathcal{N}} D(a_j)	
}
where
\begin{compactitem}
	\item The first term is the sum of confidence scores for individual event mentions from the within-event model. 
	\begin{align}
	E(t_i, \vec[i]{r}, \vec{a})
	= \log p_{\theta}(t_i) + \sum_{j \in \mathcal{N}_i} \left[ \log p_{\theta}(t_i, r_{ij}) + \log p_{\theta}(r_{ij}, a_j) \right]
	\end{align}
	
	\item The second term is the sum of confidence scores for event relations based on the pairwise event model.
	\begin{align}
	R(t_i, t_{i'}) = \log p_{\phi}(t_i, t_{i'} \mid i, i', x)
	\end{align}
	
	\item The third term is sum of confidence scores for entity mentions, where
	\begin{align}
	D(a_j) = \log p_{\psi}(a_j \mid j, x)
	\end{align}
	and $p_{\psi}(a_j \mid j, x)$ is the marginal probability derived from the linear-chain CRF. 
\end{compactitem}
The optimization is subject to agreement constraints that enforce the overlapping variables among the 3 components to agree on their values. The joint inference problem can be formulated as an \green{integer linear problem (ILP)}\footnote{From Wikipedia: An integer linear program in canonical form: maximize $c^T x$ subject to $A x \le b, x \ge 0, x \in \mathbb{Z}^n$}. To solve it efficiently, they find solutions for the relaxation of the problem using a \green{dual decomposition algorithm $AD^3$}. 




% ============================================================================================
\lecture{Structured Prediction}{Globally Normalized Transition-Based Neural Networks}{April 27, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize D. Andor et al., ``Globally Normalized Transition-Based Neural Networks,'' (2016).}


\p \blue{Introduction}. Authors demonstrate that simple FF neural networks can achieve comparable or better accuracies than LSTMs, as long as they are \green{globally normalized}. They don't use any recurrence, but perform \green{beam search} for maintaining multiple hypotheses and introduce global normalization with a \green{CRF} objective to overcome the label bias problem that locally normalized models suffer from. 

\myspace 
\p \blue{Transition System}. Given an input sequence $x$, define:
\begin{compactitem}
	\item Set of states $S(x)$.
	\item Start state $s^{\dagger} \in S(x)$.
	\item Set of decisions $\mathcal{A}(s, x)$ for all $s \in S(x)$. 
	\item Transition function $t(s, d, x)$ returning new state $s'$ for any decision $d \in A(s, x)$.  
\end{compactitem}
The scoring function $\rho(s, d; \theta)$, which gives the score for decision $d$ in state $s$, will be defined:
\begin{align}
\rho(s, d; \theta) = \phi(s; \theta^{(l)}) \cdot \theta^{(d)}
\end{align}
which is just the familiar logits computation for decision $d$. $\theta^{(l)}$ are the parameters of the network excluding the parameters at the final layer, $\theta^{(d)}$. $\phi(s; \theta^{(l)})$ gives the representation for state $s$ computed by the neural network under parameters $\theta^{(l)}$.

\myspace
\p \blue{Global vs. Local Normalization}. 

\begin{compactitem}
	\item \textbf{Local}. Conditional probabilities $\Prob{d_j \mid s_j; \theta }$ are normalized locally over the scores for each possible action $d_j$ from the current state $s_j$. 
	\begin{align}
	\Prob[L]{d_{1:n}} 
	&= \prod_{j = 1}^{n} \Prob{d_j \mid s_j; \theta } 
	= \dfrac{   \exp \left( \sum_{j = 1}^{n} \rho  ( s_j, d_j ; \theta  ) \right)
	}{ \mpurple{ \prod_{j = 1}^{n} Z_L (s_j; \theta) }  } \\
	Z_L(s_j; \theta)
	&= \sum_{ d' \in \mathcal{A}(s_j) } \exp \left( \rho ( s_j, d'; \theta )  \right)
	\end{align}
	Beam search can be used to attempt to find the action sequence with highest probability. 
	
	\item \textbf{Global}. In contrast, a CRF defines:
	\begin{align}
	\Prob[G]{d_{1:n}}
	&= \dfrac{   \exp \left( \sum_{j = 1}^{n} \rho  ( s_j, d_j ; \theta  ) \right)
	}{ \mpurple{Z_G (\theta)}  } \\
	Z_G(\theta) 
	&=
	\sum_{d_{1:n}' \in \mathcal{D}_n } \exp \left( 
	\jnsum \rho ( s_j', d_j'; \theta )
	\right)
	\end{align}
	where $\mathcal{D}_n$ is the set of all valid sequences of decisions of length $n$. The inference problem is now to find
	\begin{align}
	\argmax_{d_{1:n} \in \mathcal{D}_n} \Prob[G]{d_{1:n}}
	&= \argmax_{d_{1:n} \in \mathcal{D}_n} \jnsum \rho(s_j, d_j; \theta) 
	\end{align}
	and we can also use beam search to approximately find the argmax. 
\end{compactitem}


\myspace
\p \blue{Training}. SGD on the NLL of the data under the model. The NLL takes a different form depending on whether we choose a locally normalized model vs a globally normalized model. 
\begin{compactitem}
	\item \textbf{Local}. 
	\begin{align}
	L_{local}(d_{1:n}^*; \theta )
	&= - \ln \Prob[L]{d_{1:n}^*; \theta} \\
	&=  -  \sum_{j = 1}^{n} \rho  ( s_j^*, d_j^* ; \theta  ) + \jnsum \ln Z_L (s_j^*; \theta) 
	\end{align}
	
	\item \textbf{Global}. 
	\begin{align}
	L_{global}(d_{1:n}^* ; \theta) 
	&=  - \ln \Prob[G]{d_{1:n}^*; \theta} \\
	&= -  \sum_{j = 1}^{n} \rho  ( s_j^*, d_j^* ; \theta  )  + \ln Z_G ( \theta) 
	\end{align}
\end{compactitem}
To make learning tractable for the globally normalized model, the authors use \green{beam search with early updates}, defined as follows. Keep track of the location of the gold path\footnote{The gold path is the predicted sequence that matches the true labeled sequence, up to the current timestep.} in the beam as the prediction sequence is being constructed. If the gold path is not found in the beam after step $j$, run one step of SGD on the following objective:
\begin{align}
L_{global-beam}(d^*_{1:j}, \theta) 
&= -\sum_{t=1}^{j} \rho(d^*_{1:t-1}, d^*_t ; \theta) 
- \ln \sum_{d'_{1:j} \in \mathcal{B}_j} \exp\left( 
\sum_{t = 1}^j \rho(d'_{1:t-1}, d'_t; \theta)
\right)
\end{align}
where $\mathcal{B}_j$ contains all paths in the beam at step $j$, \textit{and} the gold path prefix $d^*{1:j}$. If the gold path remains in the beam throughout decoding, a gradient step is performed using $\mathcal{B}_T$, the beam at the end of decoding. When training the global model, the authors \underline{first pretrain\footnote{All layers except the softmax layer.} using the local objective function, and then perform additional training steps}\newline\underline{using the global objective function.}.

\myspace
\p \blue{The Label Bias Problem}. Locally normalized models often have a very weak ability to revise earlier decisions. Here we will prove that \textbf{globally normalized models are strictly more expressive than locally normalized models}\footnote{This is for conditional models only.}. Let $\mathcal{P}_L$ denote the set of all possible distributions $p_L(d_{1:n} \mid x_{1:n})$ under the local model as the scores $\rho$ vary. Let $\mathcal{P}_G$ be the same, but for the global model.\marginnote{We are assuming that both $P_L$ and $P_G$ consist of log-linear distributions of scoring functions $\rho(d_{1:t-1}, d_t, x_{1:t})$}[2em]
\begin{center}
	\green{Theorem 3.1} {\itshape $\mathcal{P}_L$ is a strict subset\footnote{Note that $\subset$ and $\subsetneq$ \href{https://math.stackexchange.com/questions/1191715}{mean the same thing}. Matter of notational preference/being explicit/etc.} of $\mathcal{P}_G$, that is $\mathcal{P}_L \subsetneq \mathcal{P}_G$. }
\end{center}
In other words, a globally normalized model can model any distribution that a locally normalized one can, but the converse is not true. I've worked through the proof below.
\begin{example}[Proof: $P_L \subsetneq P_G$]
	\textbf{Proof that $P_L \subseteq P_G$}. For any locally normalized model with scores $\rho_L(d_{1:t-1}, d_{t}, x_{1:t})$, we can define a corresponding $p_G$ over scores
	\begin{align}
	\rho_G(d_{1:t-1}, d_t, x_{1:t}) 
	&= \ln p_L(d_t \mid d_{1:t-1}, x_{1:t}) 
	\end{align}
	By definition, this means that $p_G(d_{1:t} \mid x_{1:t}) = p_L(d_{1:t} \mid x_{1:t})$. 
	
	\textbf{Proof that $P_G \nsubseteq P_L$}. A proof by example. Consider a dataset consisting entirely of one of the following tagged sequences:
	\begin{align}
	\vec{x} = abc, &\qquad \vec{d} = ABC \\
	\vec{x} = abe, &\qquad \vec{d} = ADE  
	\end{align}
	Similar to a typical linear-chain CRF, let $\mathcal{T}$ denote the set of observed label transitions, and let $\mathcal{E}$ denote the set of observed $(x_t, d_t)$ pairs. Let $\alpha$ be the single scalar parameter of this simple model, where
	\begin{align}
	\rho(d_{1:t-1}, d_t, x_{1:t})
	&= \alpha \left( \ind_{(d_{t-1}, d_t) \in \mathcal T}
	+ \ind_{(x_t, d_t) \in \mathcal E} \right)
	\end{align}
	for all $t$. This results in the following distributions $p_G$ and $p_L$, evaluating on input sequence of length $3$
	\begin{align}
	p_G(d_{1:3} \mid x_{1:3})
	&= \frac{
		\exp\left( \alpha \sum_{t=1}^{3} ( \ind_{(d_{t-1}, d_t) \in \mathcal T}
		+ \ind_{(x_t, d_t) \in \mathcal E} )  \right)
	}{
		Z_G(x_{1:3})
	} \\
	p_L(d_{1:3} \mid x_{1:3})
	&= p_L(d_1 \mid x_1) p_L(d_2 \mid d_1, x_{1:2})  p_L(d_3 \mid d_{1:2}, x_{1:3})
	\end{align}
	where I've written $p_L$ as a product over its local CPDs because it reveals the core observation that the proof is based on: \textit{for any given subsequence $(d_{1:t-1}, x_{1:t})$, the local CPD is constrained to satisfy $\sum_{d_t} p_L(d_t \mid d_{1:t-1}, x_{1:t}) = 1$}. With this, the following comparison of $p_G$ and $p_L$ for large $\alpha$ completes the proof of $P_G \nsubseteq P_L$:
	\begin{align}
	\lim_{\alpha \rightarrow \infty} &p_G(ABC \mid abc) 
	= \lim_{\alpha \rightarrow \infty} p_G(ADE \mid abe) = 1 \\
	&p_L(ABC \mid abc) + p_L(ADE \mid abe)  \le 1 \quad (\forall \alpha) 
	\end{align}
	
	$\therefore P_L \subsetneq P_G$.
\end{example}





% ============================================================================================
\lecture{Structured Prediction}{Structured Attention Networks}{July 07, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Kim et al., ``Structured Attention Networks,'' (2017).}

\p \blue{Background: Attention Networks}. Goal: produce a context $c$ based on input sequence $x$ and query $q$. We assume we have an attention distribution\footnote{Also called the ``alignments''. It is the output of the softmax layer of attention scores in the majority of cases.} $z \sim p(z \mid x, q)$. Interpret $z$ as a categorical latent variable over $T$ categories, where $T = len(x)$ is the length of the input sequence. We can then compute the context, $c = \E[z \sim p(z \mid x, q)]{f(x, z)}$, where $f(x, z)$ is an \textit{annotation function}\footnote{In all applications I've seen, $f(x, z) = x_z$.}. 
\vspace{-0.6em}
\begin{quote}
	{\small\itshape
		We interpret the attention mechanism as taking the expectation of an annotation function $f(x, z)$ with respect to a latent variable $z \sim p$, where $p$ is parameterized to be a function of $x$ and $q$.
	}
\end{quote}
For comparisons later on with the traditional attention mechanism, here it is:
\begin{align}
c &= \sum_t^T p(z = t \mid x, q) \vec[t]{x} \label{eq:san-1} \\
p(z = t \mid x, q) &= \text{softmax}(\theta_t) 
\end{align}
where usually $x$ is the sequence of hidden state of the encoder RNN, q is the hidden state of the decoder RNN at the most recent time step, $z$ gives the source position to be attended to, and $\theta_t = \text{score}(x_t, q)$.

\myspace
\p \blue{Structured Attention}. In a \green{structured attention model}, $z$ is now a \textit{vector} of discrete random variables $z_1, \ldots, z_m$ and the attention distribution $p(z \mid x, q)$ is now modeled as a \textit{conditional random field}, specifying the structure of the $z$ variables. We also assume now that the annotation function $f$ factors into clique annotation functions $f(x, z) = \sum_C f_C(x, z_C)$, where the summation is over the $C$ factors, $\psi_C$, of the CRF. Our context vector takes the form:
\begin{align}
c = \E[z \sim p(z \mid x, q)]{f(x, z)} &= \sum_{C} \E[z \sim p(z_C \mid x, q)]{f_C(x, z_C)} \\
p(z \mid x, q) &= \inv{Z(x, q)} \prod_C \psi_C(z_C) 
\end{align}

\myspace 
\p \blue{Example 1: Subsequence Selection}. Let $m = T$, and let each $z_i \in \{0, 1\}$ be a binary R.V. Let $f(x, z) = \sum_{t}^{T} f_t(x, z_t) = \sum_t^T \vec[t]{x} \ind_{z_t=1}$\footnote{Ok, so equivalently, $z^T x$, i.e. the indicator function can just be replace by $z_t$ here}. This yields the context vector,
\begin{align}
c = \E[z_1, \ldots, z_T]{f(x, z)} = \sum_t^T p(z_t = 1 \mid x, q) \vec[t]{x} \label{eq:san-2}
\end{align}
Although this looks similar to equation \ref{eq:san-1}, we haven't yet revealed the functional form for $p(z \mid x, q)$. Two possible choices:\marginnote{The factor $\psi$ for the CRF is \textbf{NOT} the same as the factor for the Bernoulli!}[3em]
\begin{align}
\mtgreen{Linear-Chain CRF:}&\qquad p(z_1, \ldots, z_T\mid x, q) = \inv{Z(x, q)} \prod_t^T \psi_t(z_t, z_{t-1}) \\
\mtgreen{Bernoulli:}&\qquad p(z_1,\ldots, z_T \mid x, q) = \prod_t^T p(z_t = 1 \mid x, q) = \prod_t^T \sigma(\psi_t(z_t))
\end{align}
These show why equation \ref{eq:san-2} is fundamentally different than equation \ref{eq:san-1}:
\begin{compactitem}
	\item It allows for multiple inputs (or no inputs) to be selected for a given query.
	\item We can incorporate structural dependencies across the $z_t$'s. 
\end{compactitem}
Also note that all methods can use potentials from the same neural network or RNN that takes x and q as input. By this we mean, for example, that we can take the same parameters we'd use when computing the scores in our attention layer, and reinterpret them as e.g. CRF parameters. Then, we can compute the marginals $p(z_t \mid x)$ using the forward-backward algorithm\footnote{This is different than the simple softmax we usually use in an attention layer, which does not model any interdependencies between the $z_t$. The marginals we end up with when using the CRF originate from a \textit{joint} distribution over the entire sequence $z_1, \ldots z_T$. This seems potentially incredibly powerful. Need to analyze in depth.}.
\vspace{-0.5em}
\begin{quote}
	{\small\itshape Crucially this generalization from vector softmax to forward-backward is just a \textbf{series of differentiable steps}, and we can compute gradients of its output (marginals) with respect to its input (potentials), allowing the structured attention model to be trained end-to-end as part of a deep model.
	}
\end{quote}




% ============================================================================================
\lecture{Structured Prediction}{Relation Extraction: A Survey}{July 16, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Pawar et al., ``Relation Extraction: A Survey,'' (December 2017).}



\p \blue{Feature-based Methods}. For each entity pair $(e_1, e_2)$, generate a set of features and train a classifier to predict the relation\footnote{If there are N unique relations for our data, it is common to train the classifier to predict $2N$ total relations, to handle both possible orderings of relation arguments.}. Some useful features are shown in the figure below.
\myfig[0.6\textwidth]{figs/re_feature_types.png}

Authors found that SVMs outperform MaxEnt (logistic reg) classifiers for this task.


\myspace
\p \blue{Kernel methods}. Instead of explicit feature engineering, we can design kernel functions for computing similarities between representations of two relation instances\footnote{Recall that kernel methods are for the general optimization problem 
	\begin{align}
	\min_w \insum \text{loss}\left( \jnsum \alpha_j \phi(x_j)^T \phi(x_i), y_i  \right) + \lambda \jnsum \knsum \alpha_j \alpha_k \phi(x_j)^T\phi(x_k)
	\end{align}
	which changes the direct focus from feature engineering to ``similarity'' engineering.
} (a relation instance is a triplet of the form $(e_1, e_2)$), and SVM for the classification. \\

\Needspace{10\baselineskip}
One approach is the \green{sequence kernel}. We represent each relation instance as a sequence of feature vectors:
$$
(e_1, e_2) \rightarrow (\vec[1]{f}, \ldots, \vec[N]{f})
$$
where $N$ might be e.g. the number of words between the two entities, and the dimension of each $f$ is the same, and could correspond to e.g. POS tag, NER tag, etc. More formally, define the \textit{generalized subsequence kernel}, $K_n(s, t, \lambda)$, that computes some number of weighted subsequences $u$ such that
\begin{compactitem}
	\item There exist index sequences $ii := (i_1, \ldots i_n)$ and $jj := (j_1, \ldots j_n)$ of length $n$ such that 
	\begin{align}
	u_i &\in \vec[i]{s} \qquad \forall i \in  ii \\
	u_j &\in \vec[j]{t} \qquad \forall j \in jj \\ 
	\end{align}
	
	\item The weight of $u$ is $\lambda^{l(ii) + l(jj)}$, where $l(x) = \max(x) - \min(x)$ and $0 < \lambda < 1$. Sparser (more spaced out) subsequences get lower weight.
\end{compactitem}
The authors then provide the recursion formulas for $K$, and describe some extensions of sequence kernels for relation extraction.\\


\green{Syntactic Tree Kernels}.\marginnote{Syntactic Tree Kernels}[2em] Structural properties of a sentence are encoded by its constituent parse tree. The tree defines the syntax of the sentence in terms of constituents such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), POS tags (NN, VB, IN, etc.) as non-terminals and actual words as leaves. The syntax is usually governed by Context Free Grammar (CFG). Constructing a constituent parse tree for a given sentence is called \textit{parsing}. The \purple{Convolution Parse Tree Kernel} $K_T$ can be used for computing similarity between two syntactic trees.\\

\green{Dependency Tree Kernels}. For grammatical relations between words in a sentence. Words are the nodes and dependency relations are the edges (in the tree), typically from dependent to parent. In the \purple{relation instance representation}, we use the smallest subtree containing the entity pair of a given sentence. Each node is augmented with additional features like POS, chunk, entity level (name, nominal, pronoun), hypernyms, relation argument, etc.  Formally, an \textit{augmented dependency tree} is defined as a tree $T$ where\marginnote{Dependency Tree Kernels}[-3em]
\begin{compactitem}
	\item Each node $t_i$ has features $\phi(t_i) = \{v_1, \ldots, v_d \}$. 
	
	\item Let $t_i[\vec c]$ denote all children of $t_i$, and let $Pa(t_i)$ denote its parent. 
	
	\item For comparison of two nodes we use:
	\begin{compactitem}
		\item \textbf{Matching function} $m(t_i, t_j)$: equal to 1 if some important features are shared between $t_i$ and $t_j$, else 0. 
		\item \textbf{Similarity function} $s(t_i, t_j)$: returns a positive real similarity score, and defined as
		\begin{align}
		s(t_i, t_j)
		&= \sum_{v_q \in \phi(t_i)}\sum_{v_r \in \phi(t_j)} \text{Compat}(v_q, v_r)
		\end{align}
		over some compatibility function between two feature values. 
	\end{compactitem}
\end{compactitem}
Finally, we can define the overall dependency tree kernel $K(T_1, T_2)$ for similarity between trees $T_1$ and $T_2$ as follows. Let $r_i$ denote the root node of tree $T_i$.\marginnote{
	$$a_1 \le a_2 \le \ldots \le a_n$$
	$$ d(\vec a) \triangleq a_n - a_1 + 1 $$
	$$ 0 < \lambda < 1 $$
}[1em]
\graybox{
	K(T_1, T_2) 
	&= \begin{cases}
		0 & \text{if } m\left(r_1, r_2\right) = 0 \\
		s(r_1, r_2) + K_c(r_1[\vec c], r_2[\vec c]) & \text{otherwise}
	\end{cases}\\
	K_c(t_i[\vec c], t_j[\vec c])
	&= \sum_{\vec a, \vec b, l(\vec a)=l(\vec b)} \lambda^{d(\vec a) + d(\vec b)} K(t_i[\vec a], t_j[\vec b])
}
The interpretation is that, whenever a pair of matching nodes is found, \textit{all} possible matching subsequences\footnote{Note that a summation over subsequences of a sequence $\vec a$, denoted here as $\sum_{\vec a}$, expands to $\{a_1, \ldots a_n, a_1 a_2, a_1 a_3, \ldots a_1 a_n, a_1 a_2 a_3, \ldots, a_2 a_5 a_6, \ldots \}$ and so on and so forth.} of their children are found. Two subsequences $\vec a$ and $\vec b$ are said to ``match'' if $m(a_i, b_1) = 1 (\forall i < n)$. Similar to the sequence kernel seen earlier, $\lambda$ is a decay factor that penalizes sparser subsequences.





% ============================================================================================
\lecture{Structured Prediction}{Neural Relation Extraction with Selective Attention over Instances}{July 16, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Lin et al., ``Neural Relation Extraction with Selective Attention over Instances,'' (2016).}

\p \blue{Introduction}. A common distant supervision approach for RE is aligning a KB with text. For any $(e_1, r, e_2)$ in the KB, it assumes that all text mentions of $(e_1, e_2)$ express the relation $r$. Of course, this assumption will often not be true. This motivates the notion of \green{multi-instance learning}, wherein we predict whether a set of instances $\{x_1, \ldots x_n\}$ (each of which contain mention(s) of $(e_1, e_2)$) imply the existence of $(e_1, r, e_2)$ being true.

\myspace
\p \blue{Input Representation}. Each instance sentence $x$ is tokenized into a sequence of words. Each word $w_i$ is transformed into a concatenation $\vec[i]{w} \in \R^{d}$ ($d = d_w + 2 d_{pos}$), 
\begin{align}
\vec[i]{w} := [\text{word2Vec}(w_i); \text{dist}(w_i, e_1); \text{dist}(w_i, e_2)]
\end{align} 
where dist(a, b) returns the [embedded] relative distance (num tokens) between a and b in the given sentence (positive integer)\footnote{More specifically, it is an embedded representation of the relative distance. To actually implement it,  you'd first shift the relative distances such that they begin at 0, and learn 2 * window\_size + 1 embedding vectors for each of the possible position offsets. Anything outside the window is embedded into the zero vector.}.

\myspace
\p \blue{Convolutional Network}. We use a CNN to encode a sentence of embeddings $\{ \vec[1]{w}, \ldots, \vec[T]{w}  \}$ into a single sentence vector representation $\vec x$. Denote the kernel/filter/window size as $\ell$ and the number of words in the given sentence as $T$. Let $\vec[i]{q} \in \R^{\ell \cdot d}$ denote the vector for the $i$th window,
\begin{align}
\vec[i]{q} &= \vec[i-\ell+1:i]{w} \qquad (1 \le i \le T + \ell - 1)
\end{align}
and let $\matr{Q} \in \R^{(T + \ell - 1) \times \ell \cdot d}$ be defined such that row $\matr[i]{Q} = \vec[i]{q}^T$. 
It follows that, for convolution matrix $\matr W \in \R^{K \times (\ell \cdot d)}$, the output of the $k$th filter, and subsequent max-pooling, is
\begin{align}
\vec[k]{p} &= \left[ \matr W \matr{Q}^T \right]_k + \vec  b \\
\left[\vec x\right]_k &= \left[
\max(\vec[k1]{p});
\max(\vec[k2]{p});
\max(\vec[k3]{p})
\right]
\end{align}
where we've divided $\vec[k]{p}$ into three segments, corresponding to before entity 1, middle, and after entity 2 of the given sentence. The sentence vector $\vec x \in \R^{3K}$ is the concatenation of all of these, after feeding through a non-linear activation function like a ReLU. 

\myspace
\p \blue{Selective Attention over Instances}. An attention mechanism is employed over all $n$ sentence instances $x_i$ for some candidate entity pair $(e_1, e_2)$. The output is a \textbf{set vector} $\vec s$, a real-valued vector representation of the set of instances, where
\begin{align}
\vec s &= \sum_i \alpha_i \vec[i]{x} \\
\alpha_i &= \text{softmax}(\vec[i]{x} \matr{A} \vec{r})
\end{align}
is an attention-weighted sum over the instance embeddings. Note that they constrain $\matr A$ to be diagonal. Finally, the predictive distribution is defined as
\graybox{
	p(r \mid \mathcal{S}, (e_1, e_2) \theta)
	&= \text{softmax}\left(\matr{M}\vec{s} + \vec{d} \right)_r 	
}
where $\mathcal{S}$ is the set of $n$ sentences for the given entity pair $(e_1, e_2)$. 
























































% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Language Modeling}\label{Language Modeling}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------

% ===========================================================================================
\lecture{Language Modeling}{Improving Language Understanding by Generative Pre-Training}{August 24, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Radford et al., ``Improving Language Understanding by Generative Pre-Training,'' (2018).}

\p \blue{Unsupervised Pre-Training} (3.1). Given unsupervised corpus of tokens $\mathcal{U} = \{u_1, \ldots, u_n \}$, train with a standard LM objective:
\begin{align}
L_1(\mathcal U)
&= \sum_i^n \log P(u_i \mid u_{i-k}, \ldots, u_{i-1}; \Theta)
\end{align}
The authors use a \green{Transformer decoder}, i.e. literally just the decoder part of the Transformer in ``Attention is all you need.'' 

\myspace
\p \blue{Supervised Fine-Tuning} (3.2). Now we have a labeled corpus $\mathcal C$, where each instance consists of a sequence of input tokens $x^1, \ldots, x^m$, along with a label $y$. They just feed the inputs through the transformer until they obtain the final transformer block's activation $h_l^m$, and linearly project it to output space:
\begin{align}
P(y \mid x^1, \ldots, x^m )	
&= \text{softmax}(h_l^m W_y) \\
L_2(\mathcal C)
&= \sum_{(x,y)} \log P(y \mid x^1, \ldots, x^m)
\end{align}
They also found that including a language modeling auxiliary objective helped learning,
\begin{align}
L_3(\mathcal C)
&= L_2(\mathcal C) + \lambda L_1(\mathcal C)
\end{align}
\textellipsis that's it. Extremely simple, yet somehow effective.


% ===========================================================================================
\lecture{Language Modeling}{Exploring the Limits of Language Modeling}{August 30, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Josefina et al., ``Exploring the Limits of Language Modeling,'' (2016).}


\p \blue{NCE and Importance Sampling} (3.1). In this section, assume any $p(w)$ is shorthand for $p(w \mid \{w_{prev}  \} )$. 
\begin{compactitem}
	\item \green{Noise Contrastive Estimation} (NCE). Train a classifier to discriminate between true data (from distribution $p_d$) or samples coming from some arbitrary noise distribution $p_n$. If these distributions were known, we could compute
	\begin{align}
	p(Y {=} true \mid w)
	&= \frac{ p(w \mid \text{true} ) p(\text{true}) }{ p(w)  } \\
	&= \frac{ p_d(w)  p(\text{true}) }{ p(w, \text{true}) + p(w, \text{false})  } \\
	&= \frac{ p_d(w)  p(\text{true}) }{ p_d(w)p(\text{true}) + p_n(w)p(\text{false})  } \\
	&= \frac{ p_d(w) }{ p_d(w) + k p_n(w) } 
	\end{align}
	where $k$ is the number of negative samples per positive word. The idea is to train a logistic classifier $p(Y{=}true \mid w) = \sigma(\log p_{model} - \log k p_n(w))$, then $\text{softmax}(\log p_{model})$ is a good approx of $p_d(\vec w)$. 
	
	
	\item \green{Importance Sampling}. Estimates the partition function. Consider that now we have a set of $k+1$ words $W = \{w_1, \ldots, w_{k+1} \}$, where $w_1$ is the word coming from the true data, and the rest are from the noise distribution. We train a multinomial logistic regression over $k + 1$ classes,
	\begin{align}
	p(Y{=}i \mid W) 
	&= \frac{ p_d(w_i) }{  p_n(w_i)  } \inv{ \sum_{i'=1}^{k+1} p_d(w_{i'})  /   p_n(w_{i'})} \\
	&\propto_Y  \frac{ p_d(w_i) }{  p_n(w_i)  }
	\end{align} 
	and we end up seeing that IS is the same as NCE, except in the multiclass setting and with cross entropy loss instead of logistic loss. 
\end{compactitem}


\myspace
\p \blue{CNN Softmax} (3.2). Typically the logit for word $w$ is given by $z_w = h^T e_w$, where $h$ is often the output state of an LSTM, and $e_w$ is a vector of parameters that could be interpreted as the word embedding for $w$. Instead of this, the authors propose what they call \textit{CNN Softmax}, where we compute $e_w = CNN(chars_w)$. Although this makes the function mapping from $w$ to $e_w$ much smoother (due to the tied weights), it ends up having a hard time distinguishing between similarly spelled words that may have entirely different meanings. The authors use a correction factor, learned for each word, such that
\begin{align}
z_w 
&= h^T CNN(chars_w) + h^T M corr_w
\end{align}
where $M$ projects low-dimensional $corr_w$ back up to the dimensionality of the LSTM state $h$. 

\myspace
\p \blue{Char LSTM Predictions} (3.3). To reduce the computational burden of the partition function, the authors feed the word-level LSTM state $h$ through a character-level LSTM that predicts the target word one character at a time.





% ===========================================================================================
\lecture{Language Modeling}{Noise Contrastive Estimation}{December 9, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize M. Gutman and A. Hyv\"{a}rinen, ``Noise contrastive estimation: A new estimation principle for unnormalized statistical models,'' University of Helsinki  (2010).}

\p \blue{TL;DR}: A few ways of thinking about NCE:
\begin{compactitem}
	\item Instead of directly modeling a normalized word distribution $p_d(w)$, we can just model the unnormalized $\widetilde{p}_d$ distribution (and an additional parameter $c = - \ln Z$) by training our model to distinguish between true samples from $p_d$ and noise samples from some distribution $p_n$ that we choose.
	
	\item Instead of modeling $p(w \mid c)$, we model $p(D \mid w, c)$, where $D$ is binary RV indicating whether $w, c$ are from the true data distribution $p_d$ or the noise distribution $p_n$. 
\end{compactitem}

\myspace
\p \blue{Introduction}. Setup \& notation:
\begin{compactitem}
	\item We observe $\vec x \sim p_d(\cdot)$ but $p_d(\cdot)$ itself is unknown. 
	\item We model $p_d$ by some model $p_m(\cdot ; \alpha)$ parameterized by $\alpha$\footnote{The implicit assumption here is that $\exists \alpha^*$ such that $p_d(\cdot) = p_m(\cdot; \alpha^*)$.}.  
\end{compactitem}
So, can we get away with modeling the \textit{unnormalized} density $\widetilde{p}_m$ instead of requiring the normalization constraint to be baked in to our  optimization problem? Similar to approaches like \purple{contrastive divergence} and \purple{score matching}, \green{noise-contrastive estimation} (NCE) aims to address this question.

\myspace
\p \blue{Noise-contrastive estimation} (2.1). Let $c$ be an estimator for $-ln Z(\alpha)$, and let $\theta = \{\alpha, c\}$ denote all of our parameters. Given observed data $X = \vecseq[T]{x}$, and noise $Y = \vecseq[T]{y}$, we seek parameters $\hat{\theta}_T$ that maximize $J_T(\theta)$\footnote{This all assumes of course that $p_n$ is fully defined.}:
\graybox{
	J_T(\theta)
	&= \inv{2T} \sum_t \ln \left[ h(\vec[t]{x}) \right]
	+ \ln\left[         1 - h(\vec[t]{y}) \right] \\
	h(\vec u) 
	&= \sigma \left(    G\left(  \vec u  \right)  \right) \\
	G(\vec u)
	&= \ln p_m(\vec u) - \ln p_n(\vec u) \\
	\ln p_m(\cdot; \theta)
	&:= \ln \widetilde{p}_m(\cdot; \alpha) + c
}
where for compactness reasons I've removed the explicit dependence of all functions above (except $p_n$) on $\theta$. Notice how this fixes the issue of the model just setting $c$ arbitrarily high to obtain a high likelihood\footnote{The primary reason why MLE is traditionally unable to parameterize the partition function.}.

\myspace
\p \blue{Connection to supervised learning} (2.2). The NCE objective can be interpreted as binary logistic regression that discriminates whether a point belongs to the data ($p_d$) or to the noise ($p_n$).\marginnote{We model with a uniform prior: $P(C{=}1)=P(C{=}0)=1/2$   }[2em]
\begin{align}
P(C{=}1 \mid \vec u \in X \cup Y)
&= \frac{ P(C{=1}) p(\vec u \mid C{=}1) }{  p(\vec u) } \\
&= \frac{  p_m(\vec u) }{   p_m(\vec u) + p_n(\vec u)  } \\
&\equiv h(\vec u; \theta)
\end{align}
where we're now using the union of $X$ and $Y$, $U := \vecseq[2T]{u}$. The log-likelihood of the data under the parameters $\theta$ is
\begin{align}
\ell(\theta)
&= \sum_t^{2T} \left[
C_t \ln P(C_t{=}1 \mid \vec[t]{u};\theta) 
+ (1 - C_t) \ln P(C_t{=}0 \mid \vec[t]{u} ; \theta) \right] \\
&= \sum_t^{2T} \left[
C_t \ln h(\vec[t]{u}; \theta)
+ (1 - C_t) \ln \left[ 1 - h(\vec[t]{u}; \theta)  \right] \right] \\
&= \sum_t^T \left[
\ln h(\vec[t]{x}; \theta)
+ \ln \left[ 1 - h(\vec[t]{y}; \theta)  \right]
\right]
\end{align}
which is (up to a constant factor) the same as our NCE objective.


\myspace
\p \blue{Properties of the estimator} (2.3). As $T \rightarrow \infty$, $J_T(\theta) \rightarrow J(\theta)$, where
\begin{align}
J(\theta) 
&\triangleq \lim_{T \rightarrow \infty} J_T(\theta)
= \onehalf \E{\ln h(\vec x; \theta)     +    \ln\left[1 -  h(\vec y; \theta) \right]  } \\
\widetilde{J}(f)
&\triangleq \onehalf \E{ 
	\ln\left[   \sigma \mgreen{ \bigg( }  f\left(\vec x\right) - \ln p_n\left( \vec x \right)  \mgreen{ \bigg) }   \right] 
	+ \ln\left[  1 -  \sigma \mgreen{ \bigg( }  f\left(\vec y\right) - \ln p_n\left( \vec y \right)   \mgreen{ \bigg) }  \right]    }
\end{align}
\begin{definition}[-1em][Theorem 1]
	$\widetilde J$ attains exactly one maximum, located at $f(\cdot) = \ln p_d(\cdot)$, provided $p_d(\cdot){\ne}0 \implies p_n(\cdot){\ne}0$. 
\end{definition}




\Needspace{15\baselineskip}
\subsub{Self-Normalized NCE}

Notes from A. Mnih and Y. Teh, ``A fast and simple algorithm for training neural probabilistic language models'' (2012). 

\bluesec{Maximum likelihood learning}. Let $P_{\theta}^h(w)$ denote the probability of observing word $w$ given context $h$. For neural LMs, we assume this is the softmax of a scoring function $s_{\theta}(w, h)$ (logits). In what follows, I'll drop the explicit $\theta$ and $h$ subscript/superscript notation for brevity.
\begin{align}
\pderiv{\log P(w)}{\theta}
&= \pderiv{}{\theta} s(w, h) - \pderiv{}{\theta} \log \left[ \sum_{w'} e^{s(w', h)} \right] \\
&= \pderiv{}{\theta} s(w, h) - \sum_{w'} P(w') \pderiv{}{\theta} s(w', h) \\
&=  \pderiv{}{\theta} s(w, h) - \mred{ \E[w \sim P_{\theta}^{h}]{\pderiv{}{\theta} s(w, h)} }
\end{align}
where the expectation (in red) is expensive due to requiring $s(w, h)$ evaluated for all words in the vocabulary. One approach is \green{importance sampling} where we sample a subset of $k$ words from the vocab and compute the probabilities from that approximation:

\begin{align}
\pderiv{\log P(w)}{\theta}
&= \pderiv{}{\theta} s(w, h) - \sum_{w'} P(w') \pderiv{}{\theta} s(w', h) \\
&\approx  \pderiv{}{\theta} s(w, h) - \mgreen{ \inv{V} \sum_{j=1}^{k} v(x_j) }  \pderiv{}{\theta} s(w', h) \\
\text{where} 
\quad 
v(x)
&= \frac{ e^{s_{\theta}(x, h)}    }{Q^h(w \eq x)} 
\end{align} 
and we refer to $v$ as the \green{importance weights}. In NLP, we typically set $Q$ to the \green{Zipfian distribution}\footnote{\href{https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler}{TensorFlow} seems to define this as
	\begin{align}
	P_{Zipf}(w) &= \frac{  \log(w + 2) - \log(w + 1) }{   \log(V + 1)   }
	\end{align}
	where $V$ is the vocabulary size. I can't seem to find this definition anywhere else though. A more common form seems to be
	\begin{align}
	P(w) &= \frac{ \inv{w} }{  \sum_{w'} \inv{ w'^{s} } }
	\end{align}
	I plotted both on WolframAlpha (\href{https://www.wolframalpha.com/input/?i=plot+\%281\%2Fx+\%2F+5.187377517639620260805117675658253\%29+and+\%28log\%28x\%2B2\%29+-+log\%28x\%2B1\%29\%29+\%2F+log\%28100+\%2B+1\%29+for+x\%3D1+to+100}{link here}) and they do indeed look basically the same, especially for any reasonably large $V$. 
}

\bluesec{NCE}. In NCE, we introduce [unigram] noise distribution $P_n(w)$ and impose a prior that noise samples are $k$ times more frequent than data samples from $P_d^h(w)$, resulting in the joint distribution,
\begin{align}
P^h(D, w) 
&= P(D \eq 1) P_d^h(w) + P(D \eq 0) P_n(w) \\
&= \inv{k+1} P_d^h(w) + \frac{k}{k+1} P_n(w) 
\end{align}
Our goal is to learn the posterior distribution $P^h(D \eq 1 \mid w)$ (so we replace $P_d$ with $P_{\theta}$):\marginnote{NCE posterior}[2em]
\graybox{
	P^h(D \eq 1 \mid w, \theta)
	&= \frac{  P_{\theta}^h(w)  }{  P_{\theta}^h(w) + kP_n(w) } \label{eq:nce-posterior} 
}
In NCE, we re-parameterize $P_{\theta}$ by treating $-\log Z$ as a parameter itself, $c^h$\footnote{Reminder that the $h$ is a reminder that $Z$ is a function of the context $h$.}. 
\begin{align}
P_{\theta}^h(w) := P_{\theta^0}^h \exp(c^h)
\end{align}
where $P_{\theta^0}^h$ denotes the unnormalized distribution. It turns out that, in practice, we can impose that $\exp(c^h) \eq 1$ and use the unnormalized $P^h_{\theta^0}$ in place of the true probabilities in all that follows. \textit{Critically, note that this means we rewrite equation \ref{eq:nce-posterior} using the unnormalized probabilities in place of $P_{\theta}^h$}. The NCE objective is to find\footnote{I'll drop off $\theta$ dependence wherever obvious for the sake of compactness.} as follows, where I've shown each step of the derivation:
\begin{small}
	\begin{align}
	\theta^* 
	&= \argmax_{\theta} J^h(\theta) \\
	J^h(\theta)
	&= \E[(D,w) \sim P^h]{\log P(D \mid w, \theta)} \\
	&= \sum_{D=0}^{1} \sum_w P^h(D, w) \log P^h(D \mid w, \theta) \\
	&= \sum_w  P^h(0, w) \log P^h(0 \mid w)   + \sum_w  P^h(1, w) \log P^h(1 \mid w) \\
	&= \inv{k+1} \sum_w \left[
	k P_n(w) \log P^h(0 \mid w) 
	+ P_d^h(w) \log P^h(1 \mid w)
	\right] \\
	&= \inv{k+1} \left[
	k \E[P_n]{\log P^h(0 \mid w)}
	+ \E[P^h_d]{\log P^h(1 \mid w)}
	\right] \\
	&\propto k \E[P_n]{\log P^h(0 \mid w)}
	+ \E[P^h_d]{\log P^h(1 \mid w)}
	\end{align}
\end{small}
The gradient of the NCE objective is thus
\graybox{
	\pderiv{}{\theta} J^h(\theta)
	&= \sum_w \frac{ k P_n(w)  }{  P_{\theta}^h(w) + kP_n(w)   } \left( 
	P_d^h\left( w \right) - P_{\theta}^h\left( w \right) 
	\right)
	\pderiv{}{\theta} \log P_{\theta}^h(w)
}
\red{TODO}: incorporate more info from \href{http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf}{Chris Dyer's excellent notes}.



% ===========================================================================================
\lecture{Language Modeling}{Improving Neural Language Models with a Continuous Cache}{March 24, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize E. Grave, A. Joulin, and N. Usunier, ``Improving Neural Language Models with a Continuous Cache,'' \textit{Facebook AI Research} (Dec 2016).}

The cache stores pairs $(h_t, x_{t+1})$ of the final hidden-state representation at time t, along with the word which was \textit{generated}\footnote{They say this, but everything else in the paper strongly suggests they mean the next gold-standard input instead.} based on this representation. 
\graybox{
	p_{vocab}(w \mid \slice{x})
	&\propto \exp\left( h_t^T o_w \right) \\
	p_{cache}(w \mid \slice{h}, \slice{x}) 
	&\propto \sum_{i=1}^{t - 1} \ind_{x_{i+1}{=}w} \exp\left(\theta h_t^T h_i \right) \\
	&= \sum_{ \substack{ (x, h) \in cache \\ s.t. ~ x{=}w } }  \exp\left(\theta h_t^T h \right) \\
	p( w \mid \slice{h}, \slice{x} )
	&= (1 - \lambda) p_{vocab}(w \mid h_t) + \lambda p_{cache}(w \mid \slice{h}, \slice{x})
}
where $\theta$ is a scalar parameter that controls the flatness of the cache distribution. 





% ===========================================================================================
\lecture{Language Modeling}{Context Dependent RNN Language Model}{June 02, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize T. Mikolov and G. Zweig, ``Context Dependent Recurrent Neural Network Language Model,'' \textit{BRNO and Microsoft} (2012).}

\bluesec{Model Structure (2)}. Given one-hot input vector $\vec[t]{x}$, output a probability distribution $\vec[t]{y}$ for the next word. Incorporate a \textit{feature vector} $\vec[t]{f}$ that will contain topic information.
\begin{align}
\vec[t]{y}
&= \text{Softmax}\left( \matr V \vec[t]{h} + \matr G \vec[t]{f} \right) \\
\vec[t]{h}
&= \sigma\left( \matr U \vec[t]{x} + \matr W \vec[t-1]{h} + \matr F \vec[t]{f}  \right)
\end{align}

\bluesec{LDA for Context Modeling (3)}. ``Documents'' fed to LDA here will be individual sentences. The generative process assumed by LDA is compactly defined by the following sequence of operations\footnote{$\alpha$ is a vector with number of elements equal to number of topics.}:\marginnote{N: number of words\\$\Theta_i \equiv p(\text{topic}[i])$\\$z_n$: topic of word $n$}[2em]
\begin{align}
N &\sim \text{Poisson}(\xi) \\
\Theta &\sim \text{Dir}(\alpha) \\
z_n &\sim \text{Multinomal}(\Theta) \\
w_n &\sim \Prob{w_n \mid z_n, \beta}
\end{align}
where $\Prob{w_n \eq a \mid z_n \eq b} = \beta_{b, a}$, so we are really just sampling from row $z_n$ of $\beta$, where $\beta \in [0, 1]^{Z \times V}$ (where Z is number of topics). The result of LDA is a learned value for $\alpha$, and the topic distributions $\beta$.

\graybox{
	\vec[t]{f}
	&= \inv{Z} \prod_{i=0}^{K} \vec[ x_{t-i} ]{t} \\
	\vec[t]{f}
	&= \inv{Z} \vec[t-1]{f}^{\gamma} \vec[x_t]{t}^(1 - \gamma)
}








% ===========================================================================================
\lecture{Language Modeling}{Strategies for Training Large Vocabulary Neural Language Models}{July 27, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Chen et al., ``Strategies for Training Large Vocabulary Neural Language Models,'' \textit{FAIR} (Dec 2018). arXiv:1512.04906}


\bluesec{Setup/Notation}. Note that in everything below, the authors are using a rather primitive feed-forward network as their language model. To predict $w_t$ it just concatenates the embeddings of the previous $n$ words and feeds it through a $k$-layer FF network. Then, layer $k+1$ is the dense projection and softmax:
\begin{align}
h^{k+1} &= W^{k+1} h^k + b^{k+1} \in \R^V \\
y &= \inv{Z} \exp\left\{ h^{k+1}  \right\}
\end{align}

Using cross-entropy loss, the derivative of $\log p(w_t\eq i)$ wrt the $j$th element of the logits is:
\begin{align}
\pderiv{ \log y_i }{ h_j^{k+1} }
&= \pderiv{}{h_j^{k+1}} \left[  
h^{k+1}_i - \log Z 
\right] \\
&= \delta_{ij} - y_j
\end{align}
When computing gradients of the cross-entropy loss, $y_i$ here is the ground truth. Therefore, to increase the probability of the correct token, we need to increase the logits element for that index, and decrease the elements for the others. Note how this implies we must compute the final activations for \textit{all words in the vocabulary}. 

\bluesec{Hierarchical Softmax} (2.2). Group words into one of two clusters $\{c_1, c_2\}$, based on unigram frequency\footnote{For example, you could just put the top 50\% in $c_1$ and the rest in $c_2$.}. Then model $p(w_t \mid x) = p(c_t \mid x) p(w_t \mid c_t)$ where $c_t$ is the class that word $w_t$ was assigned to.

\bluesec{NCE} (2.5). Define $P_{noise}(w)$ by the unigram frequency distribution. For each real token $w_t$ in the training set, sample $K$ noise tokens $\{ n_k \}_{k \eq 1}^{K}$. NCE aims to minimize
\begin{align}
L_{NCE}(   \vecseq[N]{w}   ) &= \sum_{i=1}^{N} \sum_{t=1}^{len(\vec w^{(i)})  } \left[ 
\log h(w_t^{(i)}) + \sum_{k=1}^{K} \log (  1 - h( n_k^{(i)}  )   ) 
\right] \\
h(w_t) 
&= \frac{ P_{model}(w) }{   P_{model}(w) + k P_{noise}(w)   } \\
&\approx \frac{ \widetilde P_{model}(w) }{  \widetilde  P_{model}(w) + k P_{noise}(w)   } 
\end{align}
where the final approximation is what makes NCE less computationally expensive in practice than standard softmax. This would seem to imply that NCE should approach standard softmax (in terms of correctness) as $k$ increases. 


\bluesec{Takeaways}.
\begin{compactitem}
	\item Hierarchical softmax is the fastest.
	\item NCE performs well on large-volume large-vocab datasets.
	\item Similar NCE values can result in very different validation perplexities. 
	\item Sampled softmax shows good results if the number of negative samples is at 30\% of the vocab size or larger. 
	\item Sampled softmax has a lower ppl reduction per step than others. 
\end{compactitem}





% ===========================================================================================
\lecture{Language Modeling}{Large Memory Layers with Product Keys}{August 03, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Lample et al., ``Large Memory Layers with Product Keys,'' \textit{FAIR} (July 2019). arXiv:1907.05242v1}


\bluesec{Memory Design} (3.1). The high-level structure (sequence of ops) is as follows:
\begin{compactenum}
	\item \green{Query network} computes some query vector $\vec q$. 
	
	\item Compare $\vec q$ with each \green{product key} via some scoring function.
	
	\item Select the $k$ highest scoring product keys. 
	
	\item Compute output $\vec m (\vec x)$ as weighted sum over the values associated with each of the top $k$ keys from the previous step.
\end{compactenum}
The \textbf{query network} is usually just a dense layer\footnote{They choose $d_q = 512$ as the output dimensionality of their query network.}. Since they want it to output query vectors with good coverage over the key space, the put a batch normalization layer before the query network\footnote{Recall that batch norm just normalizes the batch inputs to have 0 mean and unit standard deviation, followed by a scaling and bias factor.}. \\

The \textit{standard} way of doing key assignment/weighting is as follows:
\graybox{
	\mtgreen{[KNN]}& 
	\quad 
	\mathcal I \triangleq \text{TopK}(\vec q (\vec x)^T \vec[i]{k}) 
	\qquad 
	1 \le i \le \mathcal K \label{eq:standard-top-k} \\
	\mtgreen{[Normalize]}&
	\quad
	\vec w = \text{Softmax}\left( \mathcal I   \right) \\
	\mtgreen{[Aggregate]}&
	\quad
	\vec m(\vec x) = \sum_{i \in \mathcal I} w_i \vec[i]{v}
}
where equation \ref{eq:standard-top-k} is inefficient for large memory (key-value) stores. The authors propose instead a structured set of keys that they call \green{product keys}. Spoiler alert: it's just product quantization with $m \eq 2$ subvectors. Instead of using the flat key set $\mathcal K \triangleq \vecseq[|\mathcal K|]{k}$ with each $\vec[i]{k} \in \R^{d_q}$ from earlier, we redefine it as
\graybox{
	\mathcal K \triangleq \{  (\vec c , \vec c' ) \mid c \in \mathcal C, ~ c' \in \mathcal C'   \}
}
where both $\mathcal C$ and $\mathcal C'$ are sets of \textit{sub-keys} $\vec[i]{k} \in \R^{d_q/2}$. 

\Needspace{5\baselineskip}
Then\textellipsis
\begin{compactenum}
	\item Just run each of subvectors $q_1$ and $q_2$ through the standard TopK. You'll have $k$ sub-keys for both, defined by their index into their respective codebook. 
	
	\item Let $\mathcal K := \{ (\vec[i]{c}, \vec[j]{c}') \mid i \in \mathcal I_{\mathcal C}, ~ \mathcal I_{\mathcal C'}  \}$. This new reduced-size key set $\mathcal K$ has only $k \times k$ entries.
	
	\item Run the standard algorithm using the new reduced key set $\mathcal K$. 
\end{compactenum}

\red{TODO}: finish this note








% ===========================================================================================
\lecture{Language Modeling}{XLNet}{August 17, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Yang et al., ``XLNet: Generalized Autoregressive Pretraining for Language Understanding'' \textit{CMU \& Google Brain} (May 2018).}

\bluesec{TL;DR}: Instead of minimizing the NLL using $p(w_1, \ldots, w_T)$, minimize over NLL's using every possible order of the given word sequence. 

\bluesec{Background}. Recall that BERT does denoising auto-encoding. Given text sequence $\vec x = \seq{x}$, BERT constructs a corrupted version $\hat x$ by randomly masking out some tokens. Let $\bar{\vec x}$ denote the tokens that were masked. The BERT training objective is then
\begin{align}
\mtgreen{[BERT]} ~ \max_{\theta} ~~
\log p_{\theta} (\bar{\vec x} \mid \hat{\vec x})
&\approx 
\sum_{\bar x \in \bar{\vec x}} \log p_{\theta} (\bar x \mid \hat{\vec x}) \\
p(\bar x \mid \hat{\vec x})
&=  \text{Softmax}\left(  H_{\theta}(\hat{\vec x})_t^T e(\bar x) \right)
\end{align}

\bluesec{Objective \& Architecture}. Their proposed \green{permutation language modeling objective} is:
\graybox{
	\max_{\theta} ~~  \E[\vec z \sim \mathcal{Z}_T]{  \sum_{t=1}^{T} \log p_{\theta}(x_{z_t}    \mid  \slice[z_{t-1}][z_1]{x}   )   }
}
where $\mathcal Z_T$ is the set of all possible permutations of the length-$T$ index sequence $[1..T]$. To implement this, the authors had to re-parameterize the next-token distribution to be \textbf{target position aware}:
\graybox{
	p_{\theta}(X_{z_t} \eq x \mid \slice[z_{t-1}][z_1]{x})
	&= \text{Softmax}\left(   
	g_{\theta}\left( \slice[z_{t-1}][z_1]{x} , \mred{ z_t } \right)^T 
	e\left( x\right)      
	\right)
}
They accomplish this via \green{two-stream self-attention}, a technique that utilizes two sets of hidden representations (instead of one):
\begin{compactitem}
	\item \green{Content representation}: $h_{z_t} \triangleq h_{\theta}(\slice[z_{t}][z_1]{x})$.
	
	\item \green{Query representation}: $g_{z_t} \triangleq g_{\theta}(\slice[z_{t-1}][z_1]{x}, z_t)$.
\end{compactitem}

\Needspace{10\baselineskip}
The query stream is initialized with some vector $g_i^{(0)} \eq w$, and the content stream is initialized with word embedding $h_i^{(0)} \eq e(x_i)$. For the subsequent attention layers $1 \le m \le M$, they are computed respectively as follows: 
\begin{align}
g_{z_t}^{(m)}
&\leftarrow \text{Attention}(  Q \eq g_{z_t}^{(m-1)},  K \eq  V \eq \vec[z_{<t}]{h}^{(m-1)}      )
\\
h_{z_t}^{(m)}
&\leftarrow \text{Attention}(  Q \eq h_{z_t}^{(m-1)},  K \eq  V \eq \vec[z_{ \leq t}]{h}^{(m-1)}      )
\end{align}


In practice, in order to speed up optimization, the authors do \green{partial prediction}: only train to predict over $\vec[z_{>c}]{x}$ targets rather than all of them.

\bluesec{Incorporating Ideas from Transformer-XL}. Often times, sequences are too long to feed all at once. The authors adopt relative positional encoding and segment-level recurrence from Transformer-XL. To compute the attention update with memory on a given segment, we use the content representations from the \textit{previous} segment, $\widetilde{\vec h}$, along with the current segment, $\vec[z_{\leq t}]{h}$ as follows:
\begin{align}
h_{z_t}^{(m)}
&\leftarrow \text{Attention}\left(
Q \eq h_{z_t}^{(m-1)}, 
K \eq V \eq \left[
\widetilde{\vec h}^{(m-1)};
\vec[z_{\leq t}]{h}^{(m-1)}
\right]
\right)
\end{align}





% ===========================================================================================
\lecture{Language Modeling}{Transformer-XL}{August 24, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Dai et al., ``Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context'' \textit{CMU \& Google Brain} (Jan 2019).}

\DeclareDocumentCommand{\xlh}{ m m }{ \vec[#1]{h}^{#2} }
\DeclareDocumentCommand{\xlhtilde}{ m m }{ \mgreen{ \vec[#1]{\widetilde{h}}^{#2} } }
\DeclareDocumentCommand{\xls}{ m m }{ \vec[#1]{s}^{#2} }
\DeclareDocumentCommand{\xlq}{ m m }{ \vec[#1]{q}^{#2} }
\DeclareDocumentCommand{\xlk}{ m m }{ \vec[#1]{k}^{#2} }
\DeclareDocumentCommand{\xlv}{ m m }{ \vec[#1]{v}^{#2} }

\bluesec{Segment-Level Recurrence with State Reuse}. Denote two consecutive segments of length $L$ as $\vec[\tau]{s} = [x_{\tau, 1}, \ldots, x_{\tau, L}]$ and $\vec[\tau + 1]{s} = [x_{\tau +1, 1}, \ldots, x_{\tau + 1, L}]$. Denote output of layer $n$ given input segment $\vec[\tau]{s}$ as $\vec[\tau]{h}^n \in \R^{L \times d}$, where $d$ is the hidden dimension. To obtain the output of layer $n$ given the next segment, $\vec[\tau + 1]{s}$, do:
\graybox{
	\xlh{\tau + 1}{n}
	&= \text{TransformerLayer}( \xlq{\tau + 1}{n}, \xlk{\tau + 1}{n}, \xlv{\tau + 1}{n}  ) \\
	&= \text{TransformerLayer}( 
	\mpurple{ \xlh{\tau + 1}{n - 1} } \matr[q]{W}^T, ~
	\xlhtilde{\tau + 1}{n - 1} \matr[k]{W}^T, ~
	\xlhtilde{\tau + 1}{n - 1} \matr[v]{W}^T) \\
	\xlhtilde{\tau + 1}{n-1}
	&= \left[
	\text{SG}\left(   \xlh{\tau}{n -1}   \right)
	;
	\mpurple{ \xlh{\tau + 1}{n - 1} }
	\right] \label{eq:transformer-xl-concat}
}
where the concat in \ref{eq:transformer-xl-concat} is along the length (time) dimension. In other words, $\matr Q$ remains the same, but $\matr K$ and $\matr V$ get the previous segment prepended. Ultimately this only changes the inner dot products in the attention mechanism to attend over both segments. The $L$ output attention vectors are therefore each weighted sums over the previous $2L$ timesteps instead of just $L$. 


\bluesec{Relative Positional Encodings}. Instead of absolute positional encodings (as regular transformers do), only encode the \textit{relative} positional information in the hidden states. Ignoring the scale factor of $1/\sqrt{d_{k}}$, we can write the score for query vector $q_i = W_q (e_{x_i} + u_i)$ and key vector $k_j = W_k (e_{x_j} + u_j)$, for input embeddings $e$ and positional encodings $u$ as follows. Below it we show the authors proposed re-parameterized relative encoding version.
\graybox{
	A_{i,j}^{abs}
	&= \mgreen{ e_{x_i}^T } W_q^T W_k \mgreen{ e_{x_j} }
	+ \mgreen { e_{x_i}^T } W_q^T W_k \mblue{ u_j } 
	+ \mblue{ u_i^T } W_q^T W_k \mgreen{ e_{x_j} }
	+\mblue{ u_i^T } W_q^T W_k \mblue{ u_j } \\
	A_{i,j}^{res}
	&= 
	\underbrace{ \mgreen{ e_{x_i}^T } W_q^T W_{k, \mred{E}} \mgreen{ e_{x_j} } }_{     \mscript{cont-based addr}  }
	+ \underbrace{ \mgreen { e_{x_i}^T } W_q^T W_{k, \mred{R}} \mred{ r_{i - j} } }_{ \mscript{cont-dep pos bias} }
	+ \underbrace{ \mred{ \vec{u}^T }  W_{k, \mred{E}} \mgreen{ e_{x_j} } }_{  \mscript{global cont bias} }
	+\underbrace{ \mred{ \vec{v}^T } W_{k, \mred{R}} \mred{ r_{i-j} } }_{\mscript{global pos bias}}
}
where content is abbreviated as ``cont'' and positional is abbrev as ``pos''. I've shown all differences introduced by the second version in \red{red} font. It appears that $r_{i - j}$ is literally just $u_{i - j}$ but I guess using new letters is cool. Note that they separate $W_k$ into \green{content-based} $W_{k, E}$ and \green{location-based} $W_{k, R}$. 


% ===========================================================================================
\lecture{Language Modeling}{Efficient Softmax Approximation for GPUs}{August 31, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Grave et al., ``Efficient Softmax Approximation for GPUs'' \textit{FAIR} (June 2017).}

\begin{algorithm}[Adaptive Softmax: Two-Level]
	Partition the vocabulary $\mathcal V$ into two clusters $\mathcal{V}_h$ and $\mathcal V_t$, where
	\begin{compactitem}
		\item $\mathcal V_h$ denotes the \textit{head}, consisting of the most frequent words.
		
		\item $\mathcal V_t$ denotes the \textit{tail}, associated with a \textbf{large number} of rare words.
		
		\item $|\mathcal V_h | << |\mathcal V_t|$ and $P(\mathcal V_h) >> P(\mathcal V_t)$.  
	\end{compactitem}
	
	To compute the probability of some word $w$ given context $h$, do:
	\graybox{
		\Prob{w \mid h}
		&= \begin{cases}
			P_{\mathcal V_h} (w \mid h) & \text{if } w \in \mathcal V_h \\
			P_{\mathcal V_t}(w \mid h) P_{\mathcal V_h} (\text{tail} \mid h) & \text{otherwise}
		\end{cases}
	}
	
	where both $P_{\mathcal V_h}$ and $P_{\mathcal V_t}$ are modeled with a softmax over the words in their respective clusters ($P_{\mathcal V_h}$ also includes the special ``tail'' token). 
	
\end{algorithm}

More generally, we can extend the above algorithm to N clusters (instead of 2). We can also adapt the \textit{capacity} of each cluster (varying their embedding size). The authors recommend, for each successive tail cluster, reducing the output size by a factor of 4. Of course, this then has to be followed by projecting back up to the number of words associated with the given cluster.

\red{TODO}: detail out how cross entropy loss is computed under this setup.






% ===========================================================================================
\lecture{Language Modeling}{Adaptive Input Representations for Neural Language Modeling}{September 02, 2019}

% ===========================================================================================
\vspace{-1em}
{\footnotesize A. Baevski and M. Auli, ``Adaptive Input Representations for Neural Language Modeling'' \textit{FAIR} (Feb 2019).}

\bluesec{TL;DR}: Literally just adaptive softmax but for the input embeddings. Official implementation can be found \href{https://github.com/pytorch/fairseq/blob/master/fairseq/modules/adaptive_input.py}{here}. 


\bluesec{Adaptive Input Representations} (3). Same as Grave et al., they partition the vocabulary $\mathcal V$ into
\begin{align}
\mathcal V &= \mathcal V_1 \cup \mathcal V_2 \cup \ldots \cup  \mathcal V_n
\end{align}
where $\mathcal V_1$ is the head and the rest are the tails (ordered by decreasing frequency). They reduce the capacity of each cluster by a factor of $k \eq 4$ (also same as Grave et al.). Finally, they add linear projections for each cluster's embeddings in order to ensure they all result in $d$-dimensional output embeddings (even $\mathcal V_1$). 






% ======================================================================
\lecture{Language Modeling}{Fine-Tuning Language Models from Human Preferences}{January 18, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Ziegler et al., ``Fine-Tuning Language Models from Human Preferences'' \textit{OpenAI}, (Sep 2019).}


\bluesec{Methods} (2). Setup: vocab $\Sigma$, language model $\rho$. Want to model probability of response sequence $y$ given input sequence $x$.  They initialize a \green{policy} $\pi = \rho$, then fine-tune $\pi$ with RL. Instead of \textit{defining} a reward function $r: X \times Y \mapsto \R$, the \textit{learn} one using a dataset $S$ of human annotations. Each element of $S$ is a tuple $(x, y_0, y_1, y_2, y_3, b)$ -- each $y_i$ is a multiple-choice option presented to the human labeler, and the human selects option $0 \le b < 3$. They train the reward model $r$ with loss
\begin{align}
\text{loss}(r)
&= \E[x, \{y_i\}, b \sim S]{\log \frac{ e^{r(x, y_b)} }{ \sum_i e^{r(x, y_i)}   }}
\end{align}
They initialize $r$ as a random linear function of the final embedding output of $\rho$. They fine-tune $\pi$ to optimize $r$, performing RL on the modified reward:
\graybox{
	R(x, y) = r(x,  y) - \beta \log \frac{\pi(y \mid x)}{\rho(y \mid x)}
}

\red{how is this any different from just continuing training the LM on the human annotations?}






% ======================================================================
\lecture{Language Modeling}{Cross-lingual Language Model Pretraining}{April 13, 2020}
% ======================================================================

\citepaper{G. Lample and A. Conneau}{Cross-lingual Language Model Pretraining}{FAIR}{Jan 2019}


\bluesec{Cross-lingual Language Models} (3). Use single shared vocabulary across all languages using BPE. Learn BPE splits by sampling sentences randomly from each corpus\footnote{Each corpus is associated with a unique language. There are $N$ languages (and thus corpora) total.} $C_i$ containing $n_i$ sentences, according to a multinomial distribution with probabilities $\{q_i\}_{i=1}^{N}$:\marginnote{Authors set $\alpha=0.7$}[3em]
\begin{align}
	q_i = \dfrac{p_i^{\alpha}}{\sum_{j=1}^N p_j^{\alpha}} 
	\quad \text{with} \quad 
	p_i = \dfrac{n_i}{\sum_{k=1}^{N} n_k}
\end{align}
\begin{myquote}[-1.5em]
	Sampling with this distribution increases the number of tokens associated to low-resource languages and alleviates the bias towards high-resource languages. In particular, this prevents words of low-resource languages from being split at the character level.
\end{myquote}

They experiment with the following language modeling objectives:\marginnote{They use a batch size of 64, sequence length of 256.}[10em]
\begin{compactitem}
	\item \green{CLM}: causal language modeling. Predict next word given context.
	
	\item \green{MLM}: masked language modeling. Predict masked-out tokens. 
	
	\item \green{TLM}: translation language modeling. Extension of MLM when parallel data available. Concatenate parallel sentences and predict masked-out tokens.
\end{compactitem}
\myfig[0.75\textwidth]{figs/clm_objectives.png}

\bluesec{Cross-lingual Language Model Pretraining} (4). After LM pretraining, add linear classifier on top of first-state final-layer of transformer and finetune all params on \textit{English} NLI training data. They show how the model can still be used for non-English NLI queries at evaluation. This is what they are referring to by ``zero-shot'' classification -- the fact that they can classify on non-English even though they only finetuned on English.


\bluesec{Results \& Analysis} (5). Simply training on multiple language can improve perplexity on low-resource languages. The table below shows how perplexity on Nepali improves by including various other languages during training.
\myfig[0.5\textwidth]{figs/clm_tab4.png}




% ======================================================================
\lecture{Language Modeling}{Learning and Evaluating General Linguistic Intelligence}{May 09, 2020}
% ======================================================================

\citepaper{Yogatama et al.}{Learning and Evaluating General Linguistic Intelligence}{DeepMind}{Jan 2019}

\bluesec{Introduction} (1). Authors argue for an evaluation paradigm that rewards abilities to
\begin{compactenum}
	\item Deal with the full complexity of natural language across a variety of tasks.
	\item Effectively store/reuse representations, combinatorial modules, and previousy acquired knowledge to avoid \purple{catastrophic forgetting}.
	\item Adapt to new linguistic tasks/environments with little experience.
\end{compactenum}

\bluesec{Tasks} (2). Below are the tasks \& their respective datasets we'll explore. 
\begin{compactitem}
	\item \textbf{Reading Comprehension}. SQuAD 1.1, TriviaQA, QuAC. 
	\item \textbf{Semantic Role Labeling}. QA-SRL Bank 2.0. 
	\item \textbf{Relation Extraction}. ZeroShot. 
	\item \textbf{Natural Language Inference}. MNLI, SNLI. 
\end{compactitem}

\bluesec{Models} (3). They focus on two classes of models:
\begin{compactitem}
	\item Self-attention models. They use BERT$_{\text{BASE}}$.
	\item Recurrent neural networks. Augment pretrained ELMo with a 300-dim BiLSTM\footnote{So they just stick another BLSTM at the outputs? There are already 2 BiLSTMs though\textellipsis why do this?}. Then use BiDAF to ``to aggregate context (premise) and question (hypothesis) representations to predict an answer span (a label) in question answering (natural language inference).''
\end{compactitem}

\bluesec{Evaluating Transfer} (4). Say there exists some set of $N$ examples $\mathcal A = \{(x_i, y_i)\}_{i=1}^N$ that we want to learn rapidly. We'll denote slices of this with the shorthand $\mathcal A_{i} \triangleq \{(x_j, y_j)\}_{j=1}^i$. Let $\matr W$ denote the model parameters, and $\matr[\mathcal A_i]{\hat W}$ the model parameters trained on$\mathcal A_i$.\marginnote{$|\mathcal Y|$ is the number of classes}[2em] Consider an online setup where the \green{codelength} is computed as:
\begin{align}
	\ell (\mathcal A) &= \lg |\mathcal Y| - \sum_{i=2}^{N} \lg p(y_i \mid x_i ;  \matr[\mathcal A_{i-1}]{\hat W})
\end{align}

\Needspace{10\baselineskip}
My interpretation of the authors' [vague] interpretation of $\ell$ is as follows: Alice wants to communicate all $y_i$'s in $\mathcal A$ to Bob, who only has access to the $x_i$'s. They both agree on a model, initial seed, and learning algorithm. Alice first communicates $y_1$ to Bob with a uniform code (I guess this would be a binary code of $\lg |\mathcal Y|$ bits). They then train their model on $\mathcal A_1 = (x_1, y_1)$. Alice uses this model to to communicate $y_2$ (presumably a smaller code than the initial uniform code). I have no idea in practice how this pretend scenario would be implemented, but the gist is just:
\begin{myquote}
	\textellipsis a model that performs well with a limited number of training examples will be rewarded by having a shorter codelength.
\end{myquote}

\bluesec{Experiments} (5). Here I'll list the main takeaways:
\begin{compactitem}
	\item \textbf{Unsupervised Pretraining} (5.1)
	\begin{compactitem}
		\item[\red \xmark]  Both BERT and ELMo required $\sim40k$ training examples to converge in finetuning on SQuAD1.1 and MNLI, even though they were already pretrained. 
		\item[\green\cmark] BERT was slighter faster than ELMo. 
		\item[\red\xmark] Training from scratch for 500k iterations yields much poor results than the pretrained+finetuned models of $\sim40k$ iterations. 
		\item[\green\cmark] Bert has smaller codelengths than $ELMo$ (learns from fewer examples better). 
	\end{compactitem}

	\item \textbf{Beyond Unsupervised Pretraining} (5.2). They also include supervised pretraining into the mix and observe impacts on exact match, F1, and codelength. 
	\begin{compactitem}
		\item[\green\cmark] Slightly improves EM and F1. 
		\item[\green\cmark] Significantly improves codelength. Kind of misleading/trivial though since the supervised pretraining trains the final layer prediction head that's also used for the final training/eval on SQuAD. Basically gives the models a head-start (nice pun). 
	\end{compactitem}

	\item \textbf{Generalization} (5.3). Do these models generalize to other datasets from the same task?
	\begin{compactitem}
		\item[\red\xmark] High-performing SQuAD models do \textit{not} perform well on other [QA] datasets w/o being provided w/training examples from these datasets.
	\end{compactitem}

	\item \textbf{Curriculum and Catastrophic Forgetting} (5.4). Take the best unsupervised+SQuAD-trained models from 5.1 and observe how performance on SQuAD changes as they subsequently finetune on MNLI or TriviaQA. 
	\begin{compactitem}
		\item[\red\xmark] None of the models that obtain good results on the new task are able to maintain performance on SQuAD.
		\item[\green\cmark] Training on all tasks simultaneously\footnote{By randomly sampling examples from each task uniformly throughout training.} can result in a model that performs well on all of them (especially BERT). 
	\end{compactitem}
\end{compactitem}

\bluesec{Discussion} (6). 
\begin{compactitem}
	\item Need better \textit{transfer and continual learning methods}, such as EWC and progress-and-compress. 
	
	\item Underexplored promising directions:
	\begin{compactitem}
		\item  \textit{memory module} that rapidly adapts to domain shifts and generalizes across taks.
		\item \textit{meta-learning} for NLP that doesn't require knowledge of all tasks we want to evaluate on beforehand.
	\end{compactitem}

	\item \textit{Training curriculum} can have a considerable effect on model performance. 
\end{compactitem}





% ======================================================================
\lecture{Language Modeling}{Scaling Laws for Neural Language Models}{May 09, 2020}
% ======================================================================

\citepaper{Kaplan et al.}{Scaling Laws for Neural Language Models}{OpenAI}{Jan 2020}

\bluesec{Introduction}. Key findings for transformer language models:
\begin{compactitem}
	\item \textbf{Performance depends strongly on scale, weakly on model shape}. Performance depends strongly on $(N, D  C)$:
	\begin{compactitem}
		\item $N$: number of model parameters.
		\item $D$: size of the dataset.
		\item $C$: amount of compute used for training.
	\end{compactitem}
	
	\item \textbf{Smooth power laws}. Test loss decreases reliably as a power law wrt the 3 aforementioned scale factors. 
	
	\myfig[0.6\textwidth]{figs/scaling_laws_fig_1.png}
	
	Notice how the plot above contains predictive fits that we can use for estimating performance gain from increasing each. 
	
	\item \textbf{Universality of overfitting}. 
	
	\item \textbf{Universality of training}. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. 
	
	\item \textbf{Transfer improves with test performance}. Transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set.
	
	\item \textbf{Sample efficiency}. Larger models are better.
	
	\item \textbf{Convergence is inefficient}. Just train a fat model and stop way before convergence. 
	
	\item \textbf{Optimal batch size} can be computed via gradient noise scale. 
\end{compactitem}

\Needspace{10\baselineskip}
The main scaling law equations are as follows.
\graybox{
	L(N)
		&= \lr{  \frac{N_c}{N} }^{\alpha_N} \qquad \alpha_N {\sim} 0.076, ~N_c {\sim} 8.8e13 \mtext{ non-embed params} \\
	L(D)
		&=  \lr{  \frac{D_c}{D} }^{\alpha_D} \qquad \alpha_D {\sim} 0.095, ~D_c {\sim} 5.4e13 \mtext{ tokens} \\
	L(C_{min})
		&=  \lr{  \frac{C_c^{min}}{C_{min}} }^{\alpha_{C}^{min}} \qquad \alpha_{C}^{min} {\sim} 0.050, ~C_c^{min} {\sim} 3.1e8 \mtext{ PF-days} \\
	B_{crit}(L)
		&= \frac{B_*}{L^{\inv{\alpha_B}}} \qquad B_* {\sim} 2e8 \mtext{ tokens}, ~ \alpha_B {\sim} 0.21 \\
	L(N, D)
		&= \left[
			\lr{ \frac{N_c}{N}  }^{\frac{\alpha_N}{\alpha_D}} + \frac{D_c}{D} 
		\right]^{\alpha_D}
}

Other noteworthy quotes I found throughout the paper:
\begin{compactitem}
	\item \textit{We found that results at convergence were largely independent of learning rate schedule.} (2.2)
	\item \textit{This suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in recent work.} (3.2)
	\item \textit{LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match the Transformer performance for later tokens.} (3.2.1)
	\item \textit{We also observe no dependence [on out-of-domain generalization] on model depth} (3.2.2)
	\item Datasets with about 20 million tokens may represent a different regime for language modeling -- overfitting happens very early. (4.2)
	\item Lower bound on the step that early stopping should occur (5.3)
	\begin{align}
		S_{stop}(N, D)
			&\geq \frac{S_c}{\left[  
					L(N, D) - L(N, \infty)
				\right]^{\inv{\alpha_S}}}
	\end{align}
\end{compactitem}


% ======================================================================
\lecture{Language Modeling}{Language Models are Few-Shot Learners}{May 30, 2020}
% ======================================================================

\citepaper{Brown et al.}{Language Models are Few-Shot Learners}{OpenAI}{May 2020}

% ------------ Introduction ------------ 
\bluesec{Introduction} (1). Major limititation is needing task-specific data and finetuning. Especially since large pre-trained models can exploit spurious correlations during finetuning. Humans typically just need a \textit{directive} in natural language \footnote{For example, ``Tell me if this sentence is positive or negative.''} to learn a new task. Meta-learning, or \green{in-context learning} for the LM case, may address some of these issues:
\begin{myquote}
    \textellipsis the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.
\end{myquote}

\myfig[0.8\textwidth]{figs/gpt3_fig1_1.png}

Authors train \green{GPT-3}, a 175 billion parameter LM, and measure:
\begin{compactitem}
    \item \green{Few-shot learning}: in-context learning. Allow as many demonstrations as will fit into model's context window\footnote{This is typically about 10 to 100 demonstrations.}.
    \item \green{One-shot learning}: one demonstration.
    \item \green{Zero-shot learning}: no demonstrations -- only a natural-language instruction.
\end{compactitem}
Authors find that larger models perform significantly better, although still struggle on some tasks like ANLI (natural language inference), RACE/QuAC(reading comprehension). Finally, authors include a systematic study of data contamination.

\textbf{Thoughts}: I wonder whether in-context learning is an inherent property of the model, or whether it's crucial that the training data itself has examples of this type of structure (multiple demonstrations in sequence).

% ------------ Approach ------------ 
\bluesec{Approach} (2). Below is the set of models they trained. They are the same architecture as GPT-2, ``with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer.''\marginnote{$$d_{ff} = 4 \cdot d_{model}$$ $$n_{ctx} = 2048$$}[2em]

\myfig[0.8\textwidth]{figs/gpt3_tab2_1.png}

I found it surprising how the learning rate decreases as model size (and batch size) increases, which is typically the opposite.

\textbf{Training Dataset} (2.2). Based on Common Crawl, with some processing:
\begin{compactenum}
    \item Filtered based on similarity to a range of high-quality reference corpora.
    \item Fuzzy deduplication at the document level\footnote{May want to try this technique on some of my LM datasets.}.
    \item Added known high-quality reference corpora to the training mix (WebText, Books1, Books2, English-language Wikipedia).
\end{compactenum}
During training, they sample from the different datasets \textit{proportional to their quality}, as show in the table below.
\myfig[0.8\textwidth]{figs/gpt3_tab2_2.png}

\textbf{Evaluation} (2.4). For few-shot learning, they evaluate each example in the evaluation set by randomly drawing K examples from that
task’s \textit{training set} as conditioning, delimited by 1 or 2 newlines depending on the task.

% ------------ Results: Language Modeling, Cloze, and Completion Tasks ------------ 
\bluesec{Language Modeling, Cloze, and Completion Tasks} (3.1). New SOTA perplexity on \underline{PTB} of 20.5, beating previous SOTA of 35.8. For \underline{LAMBADA}, model is asked to predict last word of sentences which require reading a paragraph of context.
\myfig[0.6\textwidth]{figs/gpt3_fig3_2.png}

Notice how one-shot performs worse than zero-shot. This is because the demonstration for fill-in-the-blank is a bit confusing and it takes a few demonstrations for the model to see the pattern. GPT3 did \textit{not} obtain SOTA on \underline{HellaSwag} and \underline{StoryCloze 2016} (picking best ending to story/instruction).

\bluesec{Closed Book Question Answering}. ``Closed-book'' QA means the model has to answer a question without conditioning on any auxiliary information. The results below show how increasing model size allows the model to effectively absorb more information required to answer questions. 
\myfig[0.6\textwidth]{figs/gpt3_fig3_3.png}


% ------------ Results: Translation ------------ 
\bluesec{Translation} (3.3). The authors made no effort to select for/against foreign languages in the training data. Still, it ended up being 93\% English and 7\% other languages. Results indicate a tendency for translation \textit{into} English to be stronger than translation \textit{from} English.

% ------------ Results: Synthetic ------------ 
\bluesec{Synthetic and Qualitative} (3.9). Below we see that a sharp increase in performance on arithmetic occurs around the 7B parameter point.
\myfig[0.5\textwidth]{figs/gpt3_fig3_10.png}

% ------------ Limitations------------ 
\bluesec{Limitations} (5). 
\begin{myquote}
    GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs.
\end{myquote}
\begin{compactitem}
    \item Difficulty with ``common sense physics.''
    \item In-context learning does little better than chance whenn evaluated one-shot or even few-shot on some ``comparison'' tasks, such as determining if two words are used the same way in a sentence (WIC), or if one sentence implies another (ANLI).
    \item Bidirectionality may improve downstream results.
    \item Limits of the pretraining objective itself.
    \item No grounding in other modalities like video/real-world interactions.
    \item Poor sample-efficiency compared to humans.
    \item Expensive/inconvenient to perform inference on.
\end{compactitem}

\begin{myquote}
    \textellipsis useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions.
\end{myquote}


\bluesec{Further Reading}. Below are a list of references from the paper that I'd like to do a deeper dive on (and why).
\begin{compactitem}
    \item Introduction
    \begin{compactitem}
        \item Larger models $\ne$ better OOD generalization: [HLW+20].
        \item Benchmark performance may exaggerate actual performance on underlying task [GSL+18, NK19].
        \item In-context learning [RWC+19].
        \item Log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20].
    \end{compactitem}

    \item Approach
    \begin{compactitem}
        \item Sparse transformer [CGRS19].
        \item Larger models can typically use a larger batch size, but require a smaller learning rate. [KMH+20, MKAT18]    
        \item Beam search for free-form completion [RSR+19].
    \end{compactitem}

    \item Results
    \begin{compactitem}
        \item LMs can answer questions about broad factual knowledge without conditioning on auxiliary information (like IR system outputs) [RRS20].
    \end{compactitem}

    \item Limitations
    \begin{compactitem}
        \item Improved fine-tuning performance of BERT vs GPT [RSR+19].
        \item Learning the objective function from humans [ZSW+19a].
    \end{compactitem}
\end{compactitem}







% ======================================================================
\lecture{Language Modeling}{Generating Long Sequences with Sparse Transformers}{June 06, 2020}
% ======================================================================

\citepaper{R. Child, S. Gray, A. Radford, and I. Sutskever}{Generating Long Sequences with Sparse Transformers}{OpenAI}{Apr 2019}

\bluesec{Self-Attention Review}. Recall the defining equations for multihead self-attention in transformers, where $Q \eq K \eq V$ are in $\R^{T \times d_{model}}$:
\begin{align}
	\text{MultiHead}(Q, K, V)
	&= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
	\text{where head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\
	\text{Attention}(Q, K, V) 
	&= \text{softmax}\left( 
	\frac{QK^T}{\sqrt{d_k}}
	\right) V
\end{align}
In practice, this is implemented in a heavily vectorized way, with a single 1x1 convolution on the input, instead of $H$ fully connected layers on each of the three $Q, K, V$. Roughly speaking, the actual implementation does the following sequence of operations:
\begin{compactenum}
	\item Using a single weight matrix (I'll omit mention of biases) $\matr W \in \R^{M \times 3M }$ (where $M \equiv d_{model}$), we project the inputs $\vec x$ from $\R^{T \times M}$ to $\R{T \times 3M}$. Note that the result is NOT $Q$, $K$, and $V$. Conceptually, $Q$, $K$, and $V$ are each just identical copies of $x$, so you could say we ``skipped'' a step. Rather, we have the \textit{projected versions} of $Q$, $K$, and $V$, which contains the slices (heads) we'll pass to the attention functions (next step). 
	
	\item Split the last dimension of the \textit{projected versions }of $Q$, $K$, and $V$ (which is $M$) into $[H, d_{qkv}]$. In practice, the practitioner defines the number of heads $H$ they want, and $d_{qkv} := M / H$.  
	
	\item Feed each of the respective $H$ slices into their own Attention function. We can see now that, conceptually, this \textit{is} the same as if we'd taken the original [not projected] $Q$, $K$, and $V$ and projected them down to $d_{qkv}$ for each of the $H$ heads and fed them through, but that's clearly less efficient than the single matrix multiply version seen in implementations.
\end{compactenum} 
\textbf{Interpreting Visualizations}. The attention maps shown in visualizations are showing the softmax outputs for specific attention heads. These are matrices $\ival[h]{A}$ with shape $(T_q, T_{kv})$, and normalized as $1 = \sum_{i=1}^{T_{kv}} A^{(h)}_{q, i}$. 

\bluesec{Factorized Self-Attention} (4.1). Let $S = \{S_1, \ldots, S_T\}$ denote a \green{connectivity map} of $T$ output vectors, where $S_i$ is itself a set of \textit{input} indices to which the $i$th \textit{output} vector attends. For example, traditional decoder LMs define $S_i = \{j : j \le i\}$. Instead of using a single connectivity map for all $H$ attention heads, \green{factorized self-attention} has $P$ heads with different connectivity maps, each a subset of the traditional one:
\graybox{
	A^{(p)}_i \subset \{ j : j \le i  \} \quad \text{with} \quad |A_i^{(p)}| \propto \sqrt[p]{T}
}
and lets $S_i = A_i^{(p)}$ (\red{so $S_i$ is now a set of sets??})


\bluesec{Two-Dimensional Factorized Attention} (4.3). Here, $P \eq 2$.  Let $\ell$ denote the \green{stride}, where typically $\ell \approx \sqrt{T}$. 
\graybox{
	A_i^{(1)}
		&= \{ t, t {+}1, \ldots, i \} \quad \text{for} \quad t = \max \lr{ 0,  i  - \ell } \\
	A_i^{(2)} 
		&= \{ j : ( i - j) \mod \ell = 0 \}
	}


\bluesec{Sparse Transformer} (5). \red{TODO}



% ======================================================================
\lecture{Language Modeling}{The Curious Case of Neural Text Degeneration}{September 15, 2020}
% ======================================================================

\citepaper{Holtzman et al.}{The Curious Case of Neural Text Degeneration}{Allen Institute of Artificial Intelligence}{Feb 2020}

\bluesec{Nucleus Sampling} (3.1). Given a distribution $P(x \mid x_{1:i-1})$, define its top-$p$ vocabulary $V^{(p)} \subset V$ as the \underline{smallest set} such that 
\graybox{
	\sum_{x \in V^{(p)}} P(x \mid x_{1: i-1}) \geq p 
}
Nucleus sampling renormalizes over this subset and samples from the resulting distribution. 
\begin{myquote}
	For high values of p, this is a small subset of vocabulary that takes up vast majority of the probability mass -- the \green{nucleus}.
\end{myquote}

\bluesec{Further Reading}. 
\begin{compactitem}
	\item \textit{Finding the optimum argmax sequence from recurrent neural language models or Transformers is not tractable}. Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks as weighted language recognizers. 
	
	\item \textit{Maximization-based decoding does not lead to high quality text.} (1) Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, (2) David Golub, and Yejin Choi. Learning to write with cooperative discriminators.
\end{compactitem}















% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Embeddings and Transfer Learning}\label{Embeddings and Transfer Learning}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------


% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Simple Baseline for Sentence Embeddings}{June 12, 2017}
% ============================================================================================


\myspace
\p \blue{Overview}. It turns out that simply taking a weighted average of word vectors and doing some PCA/SVD is a competitive way of getting unsupervised word embeddings. Apparently it \textit{beats} supervised learning with LSTMs (?!). The authors claim the theoretical explanation for this method lies in a latent variable generative model for sentences (of course).\marginnote{Discussion based on paper by Arora et al., (2017).}[-2em]

\myspace
\p \blue{Algorithm}.
\begin{compactenum}
	\item Compute the weighted average of the word vectors in the sentence:\marginnote{The authors call their weighted average the \green{Smooth Inverse Frequency (SIF)}.}[2em]
	\begin{align}
	\frac{1}{N} \sum_{i}^{N} \frac{a}{a + p(\vec[i]{w})} \vec[i]{w}
	\end{align}
	where $\vec[i]{w}$ is the word vector for the $i$th word in the sentence, $a$ is a parameter, and $p(\vec[i]{w})$ is the (estimated) word frequency [over the entire corpus]. 
	
	\item Remove the projections of the average vectors on their first principal component (``common component removal'') (y tho?).
\end{compactenum}

\myfig[0.8\textwidth]{AroraEmbeddingAlg.png}

\myspace
\p \blue{Theory}. Latent variable generative model. The model treats corpus generation as a dynamic process, where the $t$-th word is produced at time step $t$, driven by the random walk of a \green{discourse vector} $c_t \in \Re^d$ ($d$ is size of the embedding dimension). The discourse vector is \textit{not} pointing to a specific word; rather, it describes what is being talked about. We can tell how related (correlation) the discourse is to any word $w$ and corresponding vector $v_w$ by taking the inner product $c_t \cdot v_w$. Similarly, we model the probability of observing word $w$ at time $t$, $w_t$, as:
\graybox{ \Prob{w_t \mid c_t} \propto e^{c_t \cdot v_w} \label{naive-embed} }
\begin{compactitem}
	\item \textbf{The Random Walk}. If we assume that $c_t$ doesn't change much over the words in a \underline{single sentence}, we can assume it stays at some $c_s$. The authors claim that in their previous paper they showed that the MAP\footnote{Review of MAP: $$ \theta_{MAP} = \argmax_\theta \sum_i \log\left(p_X(x \mid \theta) p(\theta) \right)$$ } estimate of $c_s$ is -- up to multiplication by a scalar -- the average of the embeddings of the words in the sentence.
	
	\item \textbf{Improvements/Modifications to~\ref{naive-embed}}. 
	\begin{compactenum}
		\item Additive term $\alpha p(w)$ where $\alpha$ is a scalar. Allows words to occur even if $c_t \cdot v_w$ is very small.
		\item Common discourse vector $c_0 \in \Re^d$. Correction term for the most frequent discourse that is often related to syntax.
	\end{compactenum}
	
	\item \textbf{Model}. Given the discourse vector $c_s$ for a sentence $s$, the probability that $w$ is in the sentence (at all (?)):
	\graybox{
		\Prob{w \mid c_s} &= \alpha p(w) + (1 - \alpha) \frac{ e^{\tilde c_s \cdot v_w} }{ Z_{\tilde c_s} } \\
		\tilde c_s &= \beta c_0 + (1 - \beta) c_s
	}
	with $c_0 \perp c_s$ and $Z_{\tilde c_s}$ is a normalization constant, taken over all $w \in V$. 
	
\end{compactitem}



% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Deep Sentence Embedding Using LSTMs}{July 10, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Palangi et al., ``Deep Sentence Embeddings Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval,'' (2016).}

\p \blue{Abstract}. Sentence embeddings using LSTM cells, which automatically attenuate unimportant words and detect salient keywords. Main emphasis on applications for document retrieval (matching a query to a document\footnote{Note that this similar to topic extraction.}).

\myspace
\p \blue{Introduction}. Sentence embeddings are learned using a loss function defined on \textit{sentence pairs}. For example, the well-known Paragraph Vector\footnote{Q. V. Le and T. Mikolov, “Distributed representations of sentences and documents.”} is learned in an unsupervised manner as a distributed representation of sentences and documents, which are then used for sentiment analysis. \\

\p The authors appear to use a dataset of their own containing examples of (search-query, clicked-title) for a search engine. Their training objective is to maximize the similarity between the two vectors mapped by the LSTM-RNN from the query and the clicked document, respectively. One very interesting claim to pay close attention to:
\vspace{-1em}
\begin{quote}
	{\small \textit{We further show that different cells in the learned model indeed correspond to \purple{different topics}, and the keywords associated with a similar topic activate the same cell unit in the model.} }
\end{quote}

\myspace
\p \blue{Related Work}. (Identified by reference number)
\begin{compactitem}
	\item ~ [2] Good for sentiment, but doesn't capture fine-grained sentence structure.
	
	\item ~ [6] Unsupervised embedding method trained on the BookCorpus [7]. Not good for document retrieval task.
	
	\item ~ [9] Semi-supervised Recursive Autoencoder (RAE) for sentiment prediction.
	
	\item ~ [3] DSSM (uses bag-of-words) and [10] CLSM (uses bag of n-grams) models for IR and also sentence embeddings.
	
	\item ~ [12] Dynamic CNN for sentence embeddings. Good for sentiment prediction and question type classification. In [13], a CNN is proposed for \green{sentence matching}\footnote{Might want to look into this.}
\end{compactitem}

\p \blue{Basic RNN}. The information flow (sequence of operations) is enumerated below.
\begin{compactenum}
	\item Encode $t$th word [of the given sentence] in one-hot vector $\vec{x}(t)$. 
	
	\item Convert $\vec{x}(t)$ to a \underline{letter} tri-gram vector $\vec{l}(t)$ using fixed hashing operator\footnote{Details aside, the hashing operator serves to lower the dimensionality of the inputs a bit. In particular we use it to convert one-hot word vectors into their letter tri-grams. For example, the word ``good'' gets surrounded by hashes, `\#good\#`, and then hashed from the one-hot vector to vectorized tri-grams, ``\#go'', ``goo'', ``ood'', ``od\#''.} $\matr{H}$:
	\begin{align}
	\vec{l}(t) &= \matr{H} \vec{x}(t)
	\end{align}
	
	\item Compute the hidden state $\vec{h}(t)$, which is the sentence embedding for $t = T$, the length of the sentence.
	\begin{align}
	\vec{h}(t) &= \tanh \left(\matr{U} \vec{l}(t) + \matr{W} \vec{h}(t - 1) + \vec{b} \right)
	\end{align}
	where $\matr{U}$ and $\matr{W}$ are the usual parameter matrices for the input/recurrent paths, respectively.
\end{compactenum}

\myspace
\p \blue{LSTM}. With peephole connections that expose the internal cell state $s$ to the sigmoid computations. I'll rewrite the standard LSTM equations from my textbook notes, but with the modifications for peephole connections:
\begin{align}
f_i^{(t)} 
&= \sigma\left(b_i^f + \sum_j U_{i,j}^f x_j^{(t)} + \sum_j W_{i,j}^f h_j^{(t-1)}
+  \sum_j P_{i,j}^f s_j^{(t - 1)} \right) \\
s_i^{(t)} 
&= f_i^{(t)} \odot s_i^{(t-1)} + g_i^{(t)} \odot \sigma\left(b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} h_j^{(t-1)} \right) \\
g_i^{(t)}
&= \sigma\left(b_i^g + \sum_j U_{i,j}^g x_j^{(t)} + \sum_j W_{i,j}^g h_j^{(t-1)} 
+ \sum_j P_{i,j}^g s_j^{(t - 1)} \right) \\
q_i^{(t)}
&= \sigma\left(b_i^o + \sum_j U_{i,j}^o x_j^{(t)} + \sum_j W_{i,j}^o h_j^{(t-1)} 
+ \sum_j P_{i,j}^o s_j^{(t)}\right) 
\end{align}
The final hidden state can then be computed via
\begin{align}
h_i^{(t)} &= \tanh(s_i^{(t)}) \odot q_i^{(t)}
\end{align}


\myspace
\p \blue{Learning method}. We want to maximize the likelihood of the clicked document given query, which can be formulated as the following optimization problem:
\graybox{
	L(\matr{\Lambda}) 
	&= \min_{\matr{\Lambda}} \left\lbrace
	-\log \prod_{r = 1}^{N} \Prob{D_r^+ \mid Q_r} \right\rbrace
	=  \min_{\matr{\Lambda}} \sum_{r = 1}^{N} l_r(\matr{\Lambda}) \\
	l_r(\matr{\Lambda}) 
	&= \log\left(  1 + \jnsum e^{-\gamma \cdot \Delta_{r,j}} \right)
}
where 
\begin{compactitem}
	\item $N$ is the number of (query, clicked-doc) pairs in the corpus, while $n$ is the number of negative samples used during training.
	\item $D_r^+$ is the clicked document for $r$th query.
	\item $\Delta_{r,j} = R(Q_r, D_r^+) - R(Q_r, D_{r,j}^-)$ (R is just cosine similarity)\footnote{
		Note that $\Delta_{r,j} \in [-2, 2]$. We use $\gamma$ as a scaling factor so as to expand this range.
	}.
	\item $\matr{\Lambda}$ is all the parameter matrices (and biases) in the LSTM. 
\end{compactitem}
The authors then describe standard BPTT updates with momentum, which need not be detailed here. See the ``Algorithm 1'' figure in the paper for extremely detailed pseudo-code of the training procedure.





% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Supervised Universal Sentence Representations (InferSent)}{July 12, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Conneau et al., ``Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,'' Facebook AI Research (2017).}

\p \blue{Overview}. Authors claim universal sentence representations trained using the supervised data of the Stanford Natural Language Inference (SNLI) dataset can consistently outperform unsupervised methods like SkipThought on a wide range of transfer tasks. They emphasize that training on NLI tasks in particular results in embeddings that perform well in transfer tasks. Their best encoder is a Bi-LSTM architecture with max pooling, which they claim is SOTA when trained on the SNLI data.

\myspace
\p \blue{The Natural Language Inference Task}. Also known as \textit{Recognizing Textual Entailment} (RTE). The SNLI data consists of sentence pairs labeled as one of entailment, contradiction, or neutral. Below is a typical architecture for training on SNLI.

\myfig[0.4\textwidth]{SNLI_diagram.png}

Note that the same sentence encoder is used for both $u$ and $v$. To obtain a sentence vector from a BiLSTM encoder, they experiment with (1) the average $h_t$ over all $t$ (\green{mean pooling}), and (2) selecting the max value over each dimension of the hidden units [over all timesteps] (\green{max pooling})\footnote{Since the authors have already mentioned that BiLSTM did the best, I won't go over the other architectures they tried: self-attentive networks, hierarchical convnet, vanilla LSTM/GRU.}.



% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Dist. Rep. of Sentences from Unlabeled Data (FastSent)}{July 13, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Hill et al., ``Learning Distributed Representations of Sentences from Unlabelled Data,'' (2016).}

\p \blue{Overview}. A systematic comparison of models that learn distributed representations of phrases/sentences from unlabeled data. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics.\\

\p Authors propose two new phrase/sentence representation learning objectives:
\begin{compactenum}
	\item \green{Sequential Denoising Autoencoders} (SDAEs)
	
	\item \green{FastSent}: a sentence-level log-linear BOW model.
\end{compactenum}

\myspace
\p \blue{Distributed Sentence Representations}. Existing models trained on text:
\begin{compactitem}
	\item \green{SkipThought Vectors} (Kiros et al., 2015). Predict target sentences $S_{i \pm 1}$ given source sentence $S_i$. Sequence-to-sequence model.
	
	\item \green{ParagraphVector} (Le and Mikolov, 2014). Defines 2 log-linear models:
	\begin{compactenum}
		\item \textbf{DBOW}: learns a vector $\vec{s}$ for every sentence $S$ in the training corpus which, together with word embeddings $v_w$, define a softmax distribution to predict words $w \in S$ given $S$. 
		
		\item \textbf{DM}: select k-grams of consecutive words $\{w_i ~ \cdots ~ w_{i + k} \in S \}$ and the sentence vector $\vec{s}$ to predict $w_{i + k + 1}$.\marginnote{The authors use \purple{gensim} to implement ParagraphVector.}[-3em]
	\end{compactenum}
	
	\item \green{Bottom-Up Methods}. Train \textbf{CBOW} and \textbf{Skip-Gram} word embeddings on the Books corpus.
\end{compactitem}
\vspace{1em}

\p Models trained on \textit{structured} (and freely-available) resources:
\begin{compactitem}
	\item \green{DictRep} (Hill et al., 2015a). Map dictionary definitions to pre-trained word embeddings, using either BOW or RNN-LSTM encoding.
	
	\item \green{NMT}. Consider sentence representations learned by sequence-to-sequence NMT models.
\end{compactitem}

\myspace
\p \blue{Novel Text-Based Methods}. 
\begin{compactitem}
	\item \green{Sequential (Denoising) Autoencoders}. To avoid needing coherent inter-sentence narrative, try this representation-learning objective based on DAEs. For a given sentence $S$ and \textbf{noise function} $N(S \mid p_o, p_x)$ (where $p_0, p_x \in [0, 1]$),the approach is as follows:
	\begin{compactenum}
		\item For each $w \in S$, $N$ deletes $w$ with probability $p_o$.
		\item For each non-overlapping bigram $w_iw_{i + 1} \in S$, $N$ swaps $w_i$ and $w_{i + 1}$ with probability $p_x$.\marginnote{Authors recommend $p_o = p_x = 0.1$}
	\end{compactenum}
	\begin{quote}
		{\footnotesize \textit{
				We then train the same LSTM-based encoder-decoder architecture as NMT, but with the denoising objective to predict (as target) the original source sentence $S$ given a corrupted version $N(S \mid p_o, p_x)$ (as source).
		}}
	\end{quote}
	
	\item \green{FastSent}. Designed to be a more efficient/quicker to train version of SkipThought.
\end{compactitem}







% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Attention Is All You Need}{September 04, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Vaswani et al., ``Attention Is All You Need,'' (2017)}

\p \blue{Overview}. Authors refer to sequence \textit{transduction} models a lot -- just a fancy way of referring to models that transform input sequences into output sequences. Authors propose new architecture, the \green{Transformer}, based solely on attention mechanisms (no recurrence!). 

\myspace
\p \blue{Model Architecture}. 
\begin{compactitem}
	\item \textbf{Encoder}. $N \eq 6$ identical layers, each with 2 sublayers: (1) a \green{multi-head} \textbf{self-attention} mechanism and (2) a position-wise FC feed-forward network. They apply a residual connection and layer norm such that each sublayer, instead of outputting Sublayer(x), instead outputs LayerNorm(x + Sublayer(x)).
	
	\item \textbf{Decoder}. $N \eq 6$ with 3 sublayers each. In addition to the two sublayers described for the encoder, the decoder has a third sublayer, which performs \green{multi-head} attention over the output of the encoder stack. Same residual connections and layer norm. 
\end{compactitem}\marginnote{Figure shows encoder-decoder template layers. The actual model instantiates chain of 6 encoder layers and 6 decoders layers. The decoder's self-attention masks embeddings at future timesteps to zero.}[4em]
\myfig[0.4\textwidth]{figs/transformer.png}

\myspace
\p \blue{Attention}. An attention function can be described as a mapping:
\begin{align}
\text{Attn(query, }\{(k_1, v_1), \ldots, \}\text{)} 
&\Rightarrow \sum_i fn(\text{query}, k_i) v_i
\end{align}
where the query, keys, values, and output are all vectors. 
\begin{compactitem}
	\item \green{Scaled Dot-Product Attention}.
	\begin{compactenum}
		\item \textbf{Inputs}: queries $q$, keys $k$ of dimension $d_k$, values $v$ of dimension $d_v$\marginnote{Appears that $d_q \equiv d_k$.}
		\item \textbf{Dot Products}: Compute $\forall k:~ (q \cdot k) / \sqrt{d_k}$. 
		\item \textbf{Softmax}: on each dot product above. This gives the weights on the values shown earlier.
	\end{compactenum}
	In practice, this is done simultaneously for all queries in a set via the following matrix equation:
	\graybox{
		\text{Attention}(Q, K, V) 
		&= \text{softmax}\left( 
		\frac{QK^T}{\sqrt{d_k}}
		\right) V
	}
	Note that this is identical to the standard dot-product attention mechanism, except for the \textit{scaling} factor (hence the name) of $1/\sqrt{d_k}$. The scaling factor is motivated by the fact that additive attention outperforms dot-product attention for large $d_k$ and the authors stipulate this is due to the softmax having small gradients in this case, due to the large dot products\footnote{
		Assume that $\vec q$ and $\vec k$ are vectors in $\R^d$ whose components are independent RVs with $\E{q_i} = \E{k_j} = 0$ $(\forall i, j)$, and $\Var{q_i} = \Var{k_j} = 1$ $(\forall i, j)$. Then
		\begin{align}
		\E{\vec q \dotp \vec k}
		&= \E{\sum_i^d q_i k_i} 
		= \sum_i^d \E{q_i k_i} 
		= \sum_i^d \E{q_i} \E{k_i} 
		= 0 \\
		\Var{\vec q \dotp \vec k}
		&= \Var{\sum_i^d q_i k_i} 
		= \sum_i^d \Var{q_i k_i} 
		= \sum_i^d \E{q_i^2 k_i^2} - \cancel{ \E{q_i k_i} } \\
		&= \sum_i^d \E{q_i^2} \E{k_i^2} 
		= \sum_i^d \Var{q_i} \Var{k_i} 
		= d
		\end{align}
		
		See \href{https://stats.stackexchange.com/a/318258}{this S.O answer} and/or \href{http://www.odelama.com/data-analysis/Commonly-Used-Math-Formulas}{these useful formulas} for more details.
	}. \\
	
	First, let's explicitly show which indices are being normalized over, since it can get confusing when presented with the highly vectorized version above. For a given input sequence of length $T$, and for the self-attention version where $K \eq Q \eq V \in \R^{T \times d_{k}}$, the output attention vector for timestep $t$ is explicitly (ignoring the $\sqrt{d_k}$ for simplicity)
	\begin{align}
	\text{Attention}(Q, K, V)_{\mgreen{t}}
	&= \left[ \text{softmax}\left( Q K^T \right) V \right]_{\mgreen{t}} \\
	&= \sum_{t'}^{T} \dfrac{ e^{\mgreen{Q_t} \cdot K_{t'}}  }{  \sum_{t''}^{T} e^{\mgreen{Q_t} \cdot K_{t''}}    } V_{t'}
	\end{align} 
	
	\Needspace{10\baselineskip}
	Next, the gradient of the $d$th softmax output w.r.t its inputs is 
	\begin{align}
	\pderiv{ \text{Softmax}_d(\vec x) }{x_j} 
	&= \text{Softmax}_d(\vec x) \left(    \delta_{dj} - \text{Softmax}_d(\vec x)   \right)
	\end{align}
	
	
	
	\item \green{Multi-Head Attention}. Basically just doing some number $h$ of parallel attention computations. Before each of these, the queries, keys, and values are linearly projected with different,
	learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions respectively (and then fed to their respective attention function). The $h$ outputs are then concatenated and once again projected, resulting in the final values.\marginnote{
		$$ W_i^Q \in \R^{d_{model} \times d_k} $$
		$$ W_i^K \in \R^{d_{model} \times d_k} $$
		$$ W_i^V \in \R^{d_{model} \times d_v} $$
		$$ W^O \in \R^{h d_{v} \times d_{model}} $$
	}[-1em]
	\graybox{
		\text{MultiHead}(Q, K, V)
		&= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
		\text{where head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
	}
	The authors employ $h = 8$, $d_k = d_v = d_{model} / h = 64$. 
\end{compactitem}

The Transformer uses multi-headed attention in 3 ways:
\begin{compactenum}
	\item \textbf{Encoder-decoder attention}: the normal kind. Queries are previous decoder layer, and memory keys and values come from output of the [final layer of] encoder. 
	
	\item \textbf{Encoder self-attention}: all of the keys, values, and queries come from the previous \textit{layer} in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 
	
	\item \textbf{Decoder self-attention}: Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position (timestep). The masking is done on the inputs to the softmax, setting all inputs beyond the current timestep to $-\infty$. 
\end{compactenum}

\myspace
\p \blue{Other Components}. 
\begin{compactitem}
	\item \textbf{Position-wise Feed-Forward Networks (FFN)}: each layer of the encoder and decoder contains a FC FFN, applied to each position separately and identically:\marginnote{The FFN is linear $\rightarrow$ ReLU $\rightarrow$ linear.}[2em]
	\begin{align}
	\text{FFN}(x) = \max\left(0, x W_1 + b_1 \right) W_2 + b_2
	\end{align}
	
	\item \textbf{Embeddings and Softmax}: use learned embeddings to convert input/output tokens to vectors of dimension $d_{model}$, \textit{and} for the pre-softmax layer at the output of the decoder\footnote{In other words, they use the \underline{same} weight matrix for all three of (1) encoder input embedding, (2) decoder input embedding, and (3) (opposite direction) from decoder output to pre-softmax.}\marginnote{For inputs to encoder/decoder, the embedding weights are multiplied by $\sqrt{d_{model}}$}. 
	
	\item \textbf{Positional Encoding}: how the authors deal with the lack of recurrence (to make use of the sequence order). They \underline{add} a sinusoid function of the position (timestep) $pos$ and vector index $i$ to the input embeddings for the encoder and decoder\footnote{Note that the positional encodings must necessarily be of dimension $d_{model}$ to be summed with the input embeddings.}:
	\graybox{
		\text{PE}(pos, 2i) &= \sin\left( \frac{pos}{10000^{2i/d_{model}}} \right) \\
		\text{PE}(pos, 2i+1) &= \cos\left( \frac{pos}{10000^{2i/d_{model}}} \right)
	}
	The authors justify this choice:
	\begin{quote}
		{\small \textit{We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $\text{PE}_{pos+k}$ can be represented as a linear function of $\text{PE}_{pos}$.}}
	\end{quote}
\end{compactitem}


\bluesec{Summary of Add-ons}. Below is a list of all the little bells and whistles they add to the main components of the model that are easy to miss since they mention them throughout the paper in a rather unorganized fashion.
\begin{compactitem}
	\item Shared weights for encoder inputs, decoder inputs, and final softmax projection outputs. 
	\item Multiply the encoder and decoder input embedding [shared] weights by $\sqrt{d_{model}}$. \red{TODO}: why? Also this must be highly correlated with their decision regarding weight initialization (mean/stddev/technique). Add whatever they use here if they mention it.
	
	\item Adam optimizer with $\beta_1 \eq 0.9$, $\beta_2 \eq 0.98$, $\epsilon \eq 10^{-9}$.
	
	\item Learning rate schedule $\text{LR}(s) = d^{-0.5}_{model} \cdot \min \left(  s^{-0.5}, s \cdot w^{-1.5}  \right)  $ for global step $s$ and warmup steps $w \eq 4000$. 
	
	\item Dropout on sublayer outputs pre-layernorm-and-residual. Specifically, they \textit{actually} return LayerNorm(x + Dropout(Sublayer(x))). Use $P_{drop} = 0.1$. 
	
	\item Dropout the summed embeddings+positional-encodings for both encoder and decoder stacks.
	
	\item Dropout on softmax outputs. So do Dropout(Softmax(QK))V. 
	
	\item Label smoothing with $\epsilon_{ls} = 0.1$. 
\end{compactitem}










% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Poincar\'{e} Embeddings for Learning Hierarchical Representations}{Nov 16, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize M. Nickel and D. Kiela, ``Poincar\'{e} Embeddings for Learning Hierarchical Representations'' (2017)}


\p \blue{Introduction}. Dimensionality of embeddings can become prohibitively large when needed for complex data. Authors focus on mitigating this problem for large datasets whose objects can be organized according to a latent hierarchy\footnote{This begs the question: how useful would a Poincar\'{e} embedding be for situations where this assumption isn't valid?}. They propose to compute embeddings in a particular model of hyperbolic space, the \red{Poincar\'{e} ball model}, claiming it is well-suited for gradient-based optimization (they make use of \red{Riemannian optimization}).

\myspace
\p \blue{Prerequisite Math}. Recall that a hyperbola is a set of points, such that for any point $P$ of the set, the absolute difference of the distances $|PF_1|$, $|PF_2|$ to two fixed points $F_1$, $F_2$ (the foci), is constant, usually denoted by $2a$, $a > 0$. We can define a hyperbola by this set of points or by its canonical form, which are both given, respectively, as:
\begin{align}
&H = \{ P | ~ | |PF_2| - |PF_1| | = 2a \} \\
&\frac{x^2}{a^2} - \frac{y^2}{b^2} = 1
\end{align}
where $b^2 := c^2 - a^2$, $(\pm a, 0)$ are the two vertices, and $(\pm c, 0)$ are the two foci. Cannon et al. define $n$-dimensional hyperbolic space by the formula
\begin{align}
H^n = \{x   \in \R^{n+1} : ~ x * x = -1   \}
\end{align}
where $*$ denotes the non-euclidean inner product (subtracts last term; same as Minkowski sapce-time). Notice that this is the defining equation for a  \href{http://mathworld.wolfram.com/Two-SheetedHyperboloid.html}{hyperboloid of two sheets}, and Cannon et al. says ``usually we deal only with one of the two sheets.'' Hyperbolic spaces are well-suited to model hierarchical data, since both circle length and disc area grow \textit{exponentially} with $r$.

\myspace
\p \blue{Poincar\'{e} Embeddings}. Let $\mathcal{B}^d = \{\vec x \in \R^d \mid ||\vec x|| < 1\}$ be the open d-dimensional unit ball. The \green{Poincar\'{e} ball} model of hyperbolic space corresponds then to the Riemannian manifold\footnote{All five analytic models of hyperbolic geometry in Cannon et al. are differentiable manifolds with a Riemannian metric. A \green{Riemannian metric} $\mathrm{d}s^2$ on Euclidean space $\R^n$ is a function that assigns at each point $p \in \R^n$ a positive definite symmetric inner product on the tangent space at $p$, this inner product varying differentiably  with the point $p$. If $x_1, \ldots, x_n$ are the standard coordinates in $\R^n$, then $\mathrm{d}s^2$ has the form $\sum_{i,j} g_{ij} \mathrm{d}x_i \mathrm{d}x_j$, and the matrix ($g_{ij}$) depends differentiably on $x$ and is positive definite and symmetric. 
	
} $(\mathcal{B}^d, g_{\vec x})$, where
\begin{align}
g_{\vec x} &= \left( \frac{2}{1 - ||\vec x||^2} \right)^2 g^E
\end{align}
is the \green{Riemannian metric tensor}, and $g^E$ denotes the Euclidean metric tensor. The distance between two points $\vec u, \vec b \in \mathcal{B}^d$ is given as
\graybox{
	d(\vec u, \vec v) &= \text{arccosh}\left( 1 + 
	2 \frac{   ||\vec u - \vec v ||^2 }{ (1 - ||\vec u ||^2) (1 - ||\vec v||^2)}   \right)
}
The boundary of the ball corresponds to the sphere $S^{d-1}$ and is denoted by $\partial\mathcal{B}$. Geodesics in $\mathcal{B}^d$ are then circles that are orthogonal to $\partial\mathcal{B}$. To compute Poincar\'{e} embeddings for a set of symbols $\mathcal{S} = \{x_i\}_{i=1}^{n}$, we want to find embeddings $\Theta = \{\vec[i]{\theta}\}_{i=1}^{n}$, where $\vec[i]{\theta} \in \mathcal{B}^d$. Given some loss function $\mathcal{L}$ that encourages semantically similar objects to be close as defined by the Poincar\'{e} distance, our goal is to solve the optimization problem
\graybox{
	\Theta' \leftarrow \argmin_{\Theta} \mathcal{L}(\Theta) \qquad s.t.~ \forall \vec[i]{\theta} \in \Theta: ||\vec[i]{\theta}|| < 1	
}

\myspace
\p \blue{Optimization}. Let $\mathcal{T}_{\theta}\mathcal{B}$ denote the \red{tangent space} of a point $\vec{\theta} \in \mathcal{B}^d$. Let $\nabla_R \in \mathcal{T}_{\vec{\theta}}\mathcal{B}$ denote the \red{Riemannian gradient} of $\mathcal{L}(\vec{\theta})$, and $\nabla_E$ the Euclidean gradient of $\mathcal{L}(\vec{\theta})$. Using \red{RSGD}, parameter updates take the form
\graybox{
	\vec[t + 1]{\theta} &= \mathfrak{R}_{\vec[t]{\theta}} \left(
	- \eta_t \nabla_R \mathcal{L}(\vec[t]{\theta})	\right)
}
where $\mathfrak{R}_{\vec[t]{\theta}}$ denotes the \red{retraction} onto $\mathcal{B}$ at $\vec{\theta}$ and $\eta_t$ denotes the learning rate at time $t$. 






% ============================================================================================
\lecture{Embeddings and Transfer Learning}{Enriching Word Vectors with Subword Information (FastText)}{Nov 17, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ``Enriching Word Vectors with Subword Information'' (2017)}

\myspace
\p \blue{Overview}. Based on the skipgram model, but where each word is represented as a bag of character $n$-grams. A vector representation is associated \textit{each} character $n$-gram; words being represented as the \textit{sum}\footnote{It would be interesting to explore other aggregation operations than just summation.} of these representations.

\myspace
\p \blue{Skipgram with Negative Sampling}. Since this is based on skipgram, recall the objective of skipgram which is to maximize:
\begin{align}
\sum_{t = 1}^T \sum_{c \in \mathcal{C}_t} \log \Prob{w_c \mid w_t}
\end{align}
for a sequence of words $w_1, \ldots w_T$. One way of parameterizing $\Prob{w_c \mid w_t}$ is by computing a softmax over a scoring function $s: (w_t, w_c) \mapsto \R$,
\begin{align}
\Prob{w_c \mid w_t} &= \frac{
	e^{s(w_t, w_c)}  }{
	\sum_{j = 1}^{W} e^{s(w_t, j)}
}
\end{align}
However, this implies that, given $w_t$, we only predict one context word $w_c$. Instead, we can frame the problem as a set of independent binary classification tasks, and independently predict the presence/absence of context words. Let $\ell: x \mapsto \log(1 + e^{-x})$ denote the standard logistic negative log-likelihood. Our objective is:
\begin{align}
\sum_{t = 1}^T \left[
\sum_{c \in \mathcal{C}_t} \ell(s(w_t, w_c)) + \sum_{n \in \mathcal{N}_{t,c}} \ell(-s(w_t, n))
\right]
\end{align}
where $\mathcal{N}_{t,c}$ is a set of negative examples sampled from the vocabulary. A common scoring function involves associating a distinct input vector $u_w$ and output vector $v_w$ for each word $w$. Then the score is computed as $s(w_t, w_c) = \vec[w_t]{u}^T \vec[w_c]{v}$. 

\myspace
\p \blue{FastText}. Main contribution is a different scoring function $s$ that utilizes subword information. Each word $w$ is represented as a bag of character $n$-grams. Special symbols $<$ and $>$ delimit word boundaries, and the authors also insert the special sequence containing the full word (with the delimiters) in its bag of $n$-grams. The word \textit{where} is thus represented by first building its bag of $n$-grams, for the choice of $n=3$:
\begin{align}
\text{where} ~ \longrightarrow ~  \{    
<wh,~~ whe,~~ her,~~ ere,~~ re>,~~ <where>
\}
\end{align}
Such a set of $n$-grams for a word $w$ is denoted $\mathcal{G}_w$. Each $n$-gram $g$ for a word $w$ has its own vector $\vec[g]{z}$, and the final vector representation of $w$ is the sum of these. The scoring function becomes
\graybox{
	s(w, c) &= \sum_{g \in \mathcal{G}_w} \vec[g]{z}^T \vec[c]{v}
}






% ===========================================================================================
\lecture{Embeddings and Transfer Learning}{Deep Contextualized Word Representations}{August 30, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Peters et al., ``Deep Contextualized Word Representations,'' (2018).}

\p \blue{Bidirectional Language Models} (3.1). Given a sequence of $N$ tokens, a forward LM computes the probability of the sequence via
\begin{align}
p(t_1, \ldots, t_N)
&= \prod_{k=1}^{N} p(t_k \mid t_1, \ldots, t_{k-1})
\end{align}
A common approach is learning context-independent token representations $\vec[k]{x}$ and passing these through $L$ layers of forward LSTMs. The top layer LSTM output at step $k$, $\vec[k,L]{\overrightarrow{h}}$, is used to predict $t_{k+1}$ with a softmax layer. The authors' biLM combines a forward and backward LM to jointly maximize 
\begin{align}
\begin{split}
\sum_{k=1}^N \bigg[
&\log p(t_k \mid t_1, \ldots, t_{k-1}; 
\Theta_x, \mpurple{\overrightarrow{\Theta}_{LSTM}}, \Theta_s) \\
+	&\log p(t_k \mid t_{k+1}, \ldots, t_{N}; 
\Theta_x, \mpurple{\overleftarrow{\Theta}_{LSTM}}, \Theta_s) 
\bigg]
\end{split}
\end{align}
and it's important to note the shared parameters $\Theta_x$ (token representation) and $\Theta_s$ (output softmax). 

\myspace
\p \blue{ELMo} (3.2). A task-specific linear combination of the intermediate representations. 
\graybox{
	\bm{ELMo}_k^{task}
	&= \gamma^{task} \sum_{j = 0}^L s_j^{task} \vec[k,j]{h}^{LM}	
}
where $\vec{s}^{task}$ are softmax-normalized weights (so the combination is convex). The authors also mention that, in some cases, it helped to apply \green{layer normalization} to each biLM layer before weighting. 

\myspace
\p \blue{Using biLMs for Supervised NLP} (3.3). Given a pretrained biLM and a supervised architecture, we can learn the ELMo representations (jointly with the given supervised task) as follows. 
\begin{compactenum}
	\item Freeze the weights of the [pretrained] biLM. 
	\item Concatenate the token representations (e.g. GloVe) with the ELMo representation. 
	\item Pass the concatenated representation into the supervised architecture. 
\end{compactenum}
The authors found it beneficial to some dropout to ELMo, and in some cases add L2-regularization on the ELMo weights.

\myspace
\p \blue{Pretrained biLM Architecture} (3.4). In addition to the biLM we introduced earlier, the authors make the following changes/specifications for their pretrained biLMs:
\begin{compactitem}
	\item  \green{residual connections} between LSTM layers\footnote{So the output of some layer, instead of being LSTM(x), becomes (x + LSTM(x))}.
	
	\item Halved all embedding and hidden dimensions from the CNN-BIG-LSTM model in \href{https://arxiv.org/abs/1602.02410}{\textit{Exploring the Limits of Language Modeling}}. 
	
	\item The $\vec[k]{x}$ token representations use 2048 character n-gram convolutional filters followed by two \green{highway layers}. 
\end{compactitem}






% ===========================================================================================
\lecture{Embeddings and Transfer Learning}{BERT}{November 03, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Devlin et al., ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,''  Google AI Language (Oct 2018).}


\p \blue{TL;DR}. Bidirectional Encoder Representations from Transformers. Pretrained by jointly conditioning on left and right context, and can be fine-tuned with one additional non-task-specific output layer. Authors claim the following contributions:
\begin{compactitem}
	\item \textit{Demonstrate importance of bidirectional pre-training for language representations}. Ok, congrats. 
	
	\item \textit{Show that pre-trained representations eliminate needs of task-specific architectures}. We already knew this. Seriously, how is this news?
	
	\item \textit{Advances SOTA for eleven NLP tasks}. 
\end{compactitem}

\myspace 
\p \blue{BERT}. Instead of using the unidirectional transformer \textit{decoder}, they use the bidirectional \textit{encoder} architecture for their pre-trained model\footnote{Is this seriously paper-worthy?? I'm taking notes so I can easily refer back on popular approaches, but I don't see what's so special here.}. 
\myfig[0.7\textwidth]{figs/bert_input.png}

The input representation is shown in the figure above. The input is a sentence pair, as commonly seen in tasks like QA/NLI. 

\myspace
\p \blue{Pre-training Tasks} (3.3). 
\begin{compactenum}
	\item \textbf{Masked LM}: Mask 15\% of all tokens, and try to predict only those masked tokens\footnote{This is the only ``novel'' idea I've seen in the paper. Seems hacky-ish but also reasonable.} Furthermore, at training time, the mask tokens are either fed through as (a) the special \texttt{[MASK]} token 80\% of the time, (b) a random word 10\% of the time, and (c) the original word unchanged 10\% of the time. Now \textit{this} is just hackery. 
	
	\item \textbf{Next Sentence Prediction}: Given two input sentences A and B, train a binary classifier to predict whether sentence B came directly after sentence A. 
\end{compactenum}
They do the pretraining jointly, using a loss function that's the sum of the mean masked LM likelihood and mean next sentence prediction likelihood. 



% ===========================================================================================
\lecture{Embeddings and Transfer Learning}{Wasserstein is all you need}{November 25, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Singh et al., ``Wasserstein is all you need,''  EPFL Switzerland (August 2018).}

\p \blue{TL;DR}. Unsupervised representations of objects/entities via distributional + point estimate. Made possible by \green{optimal transport}. 

\myspace
\p \blue{Optimal Transport} (3). First, notation. Let\textellipsis
\begin{compactitem}
	\item $\Omega$ denote a space of possible outcomes. 
	
	\item $\mu$ denote an \textbf{empirical} probability measure, defined as some convex combination $\mu(\vec x) = \insum a_i \delta(x_i)$, where $x_i \in \Omega$. 
	
	\item $\nu$ denote a similar measure, also a convex combination, $\nu(\vec y) = \sum_j^m b_j \delta(y_j)$. 
	
	\item $M_{ij}$ denote the ground cost of moving from point $x_i$ to $y_j$. 
\end{compactitem}
Intuition break: recognize that $\mu$ and $\nu$ are just a formal description of probability densities via normalized ``counts'' $a_i$ and/or $b_j$. Those weights are basically probability mass The \green{Optimal Transport} distance between $\mu$ and $\nu$ is the following \purple{linear program}:
\graybox{
	\text{OT}(\mu, \nu; \matr M)
	&= \min_{ \matr T \in \R^{n \times m}_+  }   \sum_{ij} T_{ij} M_{ij} 
	\quad \text{s.t.} \quad
	(\forall i) \sum_j T_{ij} = a_i,
	\quad
	(\forall j) \sum_i T_{ij} = b_j
}
where $\matr T$ is called the \green{transportation matrix}. Informally, the constraints are simply enforcing bijection to/from $\mu$ and $\nu$, in that ``all the mass sent from element $i$ must be exactly $a_i$, and all mass sent to element $j$ must be exactly $b_j$''. A particular case called the \green{p-Wasserstein distance}, where $\Omega = \R^d$ and $M_{ij}$ is a distance metric over $\R^d$, is defined as
\graybox{
	W_p(\mu, \nu)
	&\triangleq \text{OT}(\mu, \nu; D_{\Omega}^p)^{1/p}
}
where $D$ is just a distance metric, e.g. for $p=2$ it could be euclidean distance. 

\myspace
\p \blue{Distributional Estimate} (4.1). Let $\mathcal C \triangleq \{ c  \}_i$ be the set of possible contexts, where each context $c_i$ can be a word/phrase/sentence/etc. For a given word $w$ and our set of observed contexts for that word, we essentially want to embed its context histogram into a space $\Omega$ (where typically $\Omega = \R^d$).  Let $\matr V$ denote a matrix of context embeddings, such that $V_{i,:} = \vec[i]{c} \in \R^d$, the embedding for context $c_i$ in what the authors call the \textit{ground space}. Combining the histogram $H^w$ containing observed context counts for word $w$ with $\matr V$, the \green{distributional estimate} of the word $w$ is defined as
\begin{align}
P_{\matr V}^w
&\triangleq \sum_{c \in \mathcal C} (H^w)_c \delta(\vec[c]{v})
\end{align}
Also, the \textit{point estimate} is just $\vec[w]{v}$, i.e. the embedding of the word $w$ when viewed as a context. 

\myspace
\p \blue{Distance} (4.2). Given some distance metric $D_{\Omega}$ in ground space $\Omega = \R^d$, the distance between words $w_i$ and $w_j$ is the solution to the following OT problem\footnote{I'm not sure whether the rightmost exponent of $p$ is a typo in the paper, but that is how it is written.}:
\begin{align}
\text{OT}(P_{\matr V}^{w_i},  P_{\matr V}^{w_j}; D_{\Omega}^p)
&:= W_p^{\lambda}( P_{\matr V}^{w_i},  P_{\matr V}^{w_j}  )^p
\end{align}

\myspace
\p \blue{Concrete Framework} (5). The authors make use of the \purple{shifted positive pointwise mutual information} (SPPMI), $\matr[s]{S}^{\alpha}$, for computing the word histograms:
\begin{align}
(H^w)_c
&:= \frac{
	\matr[s]{S}^{\alpha}(w, c)
}{  
	\sum_{c' \in \mathcal C} \matr[s]{S}^{\alpha}(w, c')
}\\
\matr[s]{S}^{\alpha}(w, c)
&\triangleq \max \left[ 
\log\left(  \frac{ \text{Count}(w, c) \sum_{c'} \text{Count}(c')^{\alpha} }{  \text{Count}(w)\text{Count}(c)^{\alpha}  }   \right) - \log(s),
~0
\right]
\end{align}






% ===========================================================================================
\lecture{Embeddings and Transfer Learning}{On the Dimensionality of Word Embedding}{December 16, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Z. Yin and Y. Shen, ``On the Dimensionality of Word Embedding,'' Stanford University (Dec 2018).}

\p \blue{Unitary Invariance of Word Embeddings} (2.1). Authors interpret result any unitary transformation (e.g. a rotation) on embedding matrix as equivalent to the original. 

\myspace
\p \blue{Word Embeddings from Explicit Matrix Factorization} (2.2). Let $M$ be the $V \times V$ co-occurrence counts matrix. One way of getting embeddings is doing a truncated SVD on $M = UDV^T$. If we want $k$-dimensional embedding vectors, we can do
\begin{align}
\matr E
&= \matr[1:k]{U} \matr[1:k, 1:k]{D}^{\alpha}
\end{align}
for some $\alpha \in [0, 1]$. 

\myspace
\p \blue{PIP Loss} (3). Given embedding matrix $E \in \R^{V \times d}$, define its \green{Pairwise Inner Product} (PIP) matrix to be 
\graybox{
	\text{PIP}(\matr E)
	&\triangleq \matr E \matr{E}^T 
}
Notice that $\text{PIP}(\matr E)_{i,j} = \langle \vec[i]{w}, \vec[j]{w} \rangle$. Define the \green{PIP loss} for comparing two embeddings $\matr[1]{E}$ and $\matr[2]{E}$ for a common vocab of $V$ words:
\graybox{
	|| \text{PIP}(\matr[1]{E}) - \text{PIP}(\matr[2]{E}) ||_F
	&= \sqrt{\sum_{i,j} \left(
		\langle  \vec[i]{w}^{(1)}, \vec[j]{w}^{(1)}    \rangle - 
		\langle  \vec[i]{w}^{(2)}, \vec[j]{w}^{(2)}    \rangle
		\right)^2}
}






% ===========================================================================================
\lecture{Embeddings and Transfer Learning}{Universal Language Model Fine-Tuning for Text Classification}{March 02, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize J. Howard and S. Ruder, ``Universal Language Model Fine-Tuning for Text Classification,'' (May 2018).}

\p \blue{TL;DR}. ULMFiT is a transfer learning method. They introduce techniques for fine-tuning language models.

\myspace
\p \blue{Universal Language Model Fine-tuning} (3). Define the general \green{inductive transfer learning} setting for NLP:
\begin{definition}[-1em]
	Given a source task $\mathcal{T}_s$ and target task $\mathcal{T}_T \ne \mathcal{T}_S$, improve performance on $\mathcal{T}_T$. 
\end{definition}
ULMFiT is defined by the following three steps:
\begin{compactenum}
	\item General-domain LM pretraining.
	\item Target task LM fine-tuning.
	\item Target task classifier fine-tuning.
\end{compactenum}

\myspace 
\p \blue{Target Task LM Fine-tuning} (3.2). For step 2, the authors propose what they call \green{discriminative fine-tuning} and \green{slanted triangular learning rates}.
\begin{itemize}
	\item \textbf{Discriminative fine-tuning}. Tune each layer with different learning rates:
	\graybox{
		\theta_t^{\ell} &= \theta_{t - 1}^{\ell} - \eta^{\ell} \cdot \nabla_{\theta^{\ell}} J(\theta)	
	}
	The authors suggest setting $\eta^{\ell - 1} = \eta^{\ell} / 2.6$. 
	
	\item \textbf{Slanted triangular learning rates}. A type of learning rate schedule that looks like the picture below. 
	\myfig[0.4\textwidth]{figs/ulmfit.png}
	
	\Needspace{10\baselineskip}
	First, we define the following hyperparameters:
	\begin{compactitem}
		\item $T$: \textit{total} number of training iterations\footnote{Steps-per-epoch times number of epochs.}
		\item $cfrac$: fraction of $T$ (in num iterations) where we're \textit{increasing} the learning rate. 
		\item $cut$: $\lfloor T \cdot cfrac  \rfloor$. Iteration where we switch from increasing the LR to decreasing it.
		\item $ratio$: $\eta_{max} / \eta_{min}$. We of course must also define $\eta_{max}$. 
	\end{compactitem}
	We can now compute the learning rate for a given iteration $t$:\marginnote{Suggested:\\ $cfrac{=}0.1$ \\ $ratio{=}32$\\ $\eta_{\max}{=}0.01$}[2em]
	\graybox{
		\eta_t &= \eta_{max} \cdot \frac{ 1 + p \cdot (ratio - 1)  }{ ratio } \\
		p &= \begin{cases}
			\frac{t}{cut} & \text{if } t < cut \\
			1 - \frac{t - cut}{ cut \cdot (1 / cfrac - 1) } & \text{otherwise} 
		\end{cases}
	}
\end{itemize}

\myspace
\p \blue{Target Task Classifier Fine-tuning} (3.3). Augment the LM with two fully-connected layers. The first with ReLU activation and the second with softmax. Each uses batch normalization and dropout. The first is fed the output hidden state of the LM concatenated with the max- and mean-pooled hidden states over all timesteps\footnote{Or just as much as we can fit into GPU memory.}:
\graybox{
	\vec[c]{h} &= \left[ 
	\vec[t]{h}, \text{maxpool}(\matr H), \text{meanpool}(\matr H)
	\right]
}
In addition to DF-T and STLR from above, they also employ the following techniques:
\begin{compactitem}
	\item  \green{Gradual Unfreezing}: first unfreeze the last layer and fine-tune it alone with all other layers frozen \textit{for one epoch}. Then, unfreeze the next layer and fine-tune the last-two layers only \textit{for the next epoch}. Continue until the entire network is being trained, at which time we just train until convergence.
	\item \green{BPTT for Text Classification}. Divide documents into fixed-length ``batches''\footnote{Not a fan of how they overloaded this term here.} of size $b$. They initialize the $i$th section with the final state of the model run on section $i -1$. 
\end{compactitem}






% ======================================================================
\lecture{Embeddings and Transfer Learning}{Hopfield Networks is All You Need}{August 23, 2020}
% ======================================================================

\citepaper{Ramsauer et al.}{Hopfield Networks is All You Need}{ELLIS Unit Linz and LIT AI Lab}{Jul 2020}

[See the review sub-section at this end of this note for a review on Hopfield networks]

\bluesec{From Binary Hopfield Networks to the Transformer}. First we define what the authors mean by ``binary modern Hopfield networks.'' Let $\matr X = \{ \vec[1]{x}, \ldots, \vec[N]{x} \}$ ($\matr X \in \{0, 1\}^{d \times N}$) be a set of desired binary vectors with $x_{ij} \in \{-1, +1\}$ $\forall i, j$. Let $\vec\xi$ denote the actual state of the units in the Hopfield model. Define the \green{energy function} $E$ with \green{interaction function} $F$:

\begin{table}{l | l | c | c}{}
 	Name & Energy Function & Interaction Function & Storage Capacity \\
	\toprule
		Modern Hopfield &
		$- \sum_{i=1}^{N} F\lr{ \vec\xi \dotp \vec[i]{x} }$ & 
		$x^n$ & 
		$d^{n-1}$ 
	\\ \hline
		Demircigil Generalization & 
		$- \exp\lr{  \text{lse} \lr{ 1, \matr{X}^T \vec\xi  }} $ & 
		$  \exp \lr{ x } $ & 
		$2^{d/2}$ 
	\\ \bottomrule
\end{table}
where
\begin{align}
 \text{lse}\lr{ \beta, \vec x}
		&\triangleq \beta^{-1} \log \lr{  \sum_{i=1}^{N} \exp\lr{  \beta x_i  }  }
\end{align}
Let $M = \max_i ||\vec[i]{x}||$. The authors generalize this energy function to continuous-valued patterns:\marginnote{Novel energy function and update rule}[2em]
\graybox{
	E 
		&= - \text{lse} ( \beta, \matr{X}^T \vec\xi )
			+ \onehalf \vec\xi^T \vec\xi + \beta^{-1} \log N + \onehalf M^2 \\
	\vec\xi^{new}
		&= f(\vec\xi) = \matr X \vec p = \matr X \text{softmax}\lr{ \beta \matr{X}^T \vec\xi }
}
with interpretation that $\vec p \in \R^{N}$ contains the softmax probabilities over each of the $N$ desired memories $\vec[i]{x}$. Notice that the ``retrieved'' memory $\vec\xi^{new}$ (after one update) has elements $\xi_d^{new}  = \sum_n p_n (\vec[n]{x})_d$, i.e. a convex combination over the desired memories. \\

\Needspace{10\baselineskip} 
Below I provide a summary of the theorems presented (and proved) by the authors:

\begin{itemdefinition}{Theorems \& Definitions}{}
	\item \purple{Convergence of E}.	Let $\vec\xi^*$ be a fixed point of the energy function. As $t \rightarrow \infty$, the energy converges $E(\vec\xi^t) \rightarrow E(\vec\xi^*)$, where $\vec\xi^{t+1} \leftarrow f(\vec\xi^t)$. 
	
	\item \purple{Convergence of $\vec\xi$}. As $t \rightarrow \infty$, $||\vec\xi^{t+1} - \vec\xi^t ||  \rightarrow 0$. 
	
	\item \green{Storage \& Retrieval of Patterns}. 
	\begin{compactitem}
		\item Assume:  $(\forall \vec[i]{x}) \exists S_i$, where $S_i$ is a sphere around $\vec[i]{x}$. 
		\item Storage: $\vec[i]{x}$ is \underline{stored} if $\exists \vec[i]{x}^* \in S_i$ to which all $\vec\xi \in S_i$ converge. 
		\item Retrieval: Given some $\vec\xi$, we say $\vec[i]{x}$ is \underline{retrieved} if $f(\vec\xi)$ converges to $\vec[i]{x}^*$. The retrieval error is $||\vec[i]{x} - \vec[i]{x}^*||$. 
	\end{compactitem}

	\item \green{Separation of Patterns}. Define the separation $\Delta_i$ of a pattern $\vec[i]{x}$ to other patterns by:
	\begin{align}
		\Delta_i 
			&\triangleq \min_{j, j\neq i} \lr{  \vec[i]{x}^T \vec[i]{x} - \vec[i]{x}^T \vec[j]{x}   } 
			= \vec[i]{x}^T \vec[i]{x} - \max_{j, j \neq i} \vec[i]{x}^T \vec[j]{x}
	\end{align}

	\item \purple{Convergence for Well-Separated Patterns}. For some query $\vec\xi$, after one update the distance of the new point $f(\vec\xi)$ to the fixed point $\vec[i]{x}^*$ is exponentially small in the separation $\Delta_i$.  The retrieval error also decreases exponentially with the separation $\Delta_i$. 
\end{itemdefinition}

\bluesec{Hopfield Update is Transformer Attention}. Now consider the case of transformers. Let some input $\vec[i]{y} \in \R^{d_{model}}$ be mapped to the Hopfield space of dimension $d_k$ to represent the query/keys for the transformer attention mechanism:
\begin{align}
	\vec[i]{x} &:= \matr[K]{W}^T \vec[i]{y} \\
	\vec[i]{\xi} &:= \matr[Q]{W}^T \vec[i]{y}
\end{align}

Denote the full sequence of $N$ inputs as the matrix $\matr Y = [ \vec[1]{y}; \ldots; \vec[N]{y} ]^T$, so $\matr Y \in \R^{N \times d_{model}}$. We can now define the query/key/value matrices:
\begin{align}
	\matr{Q}
		&= \matr{Y} \matr[Q]{W} \\
	\matr{K} 
		&= \matr{X}^T = \matr Y \matr[K]{W} \\
	\matr{V}
		&= \matr Y \matr[K]{W} \matr[V]{W} = \matr{X}^T \matr[V]{W}
\end{align}
Finally, letting $\beta := \inv{\sqrt{d_k}}$, we arrive at the formula for the transformer attention mechanism as our update rule. 


\bluesec{Analysis of Transformer and BERT Models}. The attention mechanism has three fixed point dynamics:
\begin{compactitem}
	\item[a)]  \textbf{Global fixed point}: if all $\vec[i]{x}$ are not well separated (i.e. if $p_i \approx \inv{N}$ for all $i$), then $\vec\xi$ approaches a vector close to the arithmetic mean of $\{\vec[i]{x}\}$. 
	
	 \item[b)] \textbf{Fixed point close to a single pattern}: if all $\vec[i]{x}$ are well-separated, $\vec\xi$ will converge to the most similar pattern $\vec[i]{x}$ and $\vec p$ will approach $\vec[i]{e}$. 
	
	\item[c)] \textbf{Metastable state}: when some vectors $\vec[i]{x}$ aren't well-separated from the rest, iterates near their associated ``metastable state'' will converge to it. 
\end{compactitem}
All of these can be thought of as special cases of (c), so the authors only consider metastable states, further categorizing into 4 classes:
\begin{compactitem}
	\item[\red{($\mathrm{I}$)}] Averaging over a \textit{very} large number of patterns (global fixed point). 
	\item[\orange{($\mathrm{II}$)}] Averaging over a large number of patterns (large metastable state).
	\item[\green{($\mathrm{II}$)}] Averaging over a medium number of patterns (medium metastable state). 
	\item[\blue{($\mathrm{IV}$})] Averaging over a small number of patterns (fixed point close to a single pattern). 
\end{compactitem}

The authors fed a sequence of $N$ tokens through BERT and plotted the distribution over $k$ -- the number of patterns required to sum up the softmax values to $0.9$ -- for each of the $N$ tokens in each attention head, and classified the heads into one of the four categories above. 
\myfig[0.7\textwidth]{figs/hopfield_fig2.png}


\clearpage
\subsub{Review: Hopfield Networks}
\myspace

(Mix of notes from Ch. 2 of Hertz et al. and  Ch. 42 of Mackay)

First, recall the general problem/task of \green{associative memory}: store a set of $p$ binary patterns $\{ \ivec[1]{x}, \ivec[2]{x}, \ldots \ivec[p]{x}\}$, where each $\ivec[i]{x} \in \{-1, +1\}^{N}$ is a binary vector\footnote{with +1/-1 instead of 1/0 for convenience as we'll see later.}, such that when presented with some new pattern $\vec{\hat x}$, the network responds by producing whichever stored pattern most closely resembles $\vec{\hat x}$\footnote{This notion of setting a network's activations in order for it to then re-adjust them to a different desired set of activations is referred to as a \green{content-adressable} memory.}. Below is the definition of the Hopfield model for accomplishing this behavior. 

\begin{itemdefinition}{Hopfield Network}{Let $w_{ij}$ denote connection from neuron $j$ to neuron $i$, and let $x_i$ denote the activity of neuron $i$.}
	\item \textbf{Architecture}. Consists of $I$ \underline{fully-connected} neurons with bidirectional ($(\forall w_{ij}) \exists w_{ji}$ symmetric ($w_{ij} = w_{ji}$)  connections\footnote{There are no self-connections, i.e. $w_{ii} = 0$ $\forall i$.}. Biases $w_{i0}$ may be included. 
	
	\item \textbf{Activity rule}. Each neuron updates its state via a threshold:
	\begin{align}
		x(a) = \Theta(a) \equiv \begin{cases}
			1 & a \ge 0 \\
			-1 & a < 0 
		\end{cases}
	\end{align}
	Due to feedback (fully-connected with symmetric weights), we have to specify an \textit{order} for computing these updates. 
	\begin{compactitem}
		\item \textbf{Synchronous}. All neurons compute their activations, $a_i = \vec[i]{w} \dotp \vec{x}$, then update their states simultaneously, $\vec x := \Theta(\vec a)$. 
		\item \textbf{Asynchronous}. One neuron at a time computes its activations and updates its state. The order may be fixed or random. 
	\end{compactitem}

	\item \textbf{Learning rule}. Our goal is to make a set of desired ``memories'' $\{\ivec[n]{x}\}$ be \green{stable states} (configurations) of [the activity rule of] our network.
	\graybox{
		\mtgreen{[Hebb rule]} \quad w_{ij} = \eta \sum_n \ival[n][i]{x} \ival[n][j]{x}
	}
\end{itemdefinition}

\bluesec{One Pattern}. Consider the base case where we only want to learn \textit{one pattern} $\ivec[1]{x}$. In other words, if we set the network activations $\vec x := \ivec[1]{x}$, and subsequently let the network apply it's activity rule one or more times (analogous to a forward pass), we want it to still remain in the configuration equal to $\ivec[1]{x}$. This can easily be accomplished if we define the connections to satisfy
\begin{align}
	w_{ij} \propto \ival[1][i]{x} \ival[1][j]{x}
\end{align}
since this would result in 
\begin{align}
	a_i &:= \sum_j w_{ij} \ival[1][j]{x} = \sum_j  \ival[1][i]{x} \ival[1][j]{x} \ival[1][j]{x} =  N  \ival[1][i]{x} \propto   \ival[1][i]{x} \\ 
	x_i &:= \Theta(a_i) = \Theta( \ival[1][i]{x}) =  \ival[1][i]{x} 
\end{align}
Furthermore, if fewer than half of the bits of the starting pattern are wrong\footnote{i.e. if the activation of neuron $i$ is not equal to $ \ival[1][i]{x}$}, it will still result in the correct response. For example, say $N_{c}$ bit are correct, and $N_{w} = N - N_{c}$ bits are wrong in the starting configuration ($N_c > N_w$). 
\begin{align}
	a_i &= N_{c}  \ival[1][i]{x} + \sum_{x_j \in \text{wrong neurons}} \lr{ \ival[1][i]{x} \ival[1][j]{x} } x_j \\
		&= N_c  \ival[1][i]{x} - N_w  \ival[1][i]{x} \\
		&= (N_c - N_w)  \ival[1][i]{x}
\end{align}
and thus it still results in the correct sign! In such a scenario, we say that $ \ivec{x}$ is an \green{attractor}. Also notice that, if $N_w > N_c$ instead, we'd end up in the \green{reversed state} $ -\ivec[1]{x}$. 

\bluesec{Multiple Patterns}. We can naturally generalize our observations regarding the situation when fewer than half of the bits are wrong to the scenario where we want to store a set of $p > 1$ patterns $\{ \ivec[1]{x}, \ivec[2]{x}, \ldots \ivec[p]{x}\}$. Consider what happens if we define our weight update rule as 
\graybox{
	w_{ij} &= \inv{N} \sum_{\mu = 1}^{p} \ival[\mu][i]{x} \ival[\mu][j]{x}
}
Notice how, if we set the starting configuration of the network to be one of the desired patterns, then this expands out to the same situation as having fewer than half of the bits begin wrong in the previous subsection (think about it). 







% ======================================================================
\lecture{Embeddings and Transfer Learning}{Learning to Summarize from Human Feedback}{April 10, 2020}
% ======================================================================

\citepaper{Stiennon et al.}{Learning to Summarize from Human Feedback}{OpenAI}{Sep 2020}


\bluesec{Introduction} (1). Misalignment between \textit{fine-tuning objective (NLL)} and generating \textit{high-quality outputs as determined by humans}. 
\begin{compactitem}
	\item Maximum likelihood doesn't distinguish between important errors and unimportant errors. 
	\item Probability mass is based on \textit{all} human demonstrations (input text). 
	\item Distributional shift during sampling can degrade performance. 
\end{compactitem}












% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Computer Vision}\label{Computer Vision}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------



% ======================================================================
\lecture{Computer Vision}{X3D}{February 01, 2021}
% ======================================================================

\citepaper{Christoph Feichtenhofer}{X3D: Progressive Network Expansion for Efficient Video Recognition}{FAIR}{Apr 2020}

\bluesec{Introduction} (1). Neural networks for video recognition typically just expand [inputs/features/kernels of] existing 2D architectures along spacetime. However, this is computationally demanding. This paper focuses on the \textbf{low-computation regime}. They do this by expanding along multiple possible axes:
\begin{compactitem}
	\item Temporal duration $\mcyan{\gamma_t}$ Number of frames fed as input. [XS=4], [S=13], [M=16]
	
	\item Inverse frame rate $\mgreen{\gamma_{\tau}}$ [sec/frame]. \sout{Number of seconds each frame represents}. Uh, not really? It is actually just the stride in units of number of [raw/original] frames in the video to take when sampling the $\gamma_t$ total frames. [XS=12], [S=6], [M=5]
	
	\item Spatial resolution $\mblue{\gamma_s}$. Height and width in units of pixels for each frame. 
	
	\item Network width $\mpurple{\gamma_w}$
	
	\item Bottleneck width $\mred{\gamma_b}$
	
	\item Depth $\mpink{\gamma_d}$
\end{compactitem}

Main underlying 2D architecture will be \textbf{MobileNet}, motivated by its use of \green{channel-wise separable convolutions} (CWSC). A typical convolution over a 2D image with $C$ channels has $K$ kernels, each with $D_k \times D_k \times C$ parameters. In contrast, CWSC splits the computation into two sequential layers/stages:
\begin{compactenum}
	\item For each of the $C$ input channels, convolve using a $D_k \times D_k$ kernel. Outputs are stacked along the $C$ outputs, one for each input channel \& associated kernel. 
	
	\item Apply $K$ new kernels, each with $1 \times 1 \times C$ parameters, along the stacked outputs from previous layer. 
\end{compactenum}

Notice how standard convolution thus has $C \times K \times D_k \times D_k $ parameters, while CWSC has $C \times \lr{ D_k \times D_k + K}$ parameters. Authors start with a tiny model, and progressively expand along a dimension (selected by best val performance) until a desired computation budget is reached. 

\bluesec{X3D Networks} (3). Authors are primarily interested in trade-offs between the following axes:
\begin{compactitem}
	\item \textbf{[Total] Input duration}: is it preferable to process long videos with sparse sampling of frames, or instead shorter videos with faster sampling? 
	
	\item \textbf{Spatial resolution}: is it necessary to maintain full resolution of e.g. a 4k video, or is it sufficient to use a lower resolution? 
	
	\item \textbf{Frame rate}: what is the relationship between frame-rate and channel resolution (number of filters/kernels/feature maps associated with each layer) in regards to performance [on metrics]?
	
	\item \textbf{Width}: when expanding network width, should we globally expand network width in ResNet block or expand the inner/bottleneck width?
	
	\item \textbf{Depth vs Resolution}: should going deeper be performed with expanding input resolution (in either of spatial/temporal dim)?
\end{compactitem}

\bluesec{X2D} (3.1). Basis instantiation. The input is a spatiotemporal frame with size $T \times S^2$, where $T$ is the number of [actual] frames and $S$ is the height and width of a square spatial crop. The basis instantiation sets $T=1$ (just uses the first frame). 


\myspace
\subsub{Architecture/Implementation In Detail}

Exhaustive overview of the architecture implementation code.

\bluesec{Preprocessing}. Note that sampling rate actually just means ``stride in units of number of frames'' because the authors apparently don't know how to name things appropriately. Let $N^*$, $P^*$ denote the number of frames and frames per second, respectively, in the original video. Each raw video is thus $N^* / P^*$ seconds long. Depending on the architecture (aka values of $\gamma$), we will extract some $N$ number of sequential frames ($N < N^*$) from the video with a stride of $S$ frames ($\gamma_t$). 


\bluesec{Input/Stem}. Features are [batches of] videos with shape $(C, N, H, W)$. The first pass through \texttt{VideoModelStem}, which consists of the following operations:
\begin{compactitem}
	\item Spatial convolution kernel size of 3 and a stride of 2.  Output spatial dims are halved. Channels are increased from 3 (RGB) to 24. 
	
	\item Depthwise temporal convolution with temporal kernel size of 5 and temporal stride of 1. Inputs are padded such that output temporal dim is unchanged. 
\end{compactitem}







% ======================================================================
\lecture{Computer Vision}{ResNet}{February 01, 2021}
% ======================================================================

\citepaper{He et al.}{Deep Residual Learning for Image Recognition}{Microsoft}{Dec 2015}

\bluesec{Deep Residual Learning} (3). Authors propose a building block of the form 
\begin{align}
	\vec y = \mathcal{F}\lr{ \vec x , \{ W_i \}  } + \vec x
\end{align}
where the notation $\{ W_i \}$ just means ``one or more weight matrices $W_i$.''






% ======================================================================
\lecture{Computer Vision}{SlowFast}{February 01, 2021}
% ======================================================================

\citepaper{Feichtenhofer et al.}{SlowFast Networks for Video Recognition}{FAIR}{Oct 2019}

\bluesec{Introduction}. Naively extending 2D convolution networks to 3D ignores the asymmetry of time. Perhaps it is more sensible to use a network that has two explicit components: one for understanding spatial dependencies (``slow'') and the other for temporal dependencies (``fast''). Their architecture is illustrated below.

\myfig[0.6\textwidth]{figs/slowfast_fig1.png}


\bluesec{SlowFast Networks} (3). 
\begin{compactitem}
	\item \textbf{Slow pathway}: convolutional model with a large temporal stride $\tau$ [e.g 16]. Note that the authors define $T$ as \textit{the number of frames sampled by the Slow pathway} (NOT the true number of frames from the original clip). The raw clip length is $T \times \tau$ frames. 
	
	\item \textbf{Fast pathway}: convolutional model with the following properties:
	\begin{compactitem}
		\item High frame rate. Temporal stride is $\tau / \alpha$ with $\alpha > 1$ [e.g. 8]. 
		\item High temporal resolution features. i.e. no temporal downsampling layers (pooling/stride) [until the global pooling layer before classification].
		\item Low channel capacity. Uses a ratio of $\beta$ ($\beta < 1$) [e.g. 1/8] channels wrt the slow pathway.  
	\end{compactitem}

	\item \textbf{Lateral connections}: one lateral connection between the two pathways for every ``stage''. Unidirectional fusing of fast pathway into the slow pathway.  Authors experiment with the following transformations:
	\begin{compactitem}
		\item \textit{Time-to-channel}: reshape and transpose $\lr{\alpha T, S^2, \beta C} \mapsto \lr{ T, S^2, \alpha \beta C }$
		\item \textit{Time-strided sampling}: sample one out of every $\alpha$ frames:  $\lr{\alpha T, S^2, \beta C} \mapsto \lr{ T, S^2 \beta C }$
	\end{compactitem}
\end{compactitem}
Also note that a global average pooling is performed on each pathway's [final] output. Then, two pooled feature vectors are concatenated as the input to the fully-connected classifier layer. 






% ======================================================================
\lecture{Computer Vision}{ViT}{February 01, 2021}
% ======================================================================

\citepaper{Dosovitskiy et al.}{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}{Google}{Oct 2020}


\myfig[0.6\textwidth]{figs/vit_fig1.png}

\bluesec{Method} (3). The input image pixels $\vec x \in \R^{H \times W \times C}$ are first grouped/reinterpreted as a grid of $N$ ($P \times P$) \textit{patches}, such that $N \times P \times P = H \times W$. The patches are each flattened, resulting in input $\vec[p]{x} \in \R^{N \times (P^2 \cdot C)}$. Denoting the transformer's model dimension as $D$, we therefore must first linearly project the sequence of $N$ flattened patches to dimension $D$\footnote{Note that all $N$ patches are projected with the same projection matrix $\matr E \in \R^{(P^2 \cdot C) \times D}$.}. The authors also prepend a learnable embedding to the sequence of embedded patches, $\vec[0]{z}^0 = \vec[class]{x}$, whose state at the output of the Transformer encoder ($\vec[L]{z}^0$ serves as the image representation $\vec y$.) \\

\myfig[0.7\textwidth]{figs/vit_eqs.png}

Alternatively, the authors also explore a \textbf{hybrid model} that uses patches obtained from the output of a CNN instead of the raw image. 



% ======================================================================
\lecture{Computer Vision}{Support-set bottlenecks for video-text representation learning}{February 01, 2021}
% ======================================================================

\citepaper{Patrick et al.}{Support-set bottlenecks for video-text representation learning}{FAIR, CMU, University of Oxford}{Jan 2021}


\bluesec{Introduction} (1). Noise contrastive learning discriminates any two data samples while being invariant to transformations like rotations or determining if an audio-video pair came from the same video. However, existing techniques still naively treat two different examples as different even if they share/contain similar semantic content. Authors propose \green{cross-instance captioning}, illustrated below, which modifies the text generator to take as input a learnable mixture of a support-set of videos. 

\myfig[0.8\textwidth]{figs/ssb_fig1.png}

\bluesec{Method} (3). Corpus $\mathcal C$ of video-text pairs $(v, t)$. Goal is to learn a pair of representation maps $c_v = \Psi(v)$ and $c_t = \Phi(t)$, where $c_v, c_t \in \R^d$ and semantically similar instances are close to each other. 

\textbf{Contrastive Objective}. Let $s(a, b) = \frac{a^T b}{||a|| ||b||}$ denote the similarity between vectors $a$ and $b$. Authors adopt the \green{hinge-based triplet ranking loss with hard negative mining}\footnote{Review: if we want to enforce $A > B + \alpha$, then we want to minimize $B + \alpha - A$. Hard negative mining refers to first finding a $B$ for which we are performing poorly on, and use that one. In other words, first find an instance $B$ such that $B + \alpha - A$ is \textit{maximized} (aka bad), and then try to minimize that value. Finally, by introducing the hinge function $[\cdot]_+$ we are only considering instances $B$ for which $A \leq B + \alpha$. In other words, only those instances $B$ that explicitly violate our constraint.}:\marginnote{Authors set $\alpha=0.2$}[2em]
\graybox{
	\mathcal{L}^{contrast}
		&= \inv{B} \sum_{i=1}^{B} \left[
			\max_j	\left[  \alpha - s(\mgreen{c_t^i}, \mgreen{c_v^i}) + s(\mgreen{c_t^i}, \mred{c_v^j}) \right]_+
		+ \max_j\left[  \alpha - s(\mgreen{c_t^i}, \mgreen{c_v^i}) + s(\mred{c_t^j}, \mgreen{c_v^i}) \right]_+
	\right]
}

\textbf{Cross-Captioning Objectives}. Authors modify the autoregressive negative log-likelihood to condition on a weighted average of the embeddings of other videos in the batch, which they call the \green{support set} $\mathcal S$. 
\graybox{
	\mathcal{L}^{cross-captioning}
		&= -\inv{B} \sum_{i=1}^{B} \log \Prob{ t^i \mid \bar{e}_v^i } \\
	 \bar{e}_v^i
	 	&= \sum_{j \in \mathcal S_i} \text{Softmax}\lr{ \langle c_t^i, \cdot \rangle }_j \cdot e_v^j
}
where the default support set $\mathcal S_i$ is simply all batch indices $j \neq i$. They also consider:
\begin{align}
	\mtgreen{[Identity captioning]}\qquad
		S_i &= \{ i \} \\
	\mtgreen{[Full support]}\qquad 
		S_i &= \{1, \ldots, B \} \\
	\mtgreen{[Hybrid captioning]}\qquad 
		S_i &=  \text{Avg}\left[ \{ i \}.   \{1, \ldots, B \}   \right] \\
	\mtgreen{[Cross-captioning]}\qquad 
		S_i &= \{j \neq i \}
\end{align}

The final loss is a combination of the two:\marginnote{Authors set $\lambda = 10$ to ensure similar magnitudes.}[3em ]
\begin{align}
	\mathcal{L}	
		&= \mathcal{L}^{contrast} + \lambda \mathcal{L}^{cross-captioning}
\end{align}

Finally, their architecture utilizes a pre-trained T5 transformer decoder for the caption. For the video representation $c_v = \Psi(v) = \Psi''(\Psi'(v))$, they use a video encoder $e_v = \Psi'(v)$ followed by a multi-layer transformer pooling head $c_v = \Psi''(e_v)$. 
\begin{align}
	\psi(e) &= \text{BN}\lr{  \text{FFN}\lr{  e_{attn} } + e_{attn}  } \\
	e_{attn}
		&= \text{BN}\lr{  \text{MHA}\lr{  f(e) }   + f(e)   }
\end{align}
where $f$ is a pre-encoder that refines the video representation. The output $\psi(e)$ is a contextualized sequence of representation vectors, and the authors take the first one as $c_v$. 





% ======================================================================
\lecture{Computer Vision}{TimeSformer}{March 26, 2021}
% ======================================================================

\citepaper{Bertasius et al.}{Is Space-Time Attention All You Need for Video Understanding?}{FAIR}{Feb 2021}


\bluesec{The TimeSformer Model} (3). Note that there is literally nothing novel/different compared to the typical GPT2-style transformer going on in the below description. I just wrote it out to double-check for my own sanity's sake. The reason they go over the equations in detail is because they will be modifying them in the next section. 
\begin{compactenum}
	\item Input clip $X \in \R^{H \times W \times 3 \times F}$.
	
	\item Frames are decomposed into $N \eq HW/P^2$ patches, each of size $P \times P \times 3$. 
	
	\item Flattened patches are projected to model dimensionality $D$:
	\begin{align}
		\vec[p,t]{z}^{(0)}
			&= E \vec[p,t]{x} + \vec[p,t]{e}^{pos}
	\end{align}
	Note how this implies the effective sequence length is $N \times F$. Authors also prepend special vector $\vec[0,0]{z}^{(0)}$ for classification embedding token. 
	
	\item Each of the $L$ transformer encoding blocks performs self-attention. At each block $\ell$:
	\begin{compactenum}
		\item The $\vec q$, $\vec k$, and $\vec v$ vectors are computed for each attention head $a$. 
		
		\item Self-attention weights $\vec[p,t]{\alpha}^{\ell,a} \in \R^{NF+1}$ for each query patch $(p, t)$ are given by
		\begin{align}
			\vec[p,t]{\alpha}^{\ell,a}
				&= \text{Softmax}\lr{ 
						\inv{\sqrt{D_h}} \vec[p,t]{q}^{\ell,a^{~T}} \matr{K}^{\ell, a}
				} \qquad \text{where} \quad \matr{K}^{\ell, a} \in \R^{D_h \times NF+1 }
		\end{align}
	Note that $	\vec[p,t]{\alpha}^{\ell,a}$ is itself a vector, whose indices correspond to $(p', t')$. 
	
	\item Concatenation of the  attention-weighted-average of the value vectors is fed through an MLP to obtain $\vec[p,t]{z}^{\ell}$. 
	\end{compactenum}

	\item The final video embedding is obtained from the final block for the classification token:
	\begin{align}
			\vec y &= \text{LN}\lr{  \vec[0,0]{z}^{(L)}  } \in R^D
	\end{align}
	which is subsequently fed through a 1-layer MLP to obtain the final class predictions. 
\end{compactenum}

Authors propose modified attention which they call \green{divided space-time attention}. 
\graybox{
	\vec[p,t]{\alpha}^{\ell,a^{~\text{time}}}
	&= \text{Softmax}\lr{ 
		\inv{\sqrt{D_h}} \vec[p,t]{q}^{\ell,a^{~T}} \matr[:, p,\cdot]{K}^{\ell, a}
	}
}
aka literally just use the vectors in $\matr K$ where $p'=p$. \textbf{Critically}, after computing the attention-weighted-average value vectors and concatenating the heads, they feed the resulting vector through \textit{another} attention mechanism that attends only over \textit{space}.


\Needspace{10\baselineskip}
Note that their attention variant is NOT simply a mask, but an architectural change:
\myfig[0.8\textwidth]{figs/timesformer_fig1.png}

\begin{myquote}
	During inference, unless otherwise noted, we sample a single temporal clip in the middle of the video. We use 3 spatial crops (top-left, center, bottom-right) so that the entire video clip is spatially covered. The final prediction is obtained by averaging the softmax scores of these 3 predictions.
\end{myquote}




























% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Vision and Language}\label{Vision and Language}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------



% ===========================================================================================
\lecture{Vision and Language}{Show, Ask, Attend, and Answer}{August 10, 2019}
% ===========================================================================================
\vspace{-1.5em}
{\footnotesize V. Kazemi and A. Elqursh, ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' \textit{Google Research} (April 2017).  arXiv:1704.03162v2}

\bluesec[-0.2]{Method} (3). Given a training set of image-question-answer triplets $(I, q, a)$, learn to estimate the most likely answer $\hat a$ out of the set of most frequent answers\footnote{Same approach as how we define vocabulary in language modeling tasks.} in the training data:
\begin{align}
\hat a = \argmax_{a} \Prob{a \mid I, q}
\end{align}

The method utilizes the following architectural components:
\begin{compactitem}
	\item \textbf{Image Embedding} (3.1). Extracts features $\phi \eq \text{CNN}(I)$. 
	
	\item \textbf{Question Embedding} (3.2). Encode question $q$ as the final state of LSTM: $\vec s \eq \text{LSTM}(\text{Embed}(q))$. 
	
	\item \textbf{Stacked Attention} (3.3)\footnote{Authors do a laughably poor job at describing this part in any detail, so I'm taking the liberty of filling in the blanks. Blows my mind that papers this sloppy can even be published.} Seems like they feed $\text{Concat}[\vec s, \phi]$ through two layers of convolution to produce an output $F_c$ for $c \in [1..C]$ (meaning they do $C$ such convolutions separately an in parallel, like multi-head attention). This represents the scores for the attention function. The attention output, as usual, is computed as 
	\begin{align}
	\vec[c]{x} &= \sum_{\ell} \alpha_{c, \ell} \phi_{\ell} \\
	\alpha_{c, \ell} &\propto \exp F_c(\vec s, \phi_{\ell})
	\end{align}
	where $\ell$ appears to be over all [flattened] spatial indices of $\phi$. 
	
	\item \textbf{Classifier} (3.4). Concat the \green{image glimpses} $\vec[c]{x}$ with the LSTM output $\vec s$ and feed through a couple FC layers to eventually obtain softmax probabilities over each possible answer $a_i$, $i \in [1..M]$. 
\end{compactitem}

\bluesec{Dataset}. Although the authors are horribly vague/sloppy here, it \textit{seems} like the data they use actually provides $K$ ``correct'' answers for each image-question pair. The model loss is therefore an average NLL over the $K$ true classes. 



% ===========================================================================================
\lecture{Vision and Language}{Neural Module Networks}{September 14, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Andreas et al., ``Deep Compositional Question Answering with Neural Module Networks'' \textit{UC Berkeley} (Nov 2015).}


\bluesec{NMNs for Visual QA} (4). Model and task overview:
\begin{compactitem}
	\item \textbf{Data}: 3-tuples $(w, x, y)$ containing the question, image, and answer, respectively.
	
	\item \textbf{Model}: fully specified by a collection of \green{modules} $\{m\}$. Each module $m$ has parameters $\theta_m$ and a \green{network layout predictor} $P(w)$ that maps from strings to networks. The high-level procedure is, for each $(w, x, y)$, do:
	\begin{compactenum}
		\item Instantiate a network based on $P(w)$.
		\item Pass the image $x$ (and possibly $w$ again) as inputs to the network. 
		\item Obtain network outputs encoding $p(y \mid w, x; \theta)$. 
	\end{compactenum}
	
	\item \textbf{Modules}:
\end{compactitem}
\myfig[0.6\textwidth]{figs/nmn_modules.png}

\bluesec{From strings to networks} (4.2).
\begin{compactenum}
	\item Parse question $w$ with the Stanford Parser to obtain universal dependency representation.
	
	\item Filter dependencies to those connected to the wh-word in the question. Some examples:
	\begin{compactitem}
		\item \textit{what is standing in the field} $\mapsto$ \texttt{what(stand)}
		\item \textit{what color is the truck} $\mapsto$ \texttt{color(truck)}
		\item \textit{is there a circle next to a square} $\mapsto$ \texttt{is(circle, next-to(square))}
	\end{compactitem} 
	
	\item Assign identities of modules (already have full network structure). 
	\begin{compactitem}
		\item Leaves become \texttt{attend} modules.
		\item Internal nodes become \texttt{re-attend} or \texttt{combine} modules.
		\item Root nodes become \texttt{measure} (y/n questions) or \texttt{classify} (everything else) modules.
	\end{compactitem}
\end{compactenum}

\bluesec{Answering natural language questions} (4.3). They combine the results from the module network with an LSTM, which is fed the question as input and outputs a predictive distribution over the set of answers\footnote{This is the same distribution that the root module is trying to predict}. The final prediction is a geometric average of the LSTM output probabilities and the root module output probabilities. 




% ===========================================================================================
\lecture{Vision and Language}{Learning to Compose Neural Networks for QA}{September 14, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Andreas et al., ``Learning to Compose Neural Networks for Question Answering'' \textit{UC Berkeley} (June 2016).}


\bluesec{TL;DR}. Improve initial NMN work (previous note) by (1) learning network predictor ($P(w)$ in previous paper) instead of manually specifying it, and (2) extending visual primitives from previous work to reason over structured world representations.

\myfig[0.5\textwidth]{figs/nmn2_fig1.png}

\bluesec{Model} (4). Training data consists of (world, question, answer) triplets $(w, x, y)$. The model is built around two distributions:
\begin{compactitem}
	\item \green{layout model} $p(z \mid x; \theta_{\ell})$ which predicts a layout $z$ for sentence $x$. 
	\item \green{execution model} $p_z(y \mid w; \theta_e)$ which applies the network specified by $z$ to the world representation $w$. 
\end{compactitem}

\Needspace{18\baselineskip}
\bluesec{Evaluating Modules} (4.1). The execution model is defined as
\begin{align}
p_z(y \mid w) &= \left( \llbracket z \rrbracket_w  \right)_y
\end{align}
where $\llbracket z \rrbracket_w$ denotes the output of network with layout $z$ on input world $w$. The defining equations for all modules is as follows ($\sigma \equiv $ ReLU, $sm \equiv$ softmax):\marginnote{$\bar{w}(h) \triangleq \sum_k h_k w^{(k)}$}[3em]
\newcommand\nmn[1]{\llbracket \texttt{\small #1} \rrbracket}
\graybox{
	\nmn{lookup[i]} 
	&= e_{f(i)} \\
	\nmn{find[i]}
	&= \text{sm}(a \odot \sigma(B v^i \oplus CW \oplus d) ) \\
	\nmn{relate[i](h)}
	&= \text{sm}(
	a \odot 
	\sigma( 
	Bv^i \oplus CW \oplus D \bar{w}(h) \oplus e				
	)) \\
	\llbracket \texttt{\small and} (h^1, h^2, \ldots) \rrbracket
	&= h^1 \odot h^2 \odot \cdots \\
	\nmn{describe[i](h)}
	&= sm(A \sigma (B \bar w(h)) + v^i) \\
	\nmn{exists(h)}
	&= sm\left(  \left(\max_k h_k \right)a + b  \right)
}
To train, maximize 
\begin{align}
\sum_{(w, y, z)} \log p_z (y \mid w; \theta_e)
\end{align}

\bluesec{Assembling Networks} (4.2). \red{TODO}: finish note



% ===========================================================================================
\lecture{Vision and Language}{End-to-End Module Networks for VQA}{September 14, 2019}

% ===========================================================================================
\vspace{-1em}
{\footnotesize R. Hu, J. Andreas, et al., ``Learning to Reason: End-to-End Module Networks for Visual Question Answering'' \textit{UC Berkeley, FAIR, BU} (Sep 2017).}

\bluesec{End-to-End Module Networks} (3). High-level sequence of operations, given some input question and image:
\begin{compactenum}
	\item Layout policy predicts a coarse functional expression that describes the structure of the computation.
	
	\item Some subset of function applications within the expression receive parameter vectors predicted from the text. 
	
	\item Network is assembled with the modules according to layout expression to output an answer. 
\end{compactenum}


\bluesec{Attentional Neural Modules} (3.1). A neural module $m$ is a parameterized function $y = f_m(a_1, a_2, \ldots; x_{vis}, x_{txt}, \theta_m)$, where the $a_i$ are image attention maps and the output $y$ is either an image attention map or a probability distribution over answers. The full set of modules used by the authors, along with their inputs/outputs, is tabulated below. 
\myfig[0.7\textwidth]{figs/n2nmn_modules.png}

Note that, whereas the original NMN paper (see previous note) instantiated module types based on words (e.g. describe[shape] vs describe[where]) and gave different instantiations different parameters, this paper has a single module for each module type (no``instances'' anymore). To distinguish between cases where e.g. \texttt{describe} should describe a shape vs describing a location, the module incorporates a text feature $x_{txt}^{(m)}$ computed separately/identically for each module $m$:
\begin{align}
x_{txt}^{(m)} &= \sum_{i=1}^T \alpha_i^{(m)} w_i
\end{align}

\bluesec{Layout Policy with Seq2Seq RNN} (3.2). \red{TODO} finish note













% ===========================================================================================
\lecture{Vision and Language}{Fast Multi-language LSTM-based Online Handwriting Recognition}{October 01, 2019}

% ===========================================================================================
\vspace{-1em}
{\footnotesize Carbune et al., ``Fast Multi-language LSTM-based Online Handwriting Recognition'' \textit{Google AI Perception} (Feb 2019).}


\bluesec{Introduction} (1). Task: given input strokes, i.e. sequences of points $(x, y, t)$, output it in the form of text.

\bluesec{Model Architecture} (2). The high-level sequences of operations is:
\begin{compactenum}
	\item Input time series $(v_1, \ldots, v_T)$ encoding user input. The authors experiment with two representations:
	\begin{compactenum}
		\item \textit{Raw touch points}: sequence of 5-dimensional points $(x_i, y_i, t_i, p_i, n_i)$, where $t_i$ is seconds since first touch point in current observation, $p_i$ is binary-valued equal to 0 if pen-up, else 1 if pen-down, and $n_i$ is binary on start-of-new-stroke (1 if True). 
		\item \textit{B\'{e}zier curves}: TL;DR is that they model x, y, t each as a cubic polynomial over a new variable $s \in [0, 1]$. Ultimately this means solving for some coefficients $\Omega$ of a linear system of equations: $V^T Z = V^T V \Omega$. 
	\end{compactenum}
	
	\item Several BiLSTM layers for contextual character embedding.
	
	\item Softmax layer providing character probabilities at each time step. 
	
	\item CTC decoding with beam search. They also incorporate \green{feature functions} into the output logits to help with decoding. They use the following 3 feature functions:
	\begin{compactenum}
		\item Character language models. A 7-gram LM over Unicode codepoints using Stupid back-off.
		
		\item Word language models. 3-grams pruned to between 1.25M and 1.5M entries.
		
		\item Character classes. Scoring heuristic which boosts the score of characters from the LM's alphabet.
	\end{compactenum}
\end{compactenum}


\bluesec{Training} (3). Training happens in two stages, each on a different dataset:
\begin{compactenum}
	\item Training neural network model with CTC loss on large dataset.
	
	\item Decoder tuning using \green{Bayesian optimization} through \green{Gaussian Processes} in Vizier\footnote{Vizier is a  program made by Google for black-box tuning}.
\end{compactenum}




% ======================================================================
\lecture{Vision and Language}{Multi-Language Online Handwriting Recognition}{October 02, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Keysers et al., ``Multi-Language Online Handwriting Recognition'' \textit{Google} (June 2017).}


\bluesec{System Architecture} (3). Segment-and-decode approach consisting of the following steps:
\begin{compactitem}
	\item Preprocessing (4).
	\begin{compactenum}
		\item Resampling. 
		\item Slope correction.
	\end{compactenum}
	
	\item \green{Segmentation}\footnote{\textbf{Segmentation/cut point}: a point at which another character my start. \textbf{Segment}: the (partial) strokes between 2 consecutive segmentation points.} and search lattice creation (5).
	\begin{compactenum}
		\item Segmentation goal: obtain high \textit{recall} of all actual character boundaries. Accomplished via a heuristic which creates a set of potential cut points and then a neural net which assigns a score to each.
		
		\item Segmentation lattice: a graph $(V, E)$ of ink segments. Each segment is identified by a unique integer index. 
		\begin{compactitem}
			\item Nodes (in $V$) define the path of ink segments up to that point (e.g. $\{1, 0, 2\}$) (i.e. a character hypothesis)
			\item Edges (in $E$) from a given node $v$ indicate the ink segments which are grouped in a character hypothesis. For example, if $v\eq\{i, j\}$ has some edge $k$, then that edge will have node $\{i, j, k\}$ on the other end, and $\{i, j, k\}$ is a valid character hypothesis. 
		\end{compactitem}
		It appears that each node (assign from the empty start node) is passed to the next stage as a character hypothesis to be scored/classified.
	\end{compactenum}
	
	\item Generation \& scoring of \green{character hypotheses}\footnote{\textbf{Character hypothesis}: a set of one or more segments (not necessarily consecutive).} (5.3). Goal: cetermine the characters most likely to have been written.
	\begin{compactenum}
		\item Feature extraction: they make a fixed-length dense feature vector containing \textit{pointwise} and \textit{character-global} features.
		
		\item Classification: single hidden layer NN with tanh activation followed by softmax.
		
		\item Create a labeled latice which will later be decoded to find the final recognition result.
	\end{compactenum}
	
	\item Best path search in the resulting lattice using additional knowledge sources (6). 
\end{compactitem}


\bluesec{Language Models} (6.1). They utilize two types of language models:
\begin{compactitem}
	\item Stupid-backoff entropy-pruned 9-gram character LM. This is their ``main'' LM. Depending on the language, they use about 10M to 100M n-grams.
	\item Word-based probabilistic finite automaton.  Creating using 100K most frequent words of a language.
\end{compactitem}


\bluesec{Search} (6.2). Goal: obtain a recognition result by finding the best path from the source node (no ink recognized) to the target node (all ink recognized). Algorithm: ink-aligned beam search that starts at the start node and proceeds through the lattice in topological order. 









% ======================================================================
\lecture{Vision and Language}{Learning Two-Branch Neural Networks for Image-Text Matching Tasks}{April 05, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Wang et al., ``Learning Two-Branch Neural Networks for Image-Text Matching Tasks'' \textit{UoI Urbana-Champaign, Georgia Tech}, (May 2018).}

\bluesec{Introduction}. How can we measure simantic similarity between visual data and text data? Authors propose two approaches:
\begin{compactitem}
	\item \green{Embedding Network}: map image/text to joint latent space of common dimensionality. Train with bidirectional ranking loss augmented with neighborhood information. 
	
	\item \green{Similarity Network}: binary classifier outputting +1 for positive pairs and -1 for negative pairs. 
\end{compactitem}

\bluesec{Overview of Image-Text Tasks} (3.1). 
\begin{compactitem}
	\item \green{Phrase Localization}: (image, entity mention) $\mapsto$ bounding box. 
	
	\item \green{Bidirectional Image-Sentence Retrieval}: either (image, sentence database) $\mapsto$ best matching sentence OR (sentence, image database) $\mapsto$ best matching image.
\end{compactitem}

\bluesec{Embedding Network} (3.2). Let $V$ and $T$ denote the collections of training images/regions and sentences/phrases. For a given visual input\footnote{Visual inputs can be either full images or individual regions. For phrase localization, it's a region.} $v$, let $T^+$ and $T^-$ denote matching/non-matching text samples\footnote{I'll denote invidual samples from these sets with lowercase, like $t^+$ for a positive example, and with the subscript $i$ implied/obvious given the context.}. Let $N(v)$ denote the \green{neighborhood} of image/region $v$: the set of images/regions described by the same text as $v$ (and defined similarly for the text case). They define their margin-based loss function with neighborhood-perserving constraints as:\marginnote{They set $m=0.05$}[2em]
\graybox{
	L_{marg}(a, b^+, b^-)
	&\triangleq \left[  m + d(a, b^+) - d(a, b^-) \right]_+ \\
	L_1(v)
	&= \sum_{t^+, t^-} L_{marg}(v, t^+, t^-) \\
	L_2(t)
	&= \sum_{v^+, v^-} L_{marg}(t, v^+, v^-) \\
	L_3(v)
	&= \sum_{v_j \in N(v), v_k \notin N(v)} L_{marg}(v, v_j, v_k) \\
	L_4(t)
	&= \sum_{t_j \in N(t), t_k \notin N(t)} L_{marg}(t, t_j, t_k) \\
	L(V, T)
	&= \sum_{v \in V} \left[ \lambda_1 L_1(v) + \lambda_3 L_3(v) \right]
	+   \sum_{t \in T} \left[ \lambda_2 L_2(t) + \lambda_4 L_4(t) \right]
}

\red{TODO}: write details of triplet sampling algorithm(s). 




% ======================================================================
\lecture{Vision and Language}{Learning by Abstraction: The Neural State Machine}{April 06, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize D. Hudson and C. Manning, ``Learning by Abstraction: The Neural State Machine'' \textit{Stanford}, (Nov 2019).}


\begin{itemdefinition}{Neural State Machine}{Defined as a tuple $(C, S, E, \{r_i\}_{i=0}^{N}, p_0, \delta)$}
	\item \textbf{C}: model's \purple{alphabet}, consisting of a set of concepts, embedded as learned vectors.
	
	\item \textbf{S}: states.
	
	\item \textbf{E}: directed edges that specify valid transitions between states.
	
	\item $\bm{r_i}$:  sequence of \purple{instructions}, each of dimension $d$, subsequently passed as input to $\delta$. 
	
	\item $\bm{p_0}$: $S \mapsto [0, 1]$. Initial-state probability distribution.
	
	\item $\bm{\delta_{S,E}}$: $p_i \times r_i \mapsto p_{i+1}$. \purple{State transition function}: ``a neural module that at each step i considers the distribution $p_i$ over the states as well as an input instruction $r_i$, and uses it to redistribute the probability along the edges, yielding an updated state distribution $p_{i+1}$.'' 
\end{itemdefinition}


\bluesec[-0.5]{Concept Vocabulary} (3.1). The authors define $L$ types of \purple{attributes} and also include identity and relations (the ``+2'') to define their \green{concept vocabulary} $C$:
\begin{compactitem}
	\item $C_0$: object identity (e.g. \textit{cat}, \textit{shirt})
	
	\item $\cup_{i=1}^L C_i$: attributes (e.g. \textit{colors}, \textit{materials})
	
	\item $C_{L+1}$: relations (e.g. \textit{holding}, \textit{behind})
\end{compactitem}
for a total of $L+2$ property types. They store an associated embedding for each property type and denote this set of embeddings as $D$.

\bluesec{States and Edge Transitions} (3.2). Construct a \green{probabilistic scene graph}:
\begin{compactitem}
	\item Nodes (states) represent objects in an image. Each node in $S$ is accompanied by:
	\begin{compactitem}
		\item A bounding box.
		\item \red{Dense visual features}. 
		\item Collection of discrete probability distributions $\{P_i\}_{i=0}^{L}$ for each of the object's $L+1$ \purple{semantic properties}\footnote{``Such as color, material, shape, etc.''}, defined over the concept vocabulary $\{C_i\}_{i=0}^{L}$. 
	\end{compactitem} 
	
	\item Edges (valid transitions) represent relations between objects. Each edge has an associated probability distribution $P_{L+1}$ of its \purple{semantic type}\footnote{Such as ``on top of'', ``eating''} among the concepts in $C_{L+1}$. 
\end{compactitem}

\Needspace{10\baselineskip}
Then, compute structured embedded representations for each state and transition ege. For each state $s \in S$, define a set of $L+1$ \green{property variables} $\{s^j\}_{j=0}^{L}$:
\begin{align}
s^j &= \sum_{c_k \in C_j} P_j(k) c_k
\end{align}
where $c_k \in C_j$ denotes each embedded concept of the $j$th property type, and $P_j$ refers to the corresponding distribution over these concepts [for state $s$]. 


Similarly, for each edge $e \in E$, compute relation embedding:
\begin{align}
e' &= \sum_{c_k \in C_{L+1}} P_{L+1}(k) c_k
\end{align}


\bluesec{Reasoning Instructions} (3.3). Translate the input questions into a sequence of reasoning instructions:
\begin{compactenum}
	\item Embed each question word with GloVe. 
	
	\item Compute a similarity-based distribution
	\begin{align}
	P_i &= \text{softmax}\lr{ w_i^T \matr W C }
	\end{align}
	where $\matr W$ is initialized to the identity matrix and $C$ denotes the ``matrix of all embedded concepts \emph{along with an additional learned default embedding $c'$ to account for structural or other non-content words}''.
	
	\item Translate each word into a concept-based representation\footnote{They also subsequently refer to these as the ``normalized'' words.}:
	\begin{align}
	v_i &= P_i(c') w_i  + \sum_{c \in C \setminus \{c'\}} P_i(c) c
	\end{align}
	
	
	\item Feed through attention encoder-decoder to obtain instruction sequence. 
	\begin{compactenum}
		\item Feed sequence of $P$ normalized words $V^{P \times d} = \{v_i\}_{i=1}^{P}$ through LSTM encoder to obtain final state $q$ (representing the question). 
		
		\item Run decoder for fixed number of steps $N\mred{+1}$ (?), yielding $N+1$ hidden states $\{h_i\}_{i=0}^{N}$. 
		
		\item Transform each decoder hidden state $h_i$ into a corresponding \green{reasoning instruction}:
		\begin{align}
		r_i &= \text{softmax}(h_i V^T) V
		\end{align}
	\end{compactenum}
\end{compactenum}


\bluesec{Model Simulation} (3.4). 
\begin{compactenum}
	\item Determine each \textit{instruction's type} by computing distribution over the $L+2$ embedded property types in $D$:
	\begin{align}
	R_i = \text{softmax}(r_i^T \circ D)
	\end{align}
	and denote $r_{i'} \triangleq R_i(L+1)$ that corresponds to the relation property. 
	
	\item Compare each instruction $r_i$ for all $s \in S$ and $e \in E$ via a \green{relevance score}:
	\begin{align}
	\gamma_i(s)
	&= \sigma \lr{ \sum_{j=0}^{L} R_i(j)\lr{  r_i \circ \matr[j]{W} s^j  }  } \\
	\gamma_i(e)
	&= \sigma \lr{   r_i \circ \matr[L+1]{W} e' }
	\end{align}
	
	\item Shift attention $p_i$ from current nodes $s \in S$ to their most relevant neighbors:
	\graybox{
		p_{i+1}^s 
		&= \text{softmax}_{s \in S}\lr{ \matr[s]{W} \cdot \gamma_i\lr{s} } \\
		p_{i+1}^r
		&= \text{softmax}_{s \in S}\lr{ \matr[r]{W} \cdot \sum_{(s', s) \in E} p_i(s') \cdot \gamma_i\lr{\lr{s', s}} } \\
		p_{i+1}
		&= r'_i \cdot p_{i+1}^r + (1 - r'_i) \cdot p_{i+1}^s
	}
	
	\item Repeat over $N$ steps. 
\end{compactenum}
Finally to predict the answer, feed a 2-layer FC-softmax classifier the concatenated vector $[q; m]$, where
\begin{align}
m &= \sum_{s \in S} p_N(s) \lr{ \sum_{j=0}^L R_N\lr{j} \cdot s^j }
\end{align}
``reflects the information extracted from the final states as guided by the final reasoning instruction $r_N$.''



% ======================================================================
\lecture{Vision and Language}{Tags and Attributes based Visual Commonsense Reasoning Baselines}{April 11, 2020}
% ======================================================================

\citepaper{Lin et al.}{TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning Baselines}{UoI Urbana-Champaign}{Jan 2020}

\bluesec{TL;DR}: Smaller/simpler model can get SOTA on VCR. Architecture summary:
\begin{compactenum}
	\item Feed collection of object detections through \purple{Faster RCNN}. 
	
	\item Feed query and responses [separately] through \purple{BERT}. 
	
	\item Concat word embeddings with their associated image embeddings. 
	
	\item Pass the concatenated image/word embeddings through an LSTM\footnote{This decision makes little sense to me, considering there is no meaningful temporal dependence of the data we are sequentially feeding to the LSTM.}.
	
	\item Pool outputs of LSTM to get a joint image+sentence embedding.
\end{compactenum}


\myfig[0.7\textwidth]{figs/tab_vcr_model.jpeg}


\bluesec{Attribute-Based VCR} (3). Each data point is denoted by a triplet $(\vec o, \vec q, \vecseq[4]{r})$:
\begin{compactitem}
	\item Image data denoted by the set $\vec o = \{o_i\}_{i=1}^{n_o}$ where each $o_i$ consists of a bounding box $b_i$ and class label $l_i \in \mathcal L$.
	
	\item Query sequence $\vec q = \{q_i\}_{i=1}^{n_q}$ where each $q_i$ is either a word  in $\mathcal V$ or a \purple{tag} referring to a bounding box in $\vec o$. 
	
	\item Four responses, each denoted as $\vec r = \{r_i\}_{i=1}^{n_r}$, where each $r_i$  is either a word  in $1\mathcal V$ or a \purple{tag} referring to a bounding box in $\vec o$. 
\end{compactitem}


\bluesec{Improving Visual Representation \& Image-Text Grounding} (3.3). Authors use a pretrained attribute classifier which augments every detected bounding box $b_i$ (provided by dataset) with a set of attributes (color, texture, size, emotions). Authors also develop the algorithm below for finding new text-image groundings (\textit{tags}) missing from the original VCR dataset. 

\myfig[0.7\textwidth]{figs/tab_vcr_new_tags.jpg}




% ======================================================================
\lecture{Vision and Language}{From Recognition to Cognition: Visual Commonsense Reasoning}{April 14, 2020}
% ======================================================================

\citepaper{Zellers et al.}{From Recognition to Cognition: Visual Commonsense Reasoning}{UW and Allen Institute for Artificial Intelligence}{Mar 2019}

\bluesec{Task Overview} (2). The primary task is \green{staged answering and justification} in a multiple-choice setting ($\bm{Q \rightarrow AR}$):
\begin{myquote}
	Given a question along with four answer choices, a model must first select the right answer. If its answer was correct, then it is provided four rationale choices (that could purportedly justify its correct answer), and it must select the correct rationale.
\end{myquote}
Authors present the following multiple-choice \textit{sub-tasks}:
\begin{compactitem}
	\item \green{Answering} $\bm{Q \rightarrow A}$: Given query, select the correct answer.
		
	\item \green{Justification} $\bm{QA \rightarrow R}$: Given concatenated [query, correct answer], select the correct rationale.
\end{compactitem}

\begin{itemdefinition}{VCR subtask}{A single example consists of an image $\matr I$ and:}
	\item $\vec o$: sequence of object detections. Each $o_i$ consists of:
	\begin{compactitem}
		\item bounding box $\vec b$
		\item segmentation mask $\vec m$
		\item class label $\ell_i \in \mathcal L$
	\end{compactitem}
	
	\item $\vec q$: query sequence, where each $q_i$ is either a word in $\mathcal V$ or a tag referring to an object in $\vec o$.
	
	\item Set of $N=4$ responses $\ivec[i]{r}$.
\end{itemdefinition}


\myfig[\textwidth]{figs/vcr_r2c.png}



\bluesec{Recognition to Cognition Networks} (5). Authors propose their three-stage R2C model for training on VCR.
\begin{compactenum}
	\item \textbf{Grounding}:
	\begin{compactenum}
		\item CNN learns object-level features (embeds each of the objects $o_i$ in $\vec o$). BERT is used for getting contextualized word embeddings.
		\item Bidirectional LSTM is feed the question and current response separately (but shared weights). It seems like the CNN outputs are only used when feeding in a question/response token referring/pointing to an object in $\vec o$? Very unclear/poorly written.
	\end{compactenum}

	\item \textbf{Contextualization}: Given LSTM output sequences for response and query be $\vec r$ and $\vec q$, respectively. They compute an attended query representation $\vec[i]{\hat q}$ and attended object representation $\vec[i]{\hat o}$:
	\graybox{
		\vec[i]{\hat q} 
			= \sum_j \alpha_{ij} \vec[j]{q}
			&\quad\text{with}\quad
			\alpha_{ij} 
				= \text{softmax}_j \lr{  \vec[i]{r} \matr W  \vec[j]{q}  } \\
		\vec[i]{\hat o} 
			= \sum_j \beta_{ij} \vec[j]{o}
			&\quad\text{with}\quad
			\beta_{ij} 
			= \text{softmax}_j \lr{  \vec[i]{r} \matr W  \vec[j]{o}  }
	}
	with interpretation that $\vec[i]{\hat q}$ is the contextualized version of response token $\vec[i]{r}$ attended over all \textit{query} tokens. Similarly, $\vec[i]{\hat o}$ gives an alternative representation of response token $\vec[i]{r}$ attended over  all \textit{object} tokens.
	
	\item \textbf{Reasoning}:
	\begin{compactitem}
		\item  Feed the element-wise concatenated sequence of $[\vec[i]{r}, \vec[i]{\hat q}, \vec[i]{\hat o}]$ to another BiLSTM.
		\item Concatenate the sequence output from this BiLSTM with the original question/answer representations $\vec q$ and $\vec r$.
		\item Max-pool this fat concatenated sequence (\red{along time dim presumably?}).
		\item Feed to MLP which predicts a logit for query-response compatibility. 
	\end{compactitem}
\end{compactenum}


\bluesec{Results} (6). This paper's results are particularly interesting because they highlight both (1) how well humans perform on this dataset, (2) how poorly VQA models perform, and (3) how text-only models like BERT can substantially outperform VQA models in some scenarios!

\myfig{figs/vcr_tab1.png}

However, reading the details in this section suggests (to me) that the comparisons here aren't that fair. Take with a grain of salt\textellipsis







% ======================================================================
\lecture{Vision and Language}{GQA}{April 19, 2020}
% ======================================================================

\citepaper{D. Hudson and C. Manning}{GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering}{Stanford University}{May 2019}

\bluesec{TL;DR}: New dataset for visual reasoning. 
\begin{compactitem}
	\item 113K images and 22M questions. Leverages \purple{Visual Genome} scene graph structures. 
	\item New metrics: consistency, grounding, plausibility.
	\item Method for generating semantically varied questions.
\end{compactitem}
\myfig[\textwidth]{figs/gqa_fig2.png}


\bluesec{The GQA Dataset} (3). Images, questions, and answers are accompanied by matching semantic representations:
\begin{compactitem}
	\item \textbf{Images} are annotated with a dense \green{Scene Graph}. 
	\item \textbf{Questions} are associated with a \green{functional program} which lists the series of reasoning steps needed to be performed to arrive at the answer.
	\item \textbf{Answers} are augmented with textual and visual justifications.
\end{compactitem}
The four-step dataset construction pipeline is defined as follows:
\begin{compactenum}
	\item \green{Scene Graph Normalization} (3.1). Visual Genome Scene Graph annotations\footnote{Covers 113K images from \purple{COCO} and \purple{Flickr}.}. The apply a significant amount of cleaning/normalization/rules to these. 
	
	\item \green{The Question Engine} (3.2). Generation harnesses (1) the normalized scene graphs and (2) structural patterns. Looks like sophisticated template/slot filling.
	
	\item \green{Functional Representation and Entailment} (3.3). Questions can be represented by programs composed of atomic operations such as object selection, traversal along a relation edge, or an attribute verification. These are chained together to create challenging reasoning questions. A functional program can also \green{entail} another program\footnote{For example, [knowing the answer to] \textit{What color is the apple?} can determine entailment for \textit{Is the apple red?}} . 
	
	\begin{myquote}[-0.3em]
		We define direct entailment relations between the various functional programs and use these to recursively compute all the questions that can be entailed from a given source.
	\end{myquote}
	
	\item \green{Sampling and Balancing} (3.4). Use the functional programs attached to each question to smooth out the answer distribution.
	\begin{compactenum}
		\item Derive global and local labels for each question (using its functional program). 
		
		\item Using the labels, partition the questions into \textit{groups}. 
		
		\item For each group, compute its \green{answer distribution} $P$. 
		
		\item Use rejection-sampling to fit a smoother distribution $Q$.
		\begin{myquote}[-0.2em]
			We iterate over the answers of the group in decreasing frequency order, and reweight P's head up to the current iteration to make it more comparable to the tail size. While repeating this operation as we go through the answers, iteratively ``moving'' probability from the head into the tail, we also maintain minimum and maximum ratios between each pair of subsequent answers (sorted by frequency).
		\end{myquote}
	\end{compactenum}
\end{compactenum}

\myfig[\textwidth]{figs/gqa_fig5.png}


\bluesec{Analysis and Baseline Experiments} (4). Each question has two types:
\begin{compactitem}
	\item \green{Structural}: derived from final operation in question's functional program. 
	\begin{compactitem}
		\item \textbf{Verify}: yes/no questions.
		\item \textbf{Query}: open questions.
		\item \textbf{Choose}: questions presenting two alternative choices.
		\item \textbf{Logical}: involve logical inference. 
		\item \textbf{Compare}: comparisons between two or more objects.
	\end{compactitem}

	\item \green{Semantic}: refers to main subject of question. 
	\begin{compactitem}
		\item \textbf{Object}: existence questions.
		\item \textbf{Attribute}: consider properties/position of object.
		\item \textbf{Category}: object identification within some class.
		\item \textbf{Relation}: questions asking about subject/object of a described relation.
		\item \textbf{Global}: overall properties of the scene (e.g. weather, place).
	\end{compactitem}
\end{compactitem}

\Needspace{10\baselineskip}
The authors also introduce the following metrics (4.4):
\begin{compactitem}
	\item \green{Consistency}: measures how consistent a model's answers are wrt each other. For each question-answer pair (from dataset), define $E_q \triangleq  \seq[n]{q}$ as the set of \textit{entailed questions}\footnote{Entailed questions are those for which the answer can be unambiguously inferred given (q, a).}. Then consistency can be computed as:
	\begin{align}
		\inv{|Q|} \sum_{q \in Q} \text{Acc} \lr{ E_q}
	\end{align}
	where $Q$ is the set of questions for which the model's answer $\hat a$ was the correct answer $a$. 
	
	\item \green{Validity}: fraction of answers that were at least within the question's \textit{scope}. For example, responding some color to a color question. 
	
	\item \green{Plausibility}: measures whether the answer occurs at least once in relation with the question's subject, across the whole dataset. For example, this would make ``purple'' and implausible (but valid) response for an apple's color.
	
	\item \green{Distribution}: \red{TODO}
\end{compactitem}








% ======================================================================
\lecture{Vision and Language}{A negative case analysis of visual grounding methods for VQA}{April 20, 2020}
% ======================================================================

\citepaper{R. Shrestha, K. Kafle, and C. Kanan}{A negative case analysis of visual grounding methods for VQA}{Rochester Institute of Technology}{April 2020}

\bluesec{Existing VQA Methods} (3). 
\begin{align}
	\mtgreen{[task]}& \qquad
		P(a \mid \mathcal Q, \mathcal I) = f_{VQA}(v, \mathcal Q) \\
	\mtgreen{[sensitivity]}& \qquad
		S(a, v_i) \triangleq  \lr{  \nabla_{v_i} P(a \mid \mathcal I, \mathcal Q)   }^T \vec 1
\end{align}
Existing approaches develop training objectives using $S$ in order to improve visual grounding\footnote{The two works being investigated are HINT and SCR.}. Here, the authors investigate: \textit{is [the improved performance] actually due to better visual grounding?}



\bluesec{Why Did the Performance Improve?} (4). Main experiments/findings:
\begin{compactitem}
	\item Training on \textit{irrelevant} visual regions yields similar results as relevant regions\footnote{Relevant regions are roughly determined as those having high human-based importance scores $S_h$. Here, the authors define irrelevant as essentially the opposite: $1-S_h$.}
	
	\item Training \textit{random} visual cues also performs similarly, even if the random cues are re-randomly sampled each epoch!
	
	\item Use Welch's t-test to confirm that differences in performance of each method is statistically insignificant.
	
	\item Roughly 90\%\textit{} overlap of predictions made on test set, confirming that all approaches learn similar things.
	
	\item HINT and SCR help forget linguistic priors, which is beneficial for VQA-CPv2 but not for VQAv2.
	
	\item Although HINT/SCR improve visual grounding, only $\approx 25$\% of their correct answers were most sensitive to a region in the top-3 most relevant ground truth regions.
\end{compactitem}


\bluesec{Embarrassingly Simple Regularizer} (5). They show that simply degrading training performace by introducing a loss that has only zero-vector labels achieves near SOTA on test sets\footnote{Ok this is actually hilarious.}.\marginnote{Authors set $\lambda\eq 1$}[2em]
\begin{align}
	L &:= \text{BCE}\lr{ P\lr{\mathcal A}, \mathcal{A}_{gt} } + \lambda \text{BCE}\lr{  P\lr{\mathcal A}, \vec 0 }
\end{align}






% ======================================================================
\lecture{Vision and Language}{Bottom-Up and Top-Down Attention}{April 29, 2020}
% ======================================================================

\citepaper{Anderson et al.}{Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering}{Australian National University, JD AI, Microsoft}{March 2018}

\bluesec{Jargon Rant}. ``Top-down'' refers to pixel-level attention, or equally-sized/uniform-grid over image. ``Bottom-up'' refers attention over object features. I hate this terminology/nomenclature with a passion. 

\bluesec{Bottom-Up Attention Model} (3.1). Use Faster R-CNN to obtain bounding boxes with object labels. The $k$ input image features, $V = \vecseq[k]{v}$ $, \vec[i]{v} \in \R^D$, are obtained for each of the $k$ regions output by R-CNN, then mean-pooling over the convolutional feature for the region.  

\bluesec{Captioning Model} (3.2).  Consists of two LSTM layers:
\begin{compactenum}
	\item Top-down visual attention model. 
	\begin{compactenum}
		\item Compute mean-pooled image feature $\vec{\bar v} = \inv{k} \sum_i \vec[i]{v}$. 
		\item Lookup word embedding for input word\footnote{The most recently generated word from the 2nd LSTM (LM).} at timestep t and denote as $\vec[t]{\widetilde x}^1$. 
		\item Feed to LSTM:
		\begin{align}
			\vec[t]{x}^1 &= \left[  \vec[t-1]{h}^2,   \vec{\bar v},   \vec[t]{\widetilde x}^1   \right]
		\end{align}
		where $\vec[t-1]{h}^2$ is the output of the second LSTM (below) at previous timestep.
		\item Compute normalized attention weights $\alpha_{i,t}$ for each of the $k$ image features $\vec[i]{v}$ $(1 \le i \le k)$ \marginnote{$\matr[va]{W} \in \R^{H \times V}$}[2em]
		\graybox{
				a_{i,t}
					&= \vec[a]{w}^T \tanh\lr{ \matr[va]{W} \vec[i]{v} +  \matr[ha]{W} \vec[t]{h}^1  } \\
				\vec[t]{\alpha}
					&= \text{softmax}(\vec[t]{a}) \\
				\vec[t]{\hat v}
					&= \sum_{i=1}^{K} \alpha_{i,t} \vec[i]{v}
		}
		where $\vec[a]{w}$ is learned\footnote{Basically like an importance vector.}.
	\end{compactenum}
	
	\item Language model. 
	\begin{compactenum}
		\item Input is the attention vector and output from LSTM 1: $\vec[t]{x}^2 = \left[   \vec[t]{\hat v}, \vec[t]{h}^1 \right]$
		\item As usal, predictive next-token distribution is defined as $p(y_t \mid y_{1:t-1}) = \text{softmax}\lr{  W_p \vec[t]{h}^2 + \vec[p]{b}  }$
	\end{compactenum}
\end{compactenum}

The image captioning model architecture is illustrated below. 
\myfig[0.4\textwidth]{figs/BuTd_fig3.png}

Note that the bottom-up attention model is described later, and here its outputs are simply considered as features V.

\Needspace{10\baselineskip}
\bluesec{VQA Model} (3.3). 


\begin{compactenum}
	\item Encode question as hidden state $\vec q$ of a GRU. 
	
	\item Generate unnormalized attention weights over $k$ image features $\vec[i]{v}$:
	\begin{align}
		a_i &= \vec[a]{w}^T f_a\lr{  \left[  \vec[i]{v}, \vec q \right] }
	\end{align}
	and normalize to obtain $\vec{\hat v}$ as defined earlier.
\end{compactenum}

\myfig[\textwidth]{figs/BuTd_fig4.png}





% ======================================================================
\lecture{Vision and Language}{ViLBERT}{May 04, 2020}
% ======================================================================

\citepaper{J. Lu, D. Batra, D. Parikh, and S. Lee}{ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks}{GA Tech, FAIR, Oregon State}{August 2019}

\href{https://vilbert.cloudcv.org}{[Online Demo]}


\bluesec{TL;DR}: A model for learning task-agnostic joint representations of image context and natural language. Extends BERT with multi-modal two-stream attention. Pretrained on Conceptual Captions. Evaluated on VQA, VCR, referring expressions, caption-based image retrieval. 
\myfig[0.7\textwidth]{figs/vilbert_fig3.png}


\bluesec{ViLBERT} (2.2). The ViLBERT architecture consists of two parallel BERT-style models. \red{TODO: need to understand in exact detail things like how the image regions are ``embedded'', how the two-stream attn is impl, ,etc}
\begin{compactitem}
	\item \textbf{Inputs}: a set of region features $\seq[\mathcal{T}]{v}$ from image $I$ and text input $\seq[T]{w}$.
	\item \textbf{Outputs}: representations $\seq[v_{\mathcal T}][v_1]{h}$ and $\seq[v_{\mathcal T}][v_1]{w}$. 
\end{compactitem}
\myfig[0.9\textwidth]{figs/vilbert_fig1.png}

where Co-TRM is a modified attention block to obtain multi-modal attention:

\myfig[0.6\textwidth]{figs/vilbert_fig2.png}


\bluesec{Vision-and-Language Transfer Tasks} (3.2). How to adapt the base ViLBERT architecture for various downstream tasks.
\begin{compactitem}
	\item \textbf{VQA}. Learn 2-layer MLP on top of the element-wise product of image and text representations $h_{IMG}$ and $h_{CLS}$, mapping to 3,129 possible answers.
	
	\item \textbf{VCR}. Concatenate question and each possible response to form four different text inputs and pass each through ViLBERT along with the image. We learn a linear layer on top of the post-elementwise product representation to predict a score for each pair. The final prediction is a softmax over these four scores.
	
	\item \textbf{RefCOCO+}. Pass the final representation $h_{v_i}$ for each image region $i$ into a learned linear layer to predict a matching score. Label each proposal box by computing the IoU with the ground truth box and thresholding at 0.5. Train with binary cross-entropy.
	
	\item \textbf{Caption-Based Image Retrieval}. Train in a 4-way multiple-choice setting by randomly sampling three distractors for each image-caption pair -- substituting a random caption, a random image, or a hard negative from among the 100 nearest neighbors of the target image. Compute the alignment score (as in alignment prediction pretraining) for each and apply a softmax. Train with cross-entropy loss to select the true image-caption pair.
\end{compactitem}




% ----------------------------------
\myspace
\subsub{12-in-1: Multi-Task Vision and Language Representation Learning}
\myspace
% ----------------------------------

\citepaper{Lu et al.}{12-in-1: Multi-Task Vision and Language Representation Learning}{GA Tech, FAIR, Oregon State}{April 2020}

\bluesec{TL;DR}: Follow-up paper by authors of ViLBERT. Whereas before they seemingly finetune all parameters of the model for each task separately, they now present a multi-task training setup that allows a \textit{single} ViLBERT model to achieve SOTA on 12 tasks. 










% ======================================================================
\lecture{Vision and Language}{Language-Conditioned Graph Networks for Relational Reasoning}{May 05, 2020}
% ======================================================================

\citepaper{Hu et al.}{Language-Conditioned Graph Networks for Relational Reasoning}{UC Berkeley, Boston University}{August 2019}


\bluesec{Context-Aware Scene Representation} (3.1). Let $N$ denote number of entities in image $I$, and let $x_i^{loc}$ ($1 \le i \le N$) denote the feature representation of entity $i$. 
\begin{compactitem}
	\item \textbf{Goal}: output a context-aware representation $x_i^{out}$ for each entity $i$ conditioned on the textual input $Q$. 
	\item \textbf{Approach}: iterative message passing over $T$ iterations on fully-connected graph of entity nodes. In addition to [the fixed] $x_i^{loc}$, each node has a \textit{context feature} $x_{i,t}^{ctx}$ that is updated during each iteration $t$. 
\end{compactitem}

\purple{Textual command extraction}. Let $Q$ denote question containing $S$ words. At each iteration $t$, the \green{textual command} $\vec[t]{c}$ is obtained as follows:
\graybox{
	\vecseq[S]{h}
		&= \text{BiLSTM}(Q) \\
	\vec q 
		&= [\vec[1]{h}; \vec[S]{h}] \\
	\vec[t]{c}
		&= \sum_{s=1}^S \alpha_{t,s} \vec[s]{h} \qquad \text{where} \\
	\alpha_{t,s}
		&= \text{Softmax}_s\lr{    \matr[1]{W} \lr{     \vec[s]{h} \odot \lr{  \matr[2]{W}^{(t)} \text{ReLU}\lr{\matr[3]{W}  \vec q  } } }  }
}

\purple{Language-Conditioned Message Passing}. 
\begin{compactenum}
	\item Compute ``joint representation'' for each node:
	\begin{align}
		\widetilde{x}_{i,t}
			&= \left[     x_i^{loc}; ~ x_{i,t-1}^{ctx}; ~ \matr[4]{W} x_i^{loc} \odot \matr[5]{W} x_{i,t-1}^{ctx} \right]
	\end{align}
	
	\item Compute directed connection weights from node $j$ (sender) to node $i$ (receiver). 
	\begin{align}
			w_{j\rightarrow i}^{(t)}
				&= \text{Softmax}_j \lr{  
						\lr{ \matr[6]{W} \widetilde{x}_{i,t}   }^T \lr{   \matr[7]{W}\widetilde{x}_{j,t} \odot   \matr[8]{W} \vec[t]{c}  }
				}
	\end{align}
	
	\item Each node sends message to each other node. 
	\begin{align}
		m_{j \rightarrow i}^{(t)}
			&= w_{j\rightarrow i}^{(t)} \cdot \lr{  \matr[9]{W}\widetilde{x}_{j,t} \odot   \matr[10]{W} \vec[t]{c}    } \\
		x_{i,t}^{ctx}
			&= \matr[11]{W} \left[     x_{i,t-1}^{ctx}; ~ \sum_{j=1}^{N}   m_{j \rightarrow i}^{(t)}   \right]
	\end{align}

	\item Final representation after $T$ iterations:
	\graybox{
		x_i^{out} &= \matr[12]{W} \left[    x_i^{loc}; x_{i, T}^{ctx} \right]
	}
\end{compactenum}


\bluesec{Application to VQA and REF} (3.2). They define a \green{single-hop answer classifier} for VQA as follows:
\begin{align}
	\beta_i 
		&= \text{Softmax}_i\lr{ \matr[13]{W} \lr{ x_i^{out} \odot \matr[14]{W} \vec q }   } \\
	y 
		&=  \matr[15]{W} \text{ReLU} \lr{ \matr[16]{W} \left[      \sum_{i=1}^{N} \beta_i x_i^{out}  ; ~ \vec q \right] }
\end{align}
where $y$ is the scores (logits) over all possible answers.






% ======================================================================
\lecture{Vision and Language}{Towards VQA Models That Can Read}{May 05, 2020}
% ======================================================================

\citepaper{Singh et al.}{Towards VQA Models That Can Read}{FAIR, GA Tech}{May 2019}

\bluesec{TL;DR}: New \green{TextVQA} dataset that requires reasoning about images containing text. Introduce \green{LoRRA} model that can better  handle OCR in the setting of VQA. 

\myfig[0.9\textwidth]{figs/lorra_model.png}

Denote the $M$ words extracted from the OCR model over the image as $s = \seq[M]{s}$. 
\graybox{
	f_{VQA}(v, q)
		&= f_{comb}^{(1)}\lr{
			f_A^{(1)}\lr{f_I\lr{v}, f_{Q}\lr{q}  }, 
			f_{Q}\lr{q}
		} \\
	f_{OCR}(s, q)
		&= f_{comb}^{(2)}\lr{
			f_A^{(2)}\lr{f_O\lr{s}, f_{Q}\lr{q}  }, 
			f_{Q}\lr{q} 
		} \\
	f_{LoRRA}(v, s, q)
		&= f_{MLP}\lr{
			[ f_{VQA}(v, q); ~ f_{OCR}(s, q) ]
	}
}







% ======================================================================
\lecture{Vision and Language}{Accuracy vs. Complexity: A Trade-off in VQA Models}{May 11, 2020}
% ======================================================================

\citepaper{Farazi et al.}{Accuracy vs. Complexity: A Trade-off in Visual Question Answering Models}{ANU, CSIRO, IIAI}{Jan 2020}


\bluesec{Feature Extraction Meta-Architecture} (2.1). Consists of both a \textit{visual} and \textit{language} feature extraction. 
\begin{compactitem}
	\item \textbf{Visual Feature}.\marginnote{$\vec[i]{v} \in \R^{d_v}$}[2em]
	\begin{align}
		\vec v = \text{CNN}(\matr I) = \{  \vec[i]{v} \mid i \in [1, G] \}
	\end{align}
	 where $G$ denote the total number of image locations in a grid. The extracted features can be categorized into three main types:
	 \begin{compactenum}
	 	\item \green{Image Level (IL)}: represent features of the whole image ($G \eq 1$). 
	 	\item \green{Spatial Grid (SG)}: spatial grid where each of the $G$ feature maps corresponds to a uniform grid location on the input image.
	 	\item \green{Bottom-Up (BU)}: each feature map corresponds to an object proposal region ($G \eq N$).
	 \end{compactenum}
 
 	\item \textbf{Language Feature}: question is tokenized into words and fed through and LSTM or transformer to obtain contextualized representations of each word. These are aggregated in some way to obtain question vector $\vec q$. 
\end{compactitem} 


\bluesec{Fusion Model Meta-Architecture} (2.2). Jointly embed into a common space, learning multimodel embedding function $\Phi$:
\begin{align}
	\vec z &= \Phi (\vec q, \vec v)
\end{align}
In the most general case, we can compute $\vec z = \mathcal W [\vec v \otimes \vec q]$, but this is parameter overkill. The authors outline a variety of instances:
\begin{compactitem}
	\item \textbf{Linear}: $\vec z = \text{FC}\lr{  \text{FC}(\vec v) + \text{FC}(\vec q) }$
	\item \textbf{C-MLP}: $\vec z = \text{MLP}\lr{ [\vec v; \vec q] }$
\end{compactitem}

\bluesec{Attention-Based Meta-Architecture} (2.3). Question-specific attention over image regions (if SG) or object proposals (if BU).


\bluesec{Experiments and Results} (5). Main insights:
\begin{compactitem}
	\item Set visual feature dimension closer to language feature embedding [by modifying the language dim size]. 
	\item PolyNet with smaller grid sizes performs surprisingly well. 
	\item Resnet152 has largest performance boost when going from IL to SG. 
	\item SeNet154 perfroms better on datasets with less language bias (e.g. VQA-CP instead of VQAv2). 
	\item Using Bottom-Up features provides a consistent accuracy gain. 
	\item VQA models are less sensitive to change in batch size.
\end{compactitem}

\myfig[0.6\textwidth]{figs/acc_comp_fig6.png}







% ======================================================================
\lecture{Vision and Language}{Iterative Answer Prediction with Multimodal Transformers for TextVQA}{May 13, 2020}
% ======================================================================

\citepaper{R. Hu, A. Singh, T. Darrell, and M. Rohrbach}{Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA}{FAIR, UC Berkeley}{Mar 2020}


\bluesec{Introduction} (1). Current approaches on the TextVQA task...
\begin{compactitem}
	\item ``rely on custom pairwise multimodal fusion mechanisms between two modalities, which limit[s] the types of possible interactions between modalities'' (\red{how is this limiting?})
	
	\item Treat VQA as multi-label classification, which obviously limits the output space of the model.
	
	\item Make poor use of OCR by just concatenating detected token embeddings together (ignoring spacial location, color, etc.).
\end{compactitem}

\myfig[0.5\textwidth]{figs/m4c_fig1.png}


\bluesec{Common Embedding Space} (3.1).  

\begin{compactitem}
	\item \textbf{Question words}. Sequence of $K$ question words are fed through pretrained BERT.\footnote{During training, the BERT parameters are fine-tuned using the question answering loss.}
	
	\item \textbf{Detected objects}. Obtain set of $M$ visual objects via \purple{Faster R-CNN}\footnote{They fine-tune the last layer of Faster-RCNN during training.}.
	\begin{align}
		x_m^{obj}
			&= LN \lr{ \matr[1]{W} x_m^{fr} } + LN\lr{ \matr[2]{W} x_m^b }
	\end{align}
	
	\item \textbf{Embedding of OCR tokens}. Obtain a set of $N$ OCR tokens (words) through external OCR system. For each token $n$:
	\begin{compactenum}
		\item FastText vector $x_n^{ft}$
		\item Appearance feature $x_n^{fr}$ from same Faster R-CNN detector.
		\item PHOC vector $x_n^{p}$. 
		\item 4-dimensional location feature $x_n^b$ based on OCR token's relative bounding box coordinates.
	\end{compactenum}
	\begin{align}
		x_n^{ocr}
			&= LN\lr{ \matr[3]{W} x_n^{ft} + \matr[4]{W} x_n^{fr} + \matr[5]{W} x_n^{p} } + LN\lr{  \matr[6]{W} x_n^{b} }
	\end{align}
\end{compactitem}


\bluesec{Multimodel fusion and iterative answer prediction with pointer-augmented transformers} (3.2). Apply $L$ transformer layers with hidden dim $d$ over the list of all $K + M + N$ entities from $\{x_k^{ques}\}$, $\{x_m^{obj}\}$, and $\{x_n^{ocr}\}$ (prev section). 

\myfig[\textwidth]{figs/m4c_fig2.png}

Let $\{z_1^{ocr}, \ldots z_{N}^{ocr}\}$ be the d-dimensional transformer outputs of the $N$ OCR tokens in the image. As show in the illustration above, they initialize decoding by feeding in special \texttt{<begin>} as $x_{1}^{dec}$, with corresponding output $z_{1}^{dec}$, then decode until $z_t^{dec}$ corresponds to \texttt{<end>}. 
\graybox{
	y_{t,i}^{voc}
		&= \lr{w_i^{voc}}^T z_t^{dec} + b_i^{voc} \\
	y_{t,n}^{ocr}
		&= \lr{  \matr{W}^{ocr}z_n^{ocr} + b^{ocr} }^T \lr{  \matr{W}^{dec} z_t^{dec} + b^{dec}   }
}
They predict by taking an argmax of $y_t^{all} = [y_t^{voc}; y_t^{ocr}]$. Train via ``multi-label sigmoid loss (instead of softmax loss) over scores $y_t^{all}$.''

\Needspace{15\baselineskip}
\bluesec{Q \& A}
\begin{compactitem}
	\QA{What happens if an image without scene text is fed as input?}{From the code at least, it would just become a vector of zeros (padding).}
	
	\QA{What is PHOC?}{Pyramidal Histogram of Characters. It's from the paper ``Word Spotting and Recognition with Embedded Attributes.'' From the paper: \textit{The proposed approach embeds text strings into a d-dimensional binary space. In a nutshell, this embedding -- which we dubbed pyramidal histogram of characters or PHOC -- encodes if a particular character appears in a particular spatial region of the string}}
	
	\QA{What is the backbone image model being used?}{Faster R-CNN pretrained on Visual Genome. Based on the MMF code, it is \purple{detectron}.}
	
	\QA{What is the detectron model exactly?}{}
	
	\QA{What is the training dataset?}{Best guess is textVQA because their example snippet \href{https://github.com/ronghanghu/pythia/tree/project/m4c_captioner_pre_release/projects/M4C}{here} uses it.}
	
	\QA{The phrase ``single-hop'' attention seems to just literally mean ``attention'' -- what would be an example of ``multi-hop'' attention?}{}
\end{compactitem}









% ======================================================================
\lecture{Vision and Language}{CVPR 2020}{June 21, 2020}
% ======================================================================

Quick summaries of various papers presented at CVPR 2020. 

\myspace
\subsub{Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations}
\myspace
\citepaper{Wu et al.}{Unified Visual-Semantic Embeddings: Bridging Vision and Language with Structured Meaning Representations}{Fudan University}{2020}

\begin{myquote}
	Given an image-caption pair, we first parse the caption into a structured meaning representation, composed by a set of semantic components: object nouns, prenominal modifiers, and relational dependencies. We encode different types of semantic components with type-specific encoders. A caption encoder combines the embedding of the semantic components into a \green{caption semantic embedding}. Jointly, we encode images with a convolutional neural network (CNN) into the same, unified VSE space. The distance between the image embedding and the sentential embedding measures the semantic similarity between the image and the caption. We employ a multi-task learning approach for the joint learning of embeddings for semantic components (as the “basis” of the VSE space) as well as the caption encoder (as the combiner of semantic components). 
\end{myquote}

\myfig[0.8\textwidth]{figs/uvse_fig3.png}

They align modalities vai a bidirectional margin-based ranking loss. They employ a sematnic parser on the image captions.\textit{}


\myspace
\subsub{Counterfactual Samples Synthesizing for Robust Visual Question Answering}
\myspace

\citepaper{Chen et al.}{Counterfactual Samples Synthesizing for Robust Visual Question Answering}{Zhejiang University}{Mar 2020}

\begin{myquote}
	We propose a model-agnostic Counterfactual Samples Synthe- sizing (CSS) training scheme. The CSS generates numerous counterfactual training samples by masking critical objects in images or words in questions, and assigning different ground-truth answers. After training with the complementary samples (i.e., the original and generated samples), the VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities.
\end{myquote}


\bluesec{V-CSS} (4.2.1). Algorithm below consists of four main steps:
\begin{compactenum}
	\item Initial objects selection (\textsc{IO\_Sel}).
	\item Object local contributions calculation.
	\item Critical objects selection (\textsc{CO\_Sel}).
	\item Dynamic answer assigning (\textsc{DS\_Ass}).
\end{compactenum}

\myfig[0.6\textwidth]{figs/css_alg2.png}





% ======================================================================
\lecture{Vision and Language}{UniVL}{Feb 25, 2021}
% ======================================================================

\citepaper{Luo et al.}{UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation}{Microsoft Research}{Sep 2020}

\bluesec{Introduction} (1). Authors emphasize they want to train on \textit{both} understanding \textit{and} generation tasks, unlike ActBERT/HERO which only pretrain on understanding tasks. 


\bluesec{Model Architecture} (3.1). 
\begin{compactenum}
	\item \textbf{Preprocessing}. 
	\begin{compactitem}
		\item Text: tokenized using WordPiece into $n$ tokens. 
		\item Video: frames are sampled into $m$ groups. \red{TODO}: wording/notation is unclear on whether there are $m$ \textit{frames} or $m$ \textit{groups of frames}.
	\end{compactitem}
	
	\item \textbf{Single Modal Encoders}. 
	\begin{compactitem}
		\item Text: BERT-Base uncased generates text representation $\mgreen{\matr T \in \R^{n \times d}}$. 
		\item Video: S3D generates video features $\matr[v]{F} \in \R^{m \times d_v^f}$, which are then fed through a transformer encoder to obtain $\mpurple{\matr V \in \R^{m \times d}}$.\footnote{I'm assuming there is a projection from $d_v^f$ to $d$ in between?}
	\end{compactitem}

	
	\item \textbf{Cross Encoder}. $T$ and $V$ are concatenated along temporal axis ($n + m$) and fed through a transformer encoder to obtain $\mred{ \matr M \in \R^{(n + m) \times d}}$. 
	
	\item \textbf{Decoder}. Transformer decoder that is provided encoder outputs $\matr M$ to generate $\matr D \in \R^{l \times d}$. 
\end{compactenum}

\bluesec{Pre-training Objectives} (3.2).
\begin{compactitem}
	\item \textbf{Video-Text Joint}. Let's say for a given video $\vec v$, we also have the text narration (transcript) throughout. The narration is grouped temporally into chunks $\{\vec{t} \}$, wherein e.g. the person pausing for a couple seconds would indicate the end of a group. For a given chunk $\vec t$, denote $\vec[-]{t}$ and $\vec[+]{t}$ as the text transcripts immediately before and after, respectively. We define the \green{positive set} $\mathcal{P}_{\vec v, \vec t}$ as $ \{  (\vec v, \vec t), (\vec v, \vec[-]{t}), (\vec v, \vec[+]{t})  \}  $. 
	
	\begin{align}
		\mathcal{L}_{joint}(\theta) 
			&= -\E[(\vec t, \vec v) \sim \matr B]{ \log \mtext{MIL-NCE}\lr{\vec t, \vec v}  } \\
		 \mtext{MIL-NCE}(\vec t, \vec v)
		 	&= \inv{Z} \sum_{  (\hat{\vec v}, \hat{\vec t} ) \sim \mathcal{P} } \exp\lr{ \hat{\vec v}  \hat{\vec t}^T } \\
		 Z 
		 	&= \sum_{  (\hat{\vec v}, \hat{\vec t} ) \sim \mathcal{P} } \exp\lr{ \hat{\vec v}  \hat{\vec t}^T }
		 	 + \sum_{  (\widetilde{\vec v}, \widetilde{\vec t} ) \sim \mathcal{N} } \exp\lr{ \widetilde{\vec v}  \widetilde{\vec t}^T }
	\end{align}
	
	\item \textbf{CMLM}. Conditioned Masked Language Model. Just the BERT objective wherein 15\% of text tokens are masked. 
	\begin{align}
		\mathcal{L}_{CMLM}(\theta)
			&= -\E[t_{m} \sim \vec t]{\log P_{\theta}\lr{ t_m \mid t_{\neg m}, \vec v }}
	\end{align}
	where $t_{\neg m}$ means the contextual tokens surrounding the masked token $t_m$.
	
	\item \textbf{CMFM}: Conditioned Masked Frame Model. Adopt contrastive learning method to maximize the \green{mutual information} (MI) between the masked output features and the original features (AKA the NCE loss). 
	\begin{align}
		\mathcal{L}_{CMFM}(\theta)
			&= -\E[v_m \sim \vec v]{\log \text{NCE}\lr{v_m \mid v_{\neg m}, \vec t}}
	\end{align}
	
	\item \textbf{Video-Text Alignment}: \red{TODO}
	
	\item \textbf{Language Reconstruction}: \red{TODO}
\end{compactitem}

\myfig[0.7\textwidth]{figs/univl_fig3.png}

\bluesec{Pre-training Strategies} (3.3). 
\begin{compactitem}
	\item \textbf{StagedP}. Alternate between freezing BERT and training the video Transformer only (stage 1) and pre-training on all five objectives (stage 2). 
	
	\item \textbf{EnhancedV}. Mask all the text with 15\% probability. Forces the model to utilize the video information. 
	
\end{compactitem}


% ======================================================================
\lecture{Vision and Language}{R3Transformer}{Feb 25, 2021}
% ======================================================================

\citepaper{Akbari et al.}{Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language}{Columbia University \& Microsoft Research}{Nov 2020}

\myfig[0.6\textwidth]{figs/r3transformer_fig1.png}







% ======================================================================
\lecture{Vision and Language}{CLIP}{Mar 01, 2021}
% ======================================================================

\citepaper{Radford et al.}{Learning Transferable Visual Models From Natural Language Supervision}{OpenAI}{Jan 2021}

\myfig[0.7\textwidth]{figs/clip_fig1.png}

\bluesec{Data} (2.2). During data collection, they search for (image, text) pairs whose text includes one of a set of 500,000 queries. The base query list is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. They approximately class-balance the results by including up to 20,000 (image, text) pairs per query.\marginnote{Final dataset contains 400M (image, text) pairs.}[-2em]
	
\bluesec{Efficient Pretraining} (2.3). They found that a 63 million parameter transformer language model learns to recognize ImageNet classes \textit{three times
slower} than a much simpler baseline that predicts a bag-of-words encoding of the same text\footnote{Isn't this in contradiction with recent findings that models that learn quickly also generalize better? TODO: find the paper that showed that.}. Their final approach (CLIP) thus does the following:
\begin{myquote}
	Given a batch of $N$ (image, text) pairs, CLIP is trained to predict which of the $N \times N$ possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch while minimizing the cosine similarity of the embeddings of the $N^2 - N$ incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores.
\end{myquote}

\bluesec{Choosing and Scaling a Model} (2.4). They consider two different architectures for the image encoder:
\begin{compactitem}
	\item ResNet-50 \footnote{With ResNet-D improvements from He et al. (2019) and the antialiased rect-2 blue pooling from Zhang (2019)}. They replace the global average pooling layer with an attention pooling mechanism.
	
	\item ViT. They add an additional layer normalization to the combined patch and position embeddings before the transformer and use a ``slightly different initialization scheme''.
\end{compactitem}
The text encoder is a GPT2 architecture with 12 layers, 512 state size, 8 heads. They interpret the final layer representation of EOS as the feature representation, which is then layer normalized and linearly projected into the multi-model embedding space. 

Below is a pseudocode implementation of CLIP:

\myfig[0.6\textwidth]{figs/clip_fig3.png}


\bluesec{Further Reading}. 
\begin{compactitem}
	\item Contrastive objectives can learn better representations than their equivalent predictive objective (Tian et al., 2019).
	\item Multi-class N-pair loss (Sohn 2016). 
	\item InfoNCE loss (Oord et al. 2018).
\end{compactitem}

The BoW classifier from Tian et al. optimizes a one-versus-all objective, with the goal of assigning high scores to words associated with some image (each training image can have multiple words associated with it). For convenience, below is a review of one-versus-all classification\footnote{Notes based off James et al., ``An Introduction to Statistical Learning'' Ch 9.2-9.4.}.
\begin{example}[Digression: one-versus-all classification]
	Recall that a \green{support vector classifier} performs binary classification on $p$ input features $x_p$ by learning a parameter $\beta_p$ associated with each feature (and offset $\beta_0$). It optimizes the following max-margin objective (for some margin $M$ and slack variables $\epsilon_i$):
	\begin{align}
		\vec{\beta}, \vec{\epsilon}
		&\leftarrow \argmax_{\vec \beta, \vec \epsilon} M \\
		\text{subject to } \sum_{j=1}^{p} \beta_j^2
		&= 1
		,~ \epsilon_i \geq 0
		,~ \sum_{i=1}^{n} \leq C \\
		y_i \lr{ \beta_0 + \vec{\beta}^T\vec[i]{x} }
		&\geq M(1 - \epsilon_i)
	\end{align}
	At test time, we classify $\vec{x}^*$ as $\text{sign} \lr{ f(x^*) } = \text{sign}\lr{ \beta_0 + \vec{\beta}^T \vec{x}^*}$. One can derive that the solution to this optimization problem can be represented as a linear combination of inner products:
	\begin{align}
		f\vec x) 
		&= \beta_0 \sum_{i=1}^{n} \alpha_i \langle \vec x, \vec[i]{x} \rangle  \\
		&= \beta_0 + \sum_{i \in \mathcal S} \alpha_i \langle \vec x, \vec[i]{x} \rangle
	\end{align}
	where $\mathcal S$ denotes the set of \green{support vectors}: the training observations $\vec[i]{x}$ that are either directly on the margin or on the wrong side of the margin for their class. Finally, the \green{Support Vector Machine} (SVM) is a generalization of this that replaces inner products with the notion of a \purple{kernel} $K(\vec x, \vec[i]{x})$:
	\begin{align}
		f(\vec x) 
		&= \beta_0 + \sum_{i \in \mathcal S} \alpha_i K(\vec x, \vec[i]{x})
	\end{align} 
	
	
	One approach for multi-class classification with SVMs is \green{one-versus-all classification}. Let $K$ denote the number of classes. We fit $K$ separate SVMs, where the sign of the output of the $k$th SVM, $f_k$, should be $+1$ if the input belongs to class $k$, and $-1$ otherwise. At test time, we classify $\vec{x}^*$ as $\argmax_k f_k(x^*)$. 
\end{example}


% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Generative Models}\label{Generative Models}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------



% ============================================================================================
\lecture{Generative Models}{WaveNet}{January 15, 2017}

\myspace
\p \blue{Introduction}. 
\begin{compactitem}
	\item Inspired by recent advances in \green{neural autoregressive generative models}, and based on the PixelCNN architecture.  
	
	\item Long-range dependencies dealt with via ``dilated causal convolutions, which exhibit very large receptive fields.''
\end{compactitem}

\myspace
\p \blue{WaveNet}. The joint probability of a waveform $x = \{x_1, \ldots, x_T\}$ is factorised as a product of conditional probabilities, 
\begin{align}
p(x) = \prod_{t = 1}^{T} p(x_t \mid x_1, \ldots, x_{t - 1})
\end{align}
which are modeled by a stack of convolutional layers (no pooling). Main ingredient of WaveNet is \textit{dilated} causal convolutions, illustrated below. Note the absence of recurrent connections, which makes them faster to train than RNNs, but at the cost of requiring many layers,  or large filters to increase the receptive field\footnote{Loose interpretation of receptive fields here is that large fields can take into account more info (back in time) as opposed to smaller fields, which can be said to be ``short-sighted''}. 

\myfig[0.5\textwidth]{CausalConv.PNG}

\begin{definition}[-1em][Dilated Convolution]
	A dilated convolution (a convolution with holes) is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step. It is equivalent to a convolution with a larger filter derived from the original filter by dilating it with zeros, but is significantly more efficient. A dilated convolution effectively allows the network to operate on
	a coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but
	here the output has the same size as the input. As a special case, dilated convolution with dilation
	1 yields the standard convolution.
\end{definition}

\myspace
\p \blue{Softmax distributions}. To deal with the fact that there are $2^{16}$ possible values, first apply a \green{$\mu$-law companding transformation}\footnote{In telecommunication and signal processing \textbf{companding} (occasionally called compansion) is a method of mitigating the detrimental effects of a channel with limited dynamic range.} to data, and then quantize it to 256 possible values:\marginnote{$-1 < x_t  < 1$ and $\mu = 255$}[2em]
\graybox{
	f(x_t) = \mathrm{sign}(x_t)\frac{\ln(1+\mu|x_t|)}{\ln(1 + \mu)}
}
which (after plotting in Wolfram) looks identical to the sigmoid function.

\myspace
\p \blue{Gated Activation and Residual/Skip Connections}. Use the same gated activation unit as PixelCNN:
\begin{align}
z = \tanh\left( W_{f,k} * x \right) ~ \odot ~ \sigma\left( W_{g,k} * x \right)
\end{align}
where $*$ denotes conv operator, $\odot$ denotes elem-wise mult., $k$ is layer index, $f,g$ denote filter/gate, and $W$ is learnable conv filter. This is illustrated below, along with the residual/skip connections used to speed up convergence/enable training deeper models.

\myfig[0.6\textwidth]{WaveNetRes.PNG}

The 1x1 blocks are 1x1 convolutions (i.e. position-wise dense layers). 

\Needspace{15\baselineskip}
\bluesec{Conditional Wavenets}. Can also model conditional distribution of $x$ given some additional $h$ (e.g. speaker identity).
\begin{align}
p(\vec x \mid \vec h) = \prod_{t = 1}^{T} p(x_t \mid x_1, \ldots, x_{t - 1}, h)
\end{align}
\begin{compactitem}[$\rightarrow$]
	\item \textbf{Global conditioning}. Single $h$ that influences output dist. accross all times. Activation becomes:
	\begin{align}
	z = \tanh\left( W_{f,k} * \vec x +  V_{f,k}^T \vec h\right) 
	~ \odot ~ 
	\sigma\left( W_{g,k} * \vec x + V_{g,k}^T \vec h\right)
	\end{align}
	
	\item \textbf{Local conditioning}. Have a second time-series $h_t$. They first transform this $h_t$ using a \green{transposed CNN (learned upsampling)} that maps it to a new time-series $\vec y = f(\vec h)$ w/same resolution as $\vec x$. 
	\begin{align}
	z = \tanh\left( W_{f,k} * \vec x +  V_{f,k} * \vec y \right) 
	~ \odot ~ 
	\sigma\left( W_{g,k} * \vec x + V_{g,k}^T \vec y\right)
	\end{align}
	
	
\end{compactitem}




\myspace
\p \blue{Experiments}. 
\begin{itemize}
	\item \textbf{Multi-Speaker Speech Generation}. Dataset: multi-speaker corpus of 44 hours of data from 109 different speakers\footnote{Speakers encoded as ID in form of a one-hot vector}. Receptive field of 300 milliseconds.
	
	
	\item \textbf{Text-to-Speech}. Single-speaker datasets of 24.6 hours (English) and 34.8 hours (Chinese) speech. Locally conditioned on \textit{linguistic features}. Receptive field of 240 milliseconds. Outperformed both LSTM-RNN and HMM.
	
	
	\item \textbf{Music}. Trained the WaveNets to model two music datasets: (1) 200 hours of annotated music audio, and (2) 60 hours of solo piano music from youtube. Larger receptive fields sounded more musical.
	
	\item \textbf{Speech Recognition}. ``With WaveNets we have shown that layers of dilated convolutions allow the receptive field to grow longer in a much cheaper way than using LSTM units.''
\end{itemize}

\myspace
\p \blue{Conclusion} (verbatim): ``This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned
on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition.''






% ============================================================================================
\lecture{Generative Models}{Latent Dirichlet Allocation}{July 22, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Blei et al., ``Latent Dirichlet Allocation,'' (2003).}

\myspace
\p \blue{Introduction}. At minimum, one should be familiar with generative probabilistic models, mixture models, and the notion of latent variables before continuing. The ``Dirichlet'' in LDA of course refers to the \green{Dirichlet distribution}, which is a generalization of the beta distribution, $B$. It's PDF is defined as\footnote{
	Recall that for positive integers $n$, $\Gamma(n) = (n - 1)!$.
}\footnote{The Dirichlet distribution is \green{conjugate} to the multinomial distribution. TODO: Review how to interpret this.}:\marginnote{$$\sum_{i=1}^{K} \vec[i]{x} = 1$$  $$(\forall i \in [1,K]):\vec[i]{x} \ge 0$$}[1em]
\begin{align}
\text{Dir}(\vec{x}; \vec{\alpha}) = \frac{1}{B(\vec{\alpha})} \prod_{i=1}^{K} \vec[i]{x}^{\vec[i]{\alpha} - 1} 
\quad \text{where} \quad
B(\vec{\alpha}) = \frac{ \prod_{i=1}^{K} \Gamma\left( \vec[i]{\alpha}\right) }{  \Gamma(\sum_{i=1}^{K} \vec[i]{\alpha}) }
\label{dirichlet}
\end{align}
Main things to remember about LDA:
\begin{compactitem}[-]
	\item Generative probabilistic model for collections of \underline{discrete} data such as text corpora.
	\item Three-level \green{hierarchical Bayesian model}. Each document is a mixture of topics, each topic is an infinite mixture over a set of topic probabilities.
\end{compactitem}
Condensed comparisons/history of related models leading up to LDA:
\begin{compactitem}[-]
	\item \textbf{TF-IDF}. Design matrix $\matr{X} \in \R^{V \times M}$, where $M$ is the number of docs, and $\matr[i,j]{X}$ gives the TF-IDF value for $i$th word in vocabulary and corresp. to document $j$. 
	
	\item \textbf{LSI}:\footnote{Recall that LSI is basically PCA but without subtracting off the means} Performs SVD on the TF-IDF design matrix $\matr{X}$ to identify a linear subspace in the space of tf-idf features that captures most of the variance in the collection.
	
	\item \textbf{pLSI}: \red{TODO}
\end{compactitem}
\vspace{-1em}
\begin{quote}
	{\small \textit{pLSI is incomplete
			in that it provides no probabilistic model at the level of documents. In pLSI, each document is
			represented as a list of numbers (the mixing proportions for topics), and there is no generative
			probabilistic model for these numbers.}}
\end{quote}




\myspace
\p \blue{Model}. LDA assumes the following generative process for each document (word sequence) $\vec{w}$:
\begin{compactenum}
	\item \blue{$N \sim \text{Poisson}(\lambda)$}: Sample $N$, the number of words (length of $\vec{w}$), from $\text{Poisson}(\lambda) = e^{-\lambda} \frac{\lambda^n}{n!}$. The parameter $\lambda$ \textit{should} represent the average number of words per document.
	
	\item \red{$\theta \sim \text{Dir}(\alpha)$}: Sample $k$-dimensional vector $\theta$ from the Dirichlet distribution (eq.~\ref{dirichlet}), $\text{Dir}(\alpha)$. $k$ is the number of topics (pre-defined by us). Recall that this means $\theta$ lies in the (k-1) simplex. The Dirichlet distribution thus tells us the probability density of $\theta$ over this simplex -- it defines the probability of $\theta$ being at a given position on the simplex.
	
	\item Do the following $N$ times to \underline{generate the words} for this document.
	\begin{compactenum}
		\item \green{$z_n \sim \text{Multinomial}(\theta)$}. Sample a topic $z_n$.
		
		\item \purple{$w_n \sim \Prob{w_n \mid z_n, \beta} $}: Sample a word $w_n$ from $\Prob{w_n \mid z_n, \beta}$, a ``multinomial probability conditioned on topic $z_n$.''\footnote{\red{TODO}: interpret meaning of the multinomial distributions here. Seems a bit different than standard interp...} The parameter $\beta$ gives the distribution of words given a topic: 	
		\begin{align}
		\beta_{ij} &= \Prob{w_j \mid z_i} 
		\end{align}
		In other words, we really sample $w_n \sim \beta_{i,:}$
	\end{compactenum}
\end{compactenum}
The defining equations for LDA are thus:
\graybox{
	&\Prob{\theta, \vec{z}, \vec{w} \mid \alpha, \beta}
	= \mred{\Prob{\theta \mid \alpha}} \prod_{n=1}^{N} 
	\mgreen{ \Prob{z_n \mid \theta} } 
	\mpurple{ \Prob{w_n \mid z_n, \beta} } \\
	&\Prob{\vec{w} \mid \alpha, \beta}
	= \int  \mred{\Prob{\theta' \mid \alpha}} \left( \prod_{n=1}^{N}\sum_{z'_n}
	\mgreen{\Prob{z'_n \mid \theta'}}\mpurple{\Prob{w_n \mid z'_n, \beta} }
	\right) \mathrm{d}\theta' \\
	&\Prob{\mathcal{D} = \{ \vec{w}^{(1)}, \ldots, \vec{w}^{(M)} \} \mid \alpha, \beta}
	= \prod_{d=1}^{M} \Prob{\vec{w}^{(d)} \mid \alpha, \beta}
}


\p Below is the plate notation for LDA, followed by an interpretation:

\myfig[0.4\textwidth]{LDA_plate_notation.png}

\begin{compactitem}
	\item \textbf{Outermost Variables}: $\alpha$ and $\beta$. Both represent a (Dirichlet) prior distribution: $\alpha$ parameterizes the probability of a given \textit{topic}, while $\beta$ a given \textit{word}. 
	
	\item \text{Document Plate}. $M$ is the number of documents, $\vec[m]{\theta}$ gives the true distribution of topics for document $m$\footnote{In other words, the meaning of $\vec[m,i]{\theta} = x$ is ``x percent of document $m$ is about topic $i$.''}.
	
	\item \text{Topic/Word Place}. $\vec[mn]{z}$ is the topic for word $n$ in doc $m$, and $\vec[mn]{w}$ is the word. It is shaded gray to indicate it is the only \textbf{observed variable}, while all others are \textbf{latent variables}. 
\end{compactitem}



\myspace
\p \blue{Theory}. I'll quickly summarize and interpret the main theoretical points. Without having read all the details, this won't be of much use (i.e. it is for someone who has read the paper already). 
\begin{compactitem}
	\item \textbf{LDA and Exchangeability}. We assume that each document is a bag of words (order doesn't matter; frequency sitll does) \textit{and} a bag of topics. In other words, a document of $N$ words \textit{is} an unordered list of words and topics. De Finetti's theorem tells us that we can model the joint probability of the words and topics as if a random parameter $\theta$ were drawn from some distribution and then the variables within $\vec{w},\vec{z}$ were \textbf{conditionally independent given $\theta$}. LDA posits that a good distribution to sample $\theta$ from is a Dirichlet distribution.
	
	\item \textbf{Geometric Interpretation}: \red{TODO}
\end{compactitem}

\myspace
\p \blue{Inference and Parameter Estimation}. As usual, we need to find a way to compute the posterior distribution of the hidden variables given a document $\vec{w}$:
\begin{align}
\Prob{\vec{\theta}, \vec{z} \mid \vec{w}, \vec{\alpha}, \vec{\beta}} 
&= \frac{ 	\Prob{\vec{\theta}, \vec{z}, \vec{w} \mid \vec{\alpha}, \vec{\beta}}  }{ \Prob{ \vec{w} \mid \vec{\alpha}, \vec{\beta}}     }
\end{align}
Computing the denominator exactly is intractable. Common approximate inference algorithms for LDA include Laplace approximation, variational approximation, and Markov Chain Monte Carlo.









% ===========================================================================================
\lecture{Generative Models}{Generative Adversarial Nets}{December 26, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Goodfellow et al., ``Generative Adversarial Nets,'' (June 2014)}

\p \blue{TL;DR}. The abstract is actually quite good:
\begin{quote}
	\vspace{-1em}
	\small\itshape
	\textellipsis we simultaneously train two models: a generative model \green{G} that captures the data distribution, and a discriminative model \purple{D} that estimates the probability that a sample came from the training data rather than \green{G}. The training procedure for \green{G} is to maximize the probability of \blue{D} making a mistake. This framework corresponds to a minimax two-player game.
\end{quote}

\myspace
\p \blue{Adversarial Nets} (3). As usual, first go over notation:
\begin{compactitem}
	\item Generator produces data samples\footnote{Note that G outputs samples $\vec x$, not probabilities. By doing this, it \textit{implicitly} defines a probability distribution $p_g(\vec x)$. This is what the authors say.}, $\vec x := \mgreen{G}(\vec z; \theta_g)$, where $\vec z \sim p_n$ (noise distribution prior). 
	
	\item Discriminator, $\mpurple{D}(\vec x; \theta_d)$, outputs probability that $\vec x$ came from (true) $p_{data}$ instead of $G$.  
\end{compactitem}
Our two-player minimax optimization problem can be written as:
\newcommand{\md}{\mpurple{D}}
\newcommand{\mg}{\mgreen{G}}
\graybox{
	\mgreen{\min_G} \mpurple{\max_D} V(\md, \mg) 
	&= \mpurple{ \E[\vec x \sim p_{data}]{\log D(\vec x)} } 
	+ \E[\vec z \sim p_n]{\log \left( 1 - \md ( \mg (\vec z) )  \right) } \label{eq:gans-1}
}

\myspace
\p \blue{Theoretical Results} (4). Below is the training algorithm.
\begin{algorithm}[SGD with GANs]
	Repeat the following for each training iteration.
	
	\begin{compactenum}
		\item Train $\md$. For $k$ steps, repeat:
		\begin{compactenum}
			\item Sample $m$ noise samples $\vecseq[m]{z}$ from noise prior $p_n$. 
			
			\item Sample $m$ data samples $\vecseq[m]{x}$ from data distribution $p_{data}$. 
			
			\item Update discriminator by \textit{ascending} $\nabla_{\theta_d} V(\md, \mg)$.
		\end{compactenum}
		
		\item Train $\mg$: Sample another $m$ noise samples $\vecseq[m]{z}$ and \textit{descend} on $\nabla_{\theta_g} V(\md, \mg)$. 
	\end{compactenum}
\end{algorithm}

\myspace
\p \blue{Global Optimality of $p_g \equiv p_{data}$} (4.1). 

\begin{definition}[-1em][Proposition 1]
	For fixed $\mg$, the optimal $\md$ is 
	\begin{align}
	D_G^*(\vec x) = \frac{ p_{data}(\vec x) }{ p_{data}(\vec x) + p_g(\vec x) } \label{eq:gans-2}
	\end{align}
\end{definition}

\begin{example}[Derivation of $D_G^*(\vec x)$.]
	
	\textbf{Aside: Law of the unconscious statistician} (LotUS). 
	The distribution $p_g(\vec x)$ should be read as ``the probability that the output of $G$ yields the value $\vec x$.'' Take a step back and recognize that $G$ is simply a function of a random variable $\vec z$. As such, we can apply familiar rules like
	\begin{align}
	\E{G(\vec z)}
	&= \E[\vec z \sim p_n]{G(\vec z)} \\
	&= \int_{\vec z} p_n(\vec z) G(\vec z) \mathrm{d}\vec z
	\end{align}
	
	However, recall that functions of random variables can themselves be interpreted as random variables. In other words, we can also use the interpretation that $G$ evaluates to some output $\vec x$ with probability $p_g(\vec x)$. 
	\begin{align}
	\E{G} 
	&= \E[\vec x \sim p_g]{\vec x} \\
	&= \int_{\vec x} p_g(\vec x) \vec x \mathrm{d}\vec x
	\end{align}
	As \href{https://srome.github.io/An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes/}{this blog post} details, this equivalence is NOT due to a change of variables, but rather by the \textbf{Law of the unconscious statistician}. \\
	
	
	\textbf{The Proof}: We can directly use LotUS to rewrite $V(\mg, \md)$:
	\begin{align}
	V(\mg, \md)
	&= \mpurple{ \E[\vec x \sim p_{data}]{\log D(\vec x)} } 
	+ \E[\vec z \sim p_n]{\log \left( 1 - \md ( \mg (\vec z) )  \right) } \\
	&=  \mpurple{ \E[\vec x \sim p_{data}]{\log D(\vec x)} } 
	+ \E[\mgreen{ \vec x \sim p_g } ]{  \log \left(1 -  \md (\mgreen{ \vec x } )   \right) } \\
	&= \int_{\vec x} \left[
	p_{data}(\vec x) \log \left(\md (\vec x)\right) +
	\mgreen{p_g(\vec x)} \log \left(1 - \md(\vec x)\right)
	\right] \mathrm{d} \vec x
	\end{align}
	LotUS allowed us to express $V(\mg, \md)$ as a continuous function over $\vec x$. More importantly, it means we can evaluate $\pderiv{V}{D}$ and take the derivative inside the integral\footnote{Also remember that $D(\cdot) \in [0, 1]$ since it is a probability distribution.}. Setting the derivative to zero and solving for $D$ yields $D*_G$, the form that maximizes $V$. 
\end{example}

\Needspace{15\baselineskip}
The authors use this proposition to define the virtual training criterion $C(G) \triangleq V(G, D^*_G)$:
\begin{align}
C(G)
&= \E[\vec x \sim p_{data}]{\log    \frac{ p_{data}(\vec x) }{ p_{data}(\vec x) + p_g(\vec x) }      } 
+  \E[\vec x \sim p_{g}]{\log    \frac{ p_{g}(\vec x) }{ p_{data}(\vec x) + p_g(\vec x) }      } \label{eq:gans-3}
\end{align}

\begin{definition}[-1em][Theorem 1.]
	The global minimum of $C(G)$ is achieved IFF $p_g = p_{data}$. At that point $C(G) = - \log4$. 
\end{definition}

\begin{example}[Proof: Theorem 1]
	The authors subtract $V(D^*_G, G; p_g{=}p_{data})$ from both sides of \ref{eq:gans-3}, do some substitions, and find that
	\begin{align}
	C(G) = 2 \cdot JSD(p_{data} || p_g) - \log 4
	\end{align}
	where $JSD$ is the \purple{Jensen-Shannon divergence}\footnote{Recall that the JSD represents the divergence of each distribution from the mean of the two}. Since $0 \le JSD(p || q)$ always, with equivalence only if $p \equiv q$, this proves Theorem 1 above.
\end{example}




% ======================================================================
\lecture{Generative Models}{Modular Generative Adversarial Networks}{October 13, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Zhao et al., ``Modular Generative Adversarial Networks'' \textit{UBC, Tencent AI Lab} (April 2018).}

\bluesec{TL;DR}. Task(s): multi-domain image \purple{generation} and image-to-image \purple{translation}.

\bluesec{Network Construction} (3.2). Let $x$ and $y$ denote the input and target image, respectively, wherever applicable. Let $\matr A = \{A_1, A_2, \cdots, A_n\}$ denote an \green{attribute set}. Four types of modules are used:
\begin{compactenum}
	\item Initial module is task-dependent (below). Output is feature map in $\R^{C \times H \times W}$. 
	\begin{compactitem}
		\item \purple{[translation]} \green{encoder $\matr E$}: $x \mapsto \matr E(x)$
		\item \purple{[generation]} \green{generator $\matr G$}: $(z, a_0) \mapsto \matr G(z, a_0) $ where $z$ is random noise and $a_0$ is a condition vector representing auxiliary information. 
	\end{compactitem}
	
	\item \underline{\green{transformer(s) $\matr[i]{T}$}} : $E(x) \mapsto \matr[i]{T}(\matr E(x), a_i)$. Modifies repr of attrib $a_i$ in the FM.
	\item \green{reconstructor $\matr R$} : $(\matr[i]{T}, \matr[j]{T}, \ldots) \mapsto y$. Reconstructs image from an intermediate FM. 
	\item \green{discriminator $\matr[i]D$}: $\matr R \mapsto \{0, 1\} \times \text{Val}(a_i)$. Predicts probability that $R$ came from $p_{true}$, and the [transformed] value of $a_i$. 
\end{compactenum}
The authors emphasize that the transformer module is their core module. It's architecture is illustrated below. 

\myfig[0.6\textwidth]{figs/mgan_transformer_fig3.png}


\bluesec{Loss Function} (3.4). 
\graybox{
	\mathcal{L}_{D}(\matr D)
	&= - \mgreen{ \insum \mathcal{L}_{adv_i} } 
	+ \lambda_{cls} \mpurple{ \insum \mathcal{L}_{cls_i}^r } \\
	\mathcal{L}_{G}(\matr E, \matr T, \matr R)
	&= \mgreen{ \insum \mathcal{L}_{adv_i} } 
	+ \lambda_{cls} \mblue{\insum \mathcal{L}_{cls_i}^{f} }
	+ \lambda_{cyc} \mred{ \lr{ \mathcal{L}_{cyc}^{\matr E \matr R} + \insum \mathcal{L}_{cyc}^{\matr[i]{T}}  } } \\
	\begin{split}
		\mgreen{ \mathcal{L}_{adv_i}(\matr E, \matr[i]T, \matr R, \matr[i]D) }
		&= \E[y \sim p_{data}(y)]{\log \matr[i]{D}(y)} + \\
		& \quad ~ \E[x \sim p_{data}(x)]{\log \lr{1 - \matr[i]{D} \lr{\matr R  \lr{\matr[i]{T} \lr{\matr E (x)}     }  }  }   } 
	\end{split}
	\\
	\begin{split}
		\mpurple{ \mathcal{L}_{cls_i}^r }
		&= - \E[x, c_i]{\log \matr[cls_i]{D}(c_i \mid x)}
	\end{split}
	\\
	\begin{split}
		\mblue{\mathcal{L}_{cls_i}^{f}}
		&= - \E[x, c_i]{ \log \matr[cls_i]{D}(c_i \mid  \matr R(\matr E(\matr[i]{T}(x))) ) }
	\end{split}
	\\
	\begin{split}
		\mred{ \mathcal{L}_{cyc}^{\bm{ER}} }
		&= \E[x]{|| \matr R(\matr E(x))   - x||_1  }
	\end{split}
	\\
	\begin{split}
		\mred{ \mathcal{L}_{cyc}^{\matr[i]{T}} }
		&= \E[x]{||   \matr[i]{T}(\matr E(x))     - \matr E(\matr R(\matr[i]{T}(\matr E(x))))       ||_1  }
	\end{split}
}
where $n$ is the total number of controllable attributes. 


% ======================================================================
\lecture{Generative Models}{Transfer Learning from Speaker Verification to TTS}{October 13, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Jia et al., ``Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis'' \textit{Google} (Jan 2019).}

\bluesec{TL;DR}: TTS that's able to generate speech in the voice of different speakers, including those unseen during training.


\bluesec{Multispeaker Speech Synthesis Model} (2). System is composed of three independently trained NNs:
\begin{compactenum}
	\item \green{Speaker Encoder}. Computes a fixed-dimensional vector from a speech signal.
	
	\item \green{Synthesizer}. Predicts a \purple{mel spectrogram} from a sequence of \purple{grapheme} or \purple{phoneme} inputs, conditioned on the speaker vector. Extension of \purple{Tacotron 2} to support multiple speakers.
	
	\item \green{Vocoder}. Autoregressive WaveNet, which converts the spectrogram into time domain waveforms.
\end{compactenum}


% ======================================================================
\lecture{Generative Models}{Tacotron 2}{November 10, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Shen et al., ``NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET ON MEL SPECTROGRAM PREDICTIONS'' \textit{Google}, \textit{UCB} (Feb 2018).}

\bluesec{TL;DR}: Seq2seq network for mapping characters to mel-scale spectrograms\footnote{A \green{mel-frequency spectrogram} is obtained by applying a nonlinear transform to the frequency axis of the short-time Fourier transform (STFT).}, followed by a modified WaveNet vocoder. 


\bluesec{Spectrogram Prediction Network} (2.2). Illustrated below.
\myfig[0.5\textwidth]{figs/tacotron2.png}

\begin{compactitem}
	\item Encoder: character language model architecture. They use convolution layers in the middle under the hypothesis that these should learn ngram-like representations. 
	
	\item Attention Decoder: \green{location-sensitive} encoder-decoder attention\footnote{Basically additive attention with cumulative weights.}. At each timestep, instead of incorporating the previous decoder prediction, they first feed it through a \green{pre-net} (2 FF layers). Furthermore, they also have a \green{post-net} which predicts a residual to add to the prediction ``to improve the overall reconstruction''. \\
	
	Finally, they also project the decoder/attention outputs to a scalar (sigmoid activation) for predicting probability that output sequence has completed.
	
\end{compactitem}

\bluesec{WaveNet Vocoder} (2.3). Inverts the mel spectrogram feature representation into time-domain waveform samples. Specifically, I assume this means they take each time slice of the predicted spectrogram (a vector over frequencies?) and do \green{local conditioning} (as described in WaveNet paper) with $\vec[t]{h}$ the spectrogram at time slice $t$. 

\begin{example}[Digression: Fourier Transforms]
	We measure sound by pressure/amplitudes as a function of time. What we actually measure is the \textit{sum total} of all the individual sinusoidal waves each with their own frequency/amplitude. To reiterate: in the \textit{time domain} our x-axis is time (duh) and our y-axis is amplitude. The \green{Fourier transform} maps our time-domain representation into a \textit{frequency domain}. Now, the x-axis is frequency (duh) and the y-axis represents how much of our original signal consisted of waves with a given frequency. For example, if our original signal was literally a 3 Hz wave superimposed over some 5 Hz wave, our frequency-domain plot would show two spikes at x=3 and x=5, and be zero everywhere else. \\
	
	Formally, let $g(t)$ denote the amplitude of some sound wave at time $t$.
	\begin{align}
	g(t) e^{-2\pi i f t}
	\end{align}
	where 
	\begin{compactitem}
		\item The negative sign in the exponent is a convention that we should think about \textit{clockwise} rotations. 
		
		\item f denotes the ``winding'' frequency: the number of full cycles represented in our wound up graph. 
		
		\item Multiplying by $g(t)$ means that the distance-to-origin (magnitude) at time $t$ will always equal $g(t)$. 
	\end{compactitem}
	Similarly, the \green{inverse Fourier transform} maps from frequency domain to time domain.
	
	
	Remember that the trick for identifying the component frequencies is by computing the \green{center of mass} of this formula:
	\begin{align}
	\inv{t_2 - t_1} \int_{t_1}^{t_2} g(t) 	e^{-2\pi i f t} \mathrm{d} t
	\end{align}
	where, to do this for one full circle, we'd set $t_1 \eq 0$ and $t_2 \eq 1/f$. If our signal $g(t)$ does contain the frequency $f$, this integral will be relatively large in comparison with other frequencies. The final formula for the Fourier transform is just removing the scale factor (i.e. the FT is just the CoM scaled by the time interval of our signal):
	\graybox{
		\hat{g}(\omega)
		&\triangleq  \Re \left\{  \int_{t_1}^{t_2} g(t) 	e^{-2\pi i \omega t} \mathrm{d} t \right\}
	}
	More intuition regarding removal of the scale factor: if there is a component wave in $g(t)$ that only exists for a small portion of time $\delta t$, it would have a smaller value of $\hat{g}(\omega)$ than a component of some different frequency (but same amplitude) that persisted throughout the entirety of our time interval.
\end{example}

\begin{example}[Digression: Spectrograms]
	Now that we know about Fourier transforms (above example), we can define what a spectrogram is. Say that, instead of a single function $g(t)$, I split time into various windows $\{t_0, t_1, t_2, t_T\}$ and store a separate function, $g_{\tau}(t)$ with $1 \le \tau \le T$, for each window. Then, I apply the FT on each function, resulting in a set of $\hat{g}_{\tau}(\omega)$. If I plot a 2D heatmap with my x-axis denoting the window $\tau$, the y-axis denoting frequency $\omega$, and values (color) denoting $\hat{g}_{\tau}(\omega)$, I have a \green{spectrogram} (technically I need to transform my y-axis to $\log \omega$ and my color/value axis to Decibels). 
\end{example}




% ======================================================================
\lecture{Generative Models}{Glow}{November 16, 2019}
% ======================================================================

\vspace{-1em}
{\footnotesize Kingma et al., ``Glow: Generative Flow with Invertible 1x1 Convolutions'' \textit{OpenAI}, (July 2018).}

\newcommand\pt{p_{\theta}}

\bluesec{Flow-based Generative Models} (2). 
\begin{align}
\vec z
&\sim p_{\theta}(\vec z) \\
\vec x 
&= \vecfn[\theta]{g}{z}
\end{align}
where $\vec g$ is invertible. \textit{Inference} is done by $\vec z =\vecfn[\theta]{f}{x} = \vinv[\theta]{g}(\vec x)$. 
\begin{align}
\log	\pt (\vec x)
&= \log \pt (\vec z) + \log | \det \pderiv{\vec z}{\vec x} | \\
&= \log \pt (\vec z) + \sum_{i=1}^K \log | \det \pderiv{\vec[i]{h}}{\vec[i-1]{h}} |
\end{align}
The log-determininant gives the change in log-density when going $\vec[i-1]{h} \rightarrow \vec[i]{h}$. We can see that maximum likelihood will encourage $\vecfn[\theta]{f}{x}$ to increase volume.


% ======================================================================
\lecture{Generative Models}{WaveGlow}{November 16, 2019}
% ======================================================================

\vspace{-1em}
{\footnotesize Prenger et al., ``WaveGlow: A Flow-Based Generative Network for Speech Synthesis''' \textit{NVIDIA}, (Oct 2018).}


\bluesec{TL;DR}: flow-based \green{vocoder}\footnote{Vocoder: network that transforms time-aligned features (e.g. spectrograms) into audio samples.}


\bluesec{WaveGlow} (2). Model the distribution of audio samples \textit{conditioned on} a mel-spectrogram. Note that the \textit{forward pass} is defined as going from $\vec x$ to $vec z$. Train by minimizing the negative log-likelihood:
\graybox{
	\vec z 
	&\sim \Gauss{\vec z; \vec 0, \matr I} \\
	\vec x
	&= \vec[0]{f} \circ \vec[1]{f} \circ \cdots \circ  \vec[k]{f}(\vec z) \\
	\log p_{\theta}(\vec x)
	&= \log p_{\theta}(\vec z) + \sum_{i=1}^{k} \log | \det \lr{   \matr J \lr{ \vinv[i]{f}(\vec x)  }  } | \\
	\vec z 
	&= \vinv[k]{f} \circ \vinv[k-1]{f} \circ \cdots \circ \vinv[0]{f}(\vec x)
}
and $\vec x$ denotes the output audio. The mel-spectrogram conditioning happens in the affine coupling layer(s). Training penalizes the norm of $\vec z$ and encourages each layer $f_i^{-1}$ to increase the log-density volume of the previous layer. 



\myfig[0.3\textwidth]{figs/waveglow.png}


\begin{align}
\vec[a]{x}, \vec[b]{x}
&= split(\vec x) \\
\log \vec s , \vec t
&= WN(\vec[a]{x}, mel-spectrogram) \\
\vec[b]{x}'
&= \vec s \odot \vec[b]{x} + \vec t \\
\vinv[coupling]{f}(\vec x)
&= concat(\vec[a]{x}, \vec[b]{x})
\end{align}











% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Meta Learning}\label{Meta Learning}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------






% ======================================================================
\lecture{Meta Learning}{Online Meta-Learning}{October 04, 2020}
% ======================================================================

\citepaper{Finn et al.}{Online Meta-Learning}{Stanford University}{Jul 2019}

\bluesec[-0.5]{Online Learning} (2.3). The agent faces a \textit{sequence} of loss functions $\{ \ell_t  \}_{t=1}^{\infty}$, one in each round $t$. At each such round $t$, the learner must decide how to update the model parameters $\vec[t]{w}$ that perform well on the loss \textit{sequence}\footnote{Not just the current loss -- we don't want to forget what was previously learned.}. The most standard notion of \green{regret} is to compare our learner's cumulative loss over time with the cumulative loss of the best \textit{fixed} model in hindsight:
\begin{align}
	\mtext{Regret}_T
		&= \sum_{t=1}^T \ell_t( \vec[t]{w} ) - \min_{\vec w} \sum_{t=1}^T \ell_t(\vec w) \label{eq:online-regret}
\end{align}
A simple algorithm for this is \green{follow the leader (FTL)}\footnote{Not sure why they're saying this is FTL...we aren't selecting the current best leader at each step per-say. We are just training on everything we've seen. That's it...}:
\begin{align}
	\vec[t+1]{w}
		&= \argmin_{\vec w} \sum_{k=1}^t \ell_k (\vec w) \label{eq:ftl}
\end{align}

\bluesec{The Online Meta-Learning Problem} (3). We now allow the agent to perform some local \textit{task-specific} updates to the model before it's deployed/evaluated at each round $t$. This is realized through an update procedure at each round $t$ as a mapping $\matr[t]{U} : \vec w \mapsto \vec{\widetilde w}$. For example, $\matr[t]{U}$ could implement MAML:
\begin{align}
	\matr[t]{U}(\vec w)	
		&= \vec w  - \alpha \nabla \hat\ell_t (\vec w)
\end{align}
where the notation $ \nabla \hat\ell_t$ is meant to be interpreted as ``approximate gradient of the true $\ell_t$ obtained with e.g. a minibatch of data from the task at round $t$.'' To compute regret, we just replace the inputs $\vec w$ to the losses in equation \ref{eq:online-regret} with $\matr[t]{U}(\vec w)$.

\bluesec{Algorithm and Analysis} (4). Authors propose the \green{follow the meta leader (FTML)} algorithm for their update rule, which again is just equation \ref{eq:ftl} but replacing $\vec w$ with $\matr[t]{U}(\vec w)$ for the loss function inputs. 

\myfig[0.9\textwidth]{figs/oml_ftml.png}




% ======================================================================
\lecture{Meta Learning}{Model-Agnostic Meta-Learning}{October 11, 2020}
% ======================================================================

\citepaper{C. Finn, P. Abbeel, and S. Levine}{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}{UC Berkeley}{Jul 2017}

\bluesec{Meta-Learning Problem Set-Up} (2.1). Some misc notation \& definitions:
\begin{compactitem}
	\item Model $f: \vec x \mapsto \vec a$
	
	\item Each task $\mathcal T$ consists of 
	\begin{compactitem}
		\item Loss function $\mathcal{L}(  \{ \vec[i]{x}, \vec[i]{a} \}_{i=1}^{H} )$.
		\item Distribution over initial observations $ q(\vec[1]{x})$.
		\item Transition distribution $q(\vec[t+1]{x}, \mid \vec[t]{x}, \vec[t]{a})$. 
		\item Episode length $H$.
	\end{compactitem} 

	\item Distribution over tasks $p(\mathcal T)$. 
	
	\item K-shot learning.
	\begin{compactenum}
		\item Sample task $\mathcal{T}_i \sim p(\mathcal T)$. 
		\item Sample $K$ times from $q_i(\vec[1]{x})$. 
		\item Train the model with loss $\mathcal{L}_{\mathcal{T}_i}$.  
	\end{compactenum}

	\item Meta-training. 
	\begin{compactenum}
		\item Run the K-shot learning procedure above. 
		\item Test model on new samples from $\mathcal{T}_i$. 
		\item Improve model based on how test error changes wrt the parameters. 
	\end{compactenum}
\end{compactitem}

\bluesec{MAML} (2.2). Defined by the meta-objective,
\graybox{
	\min_{\theta} \sum_{\mathcal{T}_i \sim p(\mathcal T)} \mathcal{L}_{\mathcal{T}_i} (f_{\theta'_i})
		= \sum_{\mathcal{T}_i \sim p(\mathcal T)} \mathcal{L}_{\mathcal{T}_i} \lr{  
			f_{\theta - \alpha \nabla_{\theta}  \mathcal{L}_{\mathcal{T}_i } (f_{\theta}) }
	}
}
\myfig[0.4\textwidth]{figs/maml_alg1.png}


\bluesec{Supervised Regression and Classification} (3.1). Here, $H = 1$ and we can drop the subscript on $\vec[t]{x}$. 
\myfig[0.4\textwidth]{figs/maml_alg2.png}

$K$-shot classification tasks use $K$ input/output pairs from each class, for a total of $NK$ data points for $N$-way classification. 





% ======================================================================
\lecture{Meta Learning}{OSAKA}{November 01, 2020}
% ======================================================================

\citepaper{Caccia et al.}{Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning}{Mila, ElementAI, Universite de Montreal, FAIR}{Jul 2020}

\bluesec[-0.3]{Introduction} (1). Terminology:
\begin{compactitem}
	\item \green{Continual learning} (CL): learn incrementally from a non-stationary data sequence involving different datasets/tasks, while mitigating catastrophic forgetting.
	\begin{compactitem}
		\item \green{Task-incremental classification}: classification datasets presented to online learner sequentially. Data for task $T_t$ samples from $P_t(\vec x, y)$. Models evaluated by their average final performance across all tasks. 
	\end{compactitem}
	\item \green{Continual-meta learning} (CML): focus on fast remembering. 
	\item \green{Meta-continual learning} (MCL): uses meta-learning to learn not to forget. 
\end{compactitem}

\begin{itemdefinition}{OSAKA}{Online faSt Adaptation and Knowledge Accumulation}
	\item Task shifts are sampled stochastically. 
	\item Task boundaries are unknown (task-agnostic setting).
	\item Target distribution is context-dependent. 
	\item Multiple levels of non-stationarity. 
	\item Tasks can be revisited. 
\end{itemdefinition}

\bluesec{A Unifying Framework} (2). Notation:
\begin{compactitem}
	\item $C$: hidden context variable that determines the data distribution, $P(X \mid C)$. 
	\item $\mathcal W$: finite set of all possible contexts. 
	\item $S$: support set (meta-training set). 
	\item $Q$: query set (meta-test set). 
	\item $\mathcal A$: learning algorithm functional, $\mathcal A : S \mapsto f_{\theta}$. 
\end{compactitem}

\begin{table}{c | c | c}{Summary of the inner and outer loop computations of each approach.}
	\toprule
	Name & 
		Inner Loop & 
		Outer Loop \\ \hline
	Meta-learning & 
		$f_{\theta_i} = \mathcal{A}_{\phi}(S_i)$ & 
		$\nabla_{\phi} \mathcal L (f_{\theta_i}, Q_i)$ \\ 
	Continual learning & 
		$f_{\theta} = \mtext{CL}(S_{1:T})$ & 
		? \\
	Meta-continual learning & 
		$f_{\theta_i} = \mtext{CL}_{\phi}(S_{i,~ 1{:}T})$ & 
		$\nabla_{\phi} \sum_t \mathcal L(f_{\theta_i}, Q_{i,t})$ \\
	Continual-meta learning & 
		$f_{\theta_t} = \mathcal A_{\phi}(S_{t-1})$ & 
		$\nabla_{\phi} \mathcal L (f_{\theta_t}, S_t)$ \\
	\bottomrule
\end{table} 

\begin{algorithm}[OSAKA (3)]
	Two stage approach consisting of a (1) pre-training and (2) deployment phase. \\
	
	\textbf{Pre-training}. Let $P(C_{pre})$ denote the distribution of contexts we'll sample from, and let $\theta_0$ denote initial model parameters. Loop:
	\begin{compactenum}
		\item Sample context $C \sim P(C_{pre})$
		\item Sample data $\vec x, \vec y \sim p(\vec x, \vec y \mid C)$
		\item Update $\theta_0$ with $\vec x, \vec y$
	\end{compactenum}

	\vspace{1em}
	\textbf{Deployment} (continual learning (CL)). Let $P(C_{cl})$ denote the distribution over contexts that we'll sample from during CL time. Denote Markov process for $C_t$ as $P(C_t \mid C_{t-1}; \alpha)$, with scalar $\alpha$ denoting stationarity level: $P(C_t \eq c \mid C_{t-1} \eq c) = \alpha$. Loop:
	\begin{compactenum}
		\item $C_t \sim P(C_{cl} \mid C_{t-1}; \alpha)$
		\item $\vec[t]{x}, \vec[t]{y} \sim p(\vec x, \vec y \mid C_t)$
		\item $\mathcal L(f_{\theta_{t-1}}(\vec[t]{x}), \vec[t]{y})$
		\item Update $\theta_t$ with $\vec[t]{x}, \vec[t]{y}$ at discretion
	\end{compactenum}
\end{algorithm}

\begin{algorithm}[Continual-MAML (4)]
	\textbf{Pre-training}. Perform MAML. Let $\eta_{\phi}$ denote the learnable inner loop learning rate. Loop:
	\begin{compactenum}
		\item $\{C_i\}_{i=1}^B \sim P(C_{pre})$
		\item For each $C_i$: 
		\begin{compactenum}
			\item $\vec[i]{x}, \vec[i]{y} \sim P(\vec x, \vec y \mid C_i)$
			\item $\theta_i \leftarrow \phi  - \eta_{\phi} \nabla_{\phi} \mathcal L \lr{   f_{\phi}\lr{  \vec[i]{x}[:k]  } ,  \vec[i]{y}[:k]     }$
		\end{compactenum}
		\item $\phi \leftarrow \phi - \eta \nabla_{\phi} \sum_i \mathcal L  \lr{   f_{\theta_i}\lr{  \vec[i]{x}[:k]  } ,  \vec[i]{y}[:k]     }$
	\end{compactenum}

	\vspace{1em}
	\textbf{Deployment} (continual learning (CL)). Initialize $\theta_0 := \phi$. Let function $g_{\lambda} : \R \mapsto (0, 1)$ control \green{update modulation} (UM). Let $\gamma$ control the \green{prolonged adaptation phase} (PAP). Loop:
	\begin{compactenum}
		\item $C_t \sim P(C_{cl} \mid C_{t-1})$
		\item $\vec[t]{x}, \vec[t]{y} \sim p(\vec x, \vec y \mid C_t)$
		\item $\mathcal L(f_{\theta_{t-1}}(\vec[t]{x}), \vec[t]{y})$
		\item Virtual model $\widetilde{\theta_t} \leftarrow \phi  - \eta_{\phi} \nabla_{\phi} \mathcal L \lr{   f_{\phi}\lr{  \vec[t]{x}  } ,  \vec[t]{y}   }$
		\item If $\Delta \mathcal L < \gamma$: (no context shift detected)
			$$  \theta_t \leftarrow \theta_{t-1} - \eta_{\phi} \nabla_{\theta} \mathcal L \lr{   f_{\theta_{t-1}}\lr{  \vec[t]{x}  } ,  \vec[t]{y}   }$$ 
		\item Else: (task boundary detected)
		\begin{compactenum}
			\item Modulated LR $\eta_t \leftarrow \eta g_{\lambda}\lr{     \mathcal L \lr{     f_{\theta_{t-1}}(\vec[t]{x}) , \vec[t]{y} }  }$ 
			\item update metaparams $\phi \leftarrow \phi - \eta_t \nabla_{\phi} \mathcal L \lr{     f_{\theta_{t-1}}(\vec[t]{x}) , \vec[t]{y} } $
			\item Reset fast params $\theta_t \leftarrow \phi - \eta_{\phi} \nabla_{\phi}  \mathcal L \lr{   f_{\phi}\lr{  \vec[t]{x}  } ,  \vec[t]{y}   }$
		\end{compactenum}
	\end{compactenum}
\end{algorithm}



% ======================================================================
\lecture{Meta Learning}{Proto-MAML}{November 01, 2020}
% ======================================================================

\citepaper{Triantafillou et al.}{Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples}{University of Toronto and Vector Institute, Google AI, UC Berkeley}{Apr 2020}

\bluesec{Task Formulation and Approaches} (2). 
\begin{compactitem}
	\item \green{Episode}: refers to learning from some number $k_c ~ (c \in [1..N])$ of labeled examples per class out of $N$ classes. This is sometimes (equivalently) presented as a \green{support set} $\mathcal S = \{  (\vec[i]{x}, \vec[i]{y}) \}_{i=1}^K$, with $K = \sum_c k_c$. After the learning episode, we usually want to be able to subsequently generalize well to unseen examples for that episode on some held-out \green{query set} $\mathcal Q$. 
	
	\item  $ \mgreen{ \mathcal{C}_{train}  }  $: disjoint set of classes used for training the model. 
\end{compactitem}
Non-episodic approach is to train a NN on all $C_{train}$ classes at once, with linear output layer over all the classes. Then, for the few-shot scenario, use this model as an \textit{embedding function} and perform e.g. kNN on the resulting output representations. Below are the main models explored by the authors:\marginnote{$\alpha$ is softmaxed cosine sim}[3em]
\graybox{
	\mtgreen{[ProtoNet]}\quad
	p(y^* \eq k \mid \vec{x}^*, \mathcal S)
		&= \mtext{Softmax}( -  || g(\vec{x}^*)  - \vec[k]{c} ||_2^2  ) \\
	\mtgreen{[MatchingNet]}\quad
	p(y^* \eq k \mid \vec{x}^*, \mathcal S)
		&= \sum_{i=1}^{|\mathcal S|} \alpha(\vec{x}^*, \vec[i]{x}) \ind_{  \{ y_i=k\} } \\
	\mtgreen{[MAML]}\quad
	p(y^* \eq k \mid \vec{x}^*, \mathcal S)
		&=	\mtext{Softmax}(\vec{b}' + \matr{W}' g(\vec{x}^*; \theta')   )
}















% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------
\mysection{Optimization}\label{Optimization}
% -----------------------------------------------------------------------------------------------------------------------------------------------
% ==================================================================================
% -----------------------------------------------------------------------------------------------------------------------------------------------



% ===========================================================================================
\lecture{Optimization}{Non-Convex Optimization for Machine Learning}{August 12, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize P. Jain and P. Kar, ``Non-convex Optimization for Machine Learning,'' (2017).}

\p \blue{Convex Analysis} (2.1). First, let's summarize some definitions.

\textbf{Convex Combination}
\begin{definition}[-1.em]
	A \green{convex combination} of a set of $n$ vectors $\vec[i]{x} \in \R^p$, $i=1\ldots n$ is a vector $\vec[\theta]{x} := \insum \theta_i \vec[i]{x}$, where $\theta_i \ge 0$ and $\insum \theta_i = 1$.\\ 
	\underline{My interp}: A weighted average where the weights can be interpreted as probability mass associated with each vector.
\end{definition}

\textbf{Convex Set}. Sets that contain all [points in] line segments that join any 2 points in the set.
\begin{definition}[-2.0em]
	A set $\mathcal C$ is called a \green{convex set} if $\forall \vec x, \vec y \in \mathcal C$ and $\lambda \in [0, 1]$, we have that $(1 - \lambda) \vec x + \lambda \vec y \in \mathcal C$ as well. 
\end{definition}

\begin{example}[Proving conv. comb. of 3 vectors $\in \mathcal C$ too.]
	After reading the definition of a convex set above, it seemed intuitive that any convex combination of points $\in \mathcal C$ should also be in it as well (i.e. generalizing the pairwise definition). Let $x, y, z \in \mathcal C$. How can we prove that $\theta_1 x + \theta_2 y + \theta_3 z$ (where $\theta_i$ satisfy the constraints of a convex comb.) is also in $\mathcal C$? Here is how I ended up doing it:
	
	\begin{compactitem}
		\item If we can prove that $\theta_1 x + \theta_2 y = (1 - \theta_3) v$ for some $v \in \mathcal{C}$, then our work is done. This is pretty easy to show via simple arithmetic. 
		\item Case 1: assume $\theta_3 < 1$, so that we can divide both sides by $1 - \theta_3$:
		$$
		v = \frac{\theta_1}{1 - \theta_3} x	+ \frac{\theta_2}{1 - \theta_3}
		$$
		Clearly, the two coefficients here sum 1 and satisfy the constraints of a convex combination, and therefore we know that $v \in \mathcal C$, and this case is done. 
		
		\item Case 2: assume $\theta_3 = 1$. Well, that means $\theta_1 = \theta_2 = 0$. Trivially, $z \in \mathcal C$ and this case is done. 
	\end{compactitem}
\end{example}

\textbf{Convex Function}
\begin{definition}[-1em]
	A continuously differentiable function $f : \R^p \mapsto \R$ is a \green{convex function} if $\forall \vec x, \vec y \in \R^p$, we have that
	\begin{align}
	f(\vec y) \ge f(\vec x) + \langle \nabla f(\vec x), \vec y - \vec x \rangle 
	\end{align}
\end{definition}
While thinking about how to gain intuition for the above, I came across chapter 3 of ``Convex Optimization'' which describes this in much more detail. It's crucial to recognize that the RHS of the inequality is the 1st-order Taylor expansion of the function $f$ about $\vec x$, evaluated at $\vec y$. In other words, \textit{the first-order Taylor approximation is a \textbf{global underestimator} of any convex function $f$}\footnote{Consider what this implies about all the 1st-order gradient-based optimizers we use.}. \\

\textbf{Strongly Convex/Smooth Function}. Informally, strong convexity ensures a convex function doesn't grow too \textit{slow}, while strong smoothness ensures a \sout{convex}\footnote{Strong smoothness alone does not imply convexity.} function doesn't grow too \textit{fast}. Formally,
\begin{definition}[-1em]
	A continuously differentiable function is considered \green{$\alpha$-strongly convex} (SC) and \green{$\beta$-strongly smooth} (SS) if $\forall \vec x, \vec y \in \R^p$ we have
	\begin{align}
	\frac{\alpha}{2} || \vec x - \vec y ||_2^2
	\le
	f(\vec y) - f(\vec x) -  \langle \nabla f(\vec x), \vec y - \vec x \rangle 
	\le 
	\frac{\beta}{2} || \vec x - \vec y ||_2^2
	\end{align}
\end{definition}
Considering the aforementioned 1st-order Taylor approximation interpretation, we see that $\alpha$ determines just how much larger $f(\vec y)$ must be compared to its linear approximation. Conversely, $\beta$ determines the upper bound for how large this discrepancy is allowed to be\footnote{Notice that SC and SS are \textit{quadratic} lower and upper bounds, respectively. This means that the allowed deltas grow as a function of the distance between $\vec x$ and $\vec y$, whereas things like Lipschitzness grow linearly.}. 

\begin{example}[Exercise 2.1: SS does not imply convexity]
	\textit{Construct a \textbf{non-convex} function $f : \R^p \mapsto \R$ that is $1$-SS.}
	\tcblower 
	
	We need to find a function whose linear approximation is always more than $\onehalf$ times the magnitude of the difference in inputs \textbf{squared}, compared to the true value. Intuitively, I'd expect any \textit{concave} function to satisfy this, since its linear approximation is a global \textit{overestimator} of the true value. So, for example, $f(\vec y) = - || \vec y ||_2^2$ would satisfy $1-SS$ while being non-convex.
\end{example}

\textbf{Lipschitz Function}
\begin{definition}[-1em]
	A function $f$ is \green{B-Lipschitz} if $\forall \vec x, \vec y \in \R^p$,
	\begin{align}
	|f(\vec x) - f(\vec y)| \le B \cdot ||\vec x - \vec y ||_2
	\end{align}
\end{definition}

\textbf{Jensen's Inequality}. Generalizes behavior of convex functions on convex combinations\footnote{It should be obvious that expectations are convex combinations.}.
\begin{definition}[-1em]
	If $X$ is a R.V. taking values in the domain of a convex function $f$, then
	\begin{align}
	\E{f(X)} \ge f(\E{X})
	\end{align}
\end{definition}


\myspace\Needspace{12\baselineskip}
\p \blue{Convex Projections} (2.2). Given any closed set $\mathcal C \in \R^p$, the projection operator $\Pi_{\mathcal C}(\cdot)$ is defined as
\begin{align}
\Pi_{\mathcal C}(\vec z)
&:= \argmin_{\vec x \in \mathcal C} || \vec x - \vec z ||_2
\end{align}
If $\mathcal C$ is a convex set, then the above reduces to a convex optimization problem. Projections onto convex sets have three particularly interesting properties. For each of them, the setup is: ``For any convex set $\mathcal C \subset \R^p$, and any $\vec z \in \R^p$, let $\hat{\vec z} := \Pi_{\mathcal C}(\vec z)$. Then $\forall \vec x \in \mathcal C$, \textellipsis''
\begin{compactitem}
	\item \textbf{Property-O}\footnote{In this case only, $\mathcal C$ need not be convex}: 
	$
	||\hat{\vec z} - \vec z||_2 \le ||\vec x - \vec z||_2
	$
	. Informally: ``the projection results in the point $\hat{\vec z}$ in $\mathcal C$ that is closest to the original $\vec z$''. This basically just restates the optimization problem. 
	
	\item \textbf{Property-I}. 
	$
	\langle \vec x - \hat{\vec z}, \vec z - \hat{\vec z} \rangle \le 0
	$
	. Informally: ``from the perspective of $\hat{\vec z}$, all points $\vec x \in \mathcal C$ are in the (informally) opposite direction of $\vec z$.''
	
	\item \textbf{Property-II}. $|| \hat{\vec z} - \vec x||_2 \le ||\vec z - \vec x||_2$. Informally: ``the projection brings the point closer to all points in $\mathcal C$ compared to its original location.''
\end{compactitem}


\begin{example}[Proving Property-I]
	A proof by contradiction.
	\begin{compactenum}
		\item Assume that $\exists \vec x \in \mathcal C$ s.t. $ 	\langle \vec x - \hat{\vec z}, \vec z - \hat{\vec z} \rangle > 0$.
		
		\item We know that $\hat{\vec z}$ is also in $\mathcal C$, and since $\mathcal C$ is convex, then for any $\lambda \in [0, 1]$, 
		\begin{align}
		\vec[\lambda]{x} := \lambda x + (1 - \lambda) \hat{\vec z}
		\end{align}
		must also be in $\mathcal C$. 
		
		\item If we can show that some value of $\lambda$ guarantees that $||\vec z - \vec[\lambda]{x}||_2 < ||\vec z - \hat{\vec z}||_2$, this would directly contradict property-O, implying $\hat{\vec z}$ is not the closest member of $\mathcal C$ to $\vec z$. I'm not sure how to actually derive the range of $\lambda$ values that satisfy this, though. 
	\end{compactenum}  
\end{example}



\myspace
\p \blue{(Convex) Projected Gradient Descent} (2.3). Our optimization problem is
\begin{align}
\min_{\vec x \in \R^p} f(\vec x) \quad\text{s.t.}\quad \vec x \in \mathcal C
\end{align}
where $\mathcal C \subset \R^p$ is a convex constraint set, and $f: \R^p \mapsto \R$ is a convex objective function. Projected gradient descent iteratively updates the value of $\vec x$ that minimizes $f$ as usual, but additionally projects the current iterate (value of best $\vec x$) onto $\mathcal C$ at the end of each iteration. That's the only difference. 



% ---------------
\myspace
\subsub{Non-Convex Projected Gradient Descent (3)}
\myspace
% ----------------

\p \blue{Non-Convex Projections} (3.1). Here we look at a few special cases where projecting onto a non-convex set can still be carried out efficiently. 
\begin{compactitem}
	\item \textbf{Projecting into sparse vectors}. The set of $s$-sparse vectors (vectors with at most $s$ nonzero elements) is denoted as
	\begin{align}
	\mathcal{B}_0(s) 
	&\triangleq \{ \vec x  \in \R^p \mid ||\vec x ||_0 \le s   \}
	\end{align}
	It turns out that $\hat{\vec z} := \Pi_{\mathcal{B}_0(s)}(\vec z)$ is obtained by setting all except the top-s elements of $\vec z$ to zero. 
	
	\item \textbf{Projecting into low-rank matrices}. The set of $m \times n$ matrices with rank at most $r$ is denoted as
	\begin{align}
	\mathcal{B}_{rank}(r) 
	&\triangleq \{
	A \in \R^{m \times n} \mid
	\text{rank}(A) \le r
	\}
	\end{align}
	and we want to project some matrix $A$ onto this set,
	\begin{align}
	\Pi_{\mathcal{B}_{rank}(r)}(A)
	:= \argmin_{X \in \mathcal{B}_{rank}(r)} ||A - X||_F
	\end{align}
	This can be done efficiently via SVD on $A$ and retaining the top $r$ singular values and vectors.
\end{compactitem}


\myspace
\p \blue{Restricted Strong Convexity and Smoothness} (3.2).

\textbf{Restricted Convexity}
\begin{definition}[-1em]
	A continuously differentiable function $f : \R^p \mapsto \R$ is said to satisfy restricted convexity over a (possibly non-convex) region $\mathcal C \subseteq \R^p$ if $\forall \vec x, \vec y \in \mathcal C$, we have that
	\begin{align}
	f(\vec y) \ge f(\vec x) + \langle \nabla f(\vec x), \vec y - \vec x \rangle 
	\end{align}
\end{definition}
and a similar rephrasing for restricted strong convexity (RSC) and restricted strong smoothness (RSS). 


% ============================================================================================
\lecture{Optimization}{Co-sampling: Training Robust Networks for Extremely Noisy Supervision}{May 02, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Han et al., ``Co-sampling: Training Robust Networks for Extremely Noisy Supervision,'' (2018).}

\p \blue{Introduction}. The authors state that current methodologies [for training networks under noisy labels] involves estimating the \green{noise transition matrix} (which they don't define). Patrini et al. (2017) define the matrix as follows:
\begin{center}
	\begin{small}
		{\itshape 
			Denote by $T \in [0, 1]^{c \times c}$ the noise transition matrix specifying the probability of one label being flipped to another, so that $\forall i, j ~ T_{ij} \triangleq \Prob{\widetilde y = e^j \mid y = e^i}$. The matrix is row-stochastic\footnote{Each row sums to 1.} and not necessarily symmetric across the classes.
		}
	\end{small}
\end{center}


\myspace
\p \blue{Algorithm}. Authors propose a learning paradigm called \green{Co-sampling}. They maintain two networks $f_{w_1}$ and $f_{w_2}$ simultaneously. For each mini-batch data $\hat{\mathcal D}$, each network selects $\mathcal{R}_T$ small-loss instances as a ``clean'' mini-batch $\hat{\mathcal D}_1$ and $\hat{\mathcal D}_2$, respectively. Each of the two networks then uses the clean mini-batch data to update the parameters $w_2$ ($w_1$) of its \textit{peer} network.
\begin{compactitem}
	\item Why small-loss instances? Because deep networks tend to fit clean instances first, then noisy/harder instances progressively after. 
	\item Why two networks? Because if we just trained a single network on clean instances, we would not be robust in extremely high-noise rates, since the training error would accumulate if the selected instances are not ``fully clean.''
\end{compactitem}
The Co-sampling paradigm algorithm is shown below.

\myfig[0.6\textwidth]{figs/cosampling_alg_1.png}





% ===========================================================================================
\lecture{Optimization}{Connectionist Temporal Classification}{October 28, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Graves et al., ``Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,'' (2006).}

\p \blue{Temporal Classification}.

\begin{compactitem}
	\item \textbf{Input space}: Let $\mathcal X = (\R^m)^*$ be the set of all sequences of $m$ dimensional real-valued vectors. 
	
	\item \textbf{Output space}: Let $\mathcal{Y} = L^*$ be the set of all sequences of a finite vocabulary of $L$ labels. 
	
	\item \textbf{Data distribution}: Denote by $\mathcal{D}_{\mathcal X \times \mathcal Y}$ the probability distribution over samples $(\vec x, \vec y)$. Let $S$ denote a set of training examples drawn from this distribution. 
\end{compactitem}

\myspace
\p \blue{From Network Outputs to Labellings} (3.1). Let $L' = L \cup \{ \epsilon \}$ denote the set of unique labels combined with the blank token $\epsilon$. We refer to the alignment sequences of length $T$ (same length as $\vec x$), i.e. elements of the set $(L')^T$, as \green{paths} and denote them $\pi$, where
\begin{align}
p(\pi \mid \vec x)
&= \prod_{t=1}^{T} y_{\pi_t}^t \qquad (\forall \pi \in (L')^T)
\end{align}
and $y_k^t$ denoting the probability of observing label $k$ at time $t$. Now that we have paths $\pi$, we need to convert them to label sequences by (1) merging repeated contiguous labels, and then (2) removing blank tokens. We denote this procedure as a many-to-one map $\mathcal{B} : L'^T \mapsto L^{\le T}$. In other words, $\mathcal B (\pi) \rightarrow \vec\ell$. We can then write the conditional posterior over possible output sequences $\vec\ell$:
\graybox{
	p(\vec\ell \mid \vec x)
	&= \sum_{\pi \in \mathcal{B}^{-1}(\vec\ell)} p(\pi \mid \vec x)
}


\myspace
\p \blue{Constructing the Classifier} (3.2). There is no tractable algorithm for exact decoding, i.e. computing
\begin{align}
h(\vec x) 
&:= \argmax_{\vec\ell \in L^{\le T}} p(\vec\ell \mid \vec x)
\end{align}
However, the following two approximate methods work well in practice:
\begin{compactenum}
	\item \textbf{Best Path Decoding}. $ h(\vec x) \approx \mathcal{B}(\pi^*) \quad \text{where} \quad \pi^* = \argmax_{\pi \in L'^T} p(\pi \mid \vec x)$. 
	
	\item \textbf{Prefix Search Decoding}.
\end{compactenum}

The authors end up using neither of the above, but rather a heuristic approach:
\vspace{-0.5em}
\begin{quote}
	{\footnotesize\itshape
		We divide the output sequence into sections that are very likely to begin and end with a blank. We do this by choosing boundary points where the probability of observing a blank label is above a certain threshold. We then calculate the most probable labelling for each section individually and concatenate these to get the final classification.}
\end{quote}

\myspace
\p \blue{The CTC Forward-Backward Algorithm} (4.1). Define the total probability of the first $s$ output labels, $\slice[s]{\ell}$, given the first $t$ inputs as 
\graybox{
	\alpha_t(s) 
	&\triangleq  p(\slice[s]{\ell} \mid \slice[t]{x}) \\
	&= \sum_{ \substack{ \pi \in L'^T \\  \mathcal{B}(  \slice[t]{\pi}  ){=}\slice[s]{\ell} } }
	\prod_{ t' = 1 }^{t} y^{t'}_{\pi_{t'}}
}
Note that the summation here could contain duplicate $\slice[t]{\pi}$ that differ only in their elements beyond $t$. \\

\p We insert a blank token at the beginning and end of $\vec\ell$ and between each pair of labels, and call this augmented sequence $\vec{\ell}'$. We have the following rules for initializing $\alpha$ at the first intput step $t{=}1$, followed by the recursion rule:
\begin{align}
\alpha_1(s) &= \begin{cases}
y_{\epsilon}^1 & s{=}1 \\
y_{ \vec[1]{\ell} }^1 & s{=}2 \\
0 & s > 2
\end{cases}\\
\alpha_t(s)
&= \begin{cases}
\bar{\alpha}_t(s) y_{\vec[s]{\ell}' }^t 	& \vec[s]{\ell}' {=} b ~ \text{or} ~ \vec[s-2]{\ell}'  \\
( \bar{\alpha}_t(s) + \alpha_{t-1}(s -2) ) y_{\vec[s]{\ell}' }^t  							& \text{otherwise}
\end{cases}\\
\bar{\alpha}_t(s)
&\triangleq \alpha_{t-1}(s) + \alpha_{t-1}(s - 1)
\end{align}
\textbf{Interpretation}:
\begin{compactitem}
	\item \textit{Initialization}: Given the first input $x_1$, the only valid paths\footnote{which remember are always the same length as x} that could potentially result in the final labeling $\vec\ell$ are $\{\epsilon \}$ and $\{\ell_1\}$.  These respectively correspond to the augmented labelings $\{\epsilon\}$ and $\{\epsilon, \ell_1\}$. This is what the authors are referring to when they say ``We allow all prefixes to start with either a blank or the first symbol in $\vec\ell$.''
	
	\item \textit{Case 1}. $\bar{\alpha}_t(s) $ corresponds to a right arrow ([s, t-1] $\rightarrow$ [s, t]) and a right-down-one arrow ([s-1, t-1] $\rightarrow$ [s, t]) (see explanations below). 
	
	\item \textit{Case 2}. $\bar{\alpha}_t(s) + \alpha_{t-1}(s -2) )$ corresponds to the 2 arrows from case 1 and additional a right-down-two arrow ([s-2, t-1] $\rightarrow$ [s, t]). 
\end{compactitem}

\textbf{Reading the lattice arrow diagrams}. The usage of the augmented label sequence $\vec{\ell}'$ can make reading these very confusing. Here's how to read them:
\begin{compactitem}
	\item \textit{Rows}: row $s$ corresponds to $\ell'_s$. 
	\item \textit{Columns}: column $t$ corresponds to $\vec[t]{x}$. 
	\item \textit{Arrows/Traversal}: this was the confusing part for me. The arrows correspond to a transition in a given \textit{path}. Each arrow direction has a different interpretation:
	\begin{compactitem}
		\item right: the path either had a repeated character (which would've been merged into a single output label) or a repeated blank token (if the row is a blank token) which would've been removed entirely in the final output label sequence. 
		\item right-down-one: transition from either label-to-blank or blank-to-label. 
		\item right-down-two: direct transition between two unique characters. 
	\end{compactitem}
\end{compactitem}

Ok, I finally get it now. In my opinion, introducing $\vec{\ell}'$ as the ``augmented label sequence'' is confusing/misleading/unnecessary/etc. It is literally just introduced so we can meaningfully talk about transitions between elements of a given path $\pi$. The traversal being done in the $\alpha$ formulas is referencing the valid \textit{paths}, NOT the final labels (at least directly). Especially confusing is when the distill article says crap like ``can't jump over $z_{s-1}$'' which is just total nonsense -- no one is jumping over anything! Literally all they mean is ``transition between unique characters in a path''. Lost so many hours confused over the wording in the distill article, which only served to confuse me further (just read the paper).  


\begin{comment}
It's worth emphasizing how to interpret these, given we've imposed this weird augmented label sequence. In as-verbose-as-possible terms, 
\begin{quote}
{\bfseries
$\alpha_t(s)$ is the probability, after running our RNN for $t$ time steps to produce the path $\slice[t]{\pi}$, that $\mathcal{B}(\slice[t]{\pi}){==}\slice[ \frac{s - 1}{2} ]{\ell}$  for which, after inserting $\epsilon$ between all elements of $\slice[ \frac{s-1}{2} ]{\ell}$, we obtain the augmented labeling $\slice[s]{\ell}'$.
}
\end{quote}

\p The way you should think about the different possible cases here is that, at time step $t$, in order for there to be nonzero probability that we can merge the sequence of $t$ RNN outputs into the augmented label sequence $\slice[s]{\ell}'$, it must be true that:
\begin{compactitem}
\item We emit the token $\ell'_s$ at time $t$ from the RNN. 

\item At the previous timestep, $t  - 1$, we emitted a token consistent with our rules for merging combined with the fact that we've inserted $\epsilon$ between every pair of tokens in the final output labeling $\vec\ell$, in order to produce $\vec{\ell}'$. 
\end{compactitem}
The weird case (in my opinion) to consider is realizing that we can emit, for example, the label $a$ at time $t - 1$, then the label $b$ at time $t$, and this would eventually get mapped to a portion of the augmented label sequence, $[a, \epsilon, b]$. 
\end{comment}

The final probability of the label sequence $\vec\ell$ given input sequence $\vec x$ is thus:
\graybox{
	p(\vec\ell \mid \vec x) &= \alpha_T(|\vec{\ell}'|) + \alpha_T(|\vec{\ell}'|-1) 
}
which  corresponds to ``total probability of all paths ending in a blank token plus total probability of all paths ending in the final label of $\vec\ell$.''




\myspace
\subsub{Sequence Modeling With CTC}
\myspace

Notes on \href{https://distill.pub/2017/ctc/}{this distill article} (note that I do NOT recommending reading this article -- the wording is horrendously confusing compared to what's actually going on). 

\bluesec{Introduction}. CTC is an approach for mapping input sequences $X = \{x_1, \ldots, x_T\}$ to label sequences $Y = \{y_1, \ldots, y_U \}$, where the lengths may vary ($T \ne U$). 


\bluesec{Alignment}. An \green{alignment} between input sequence $X$ and label sequence $Y$ is a function $f: X \mapsto Y$. Take for example the label sequence $Y = \{ h, e, l, l, o\}$ and some input sequence (e.g. raw audio) $X = \{x_1, \ldots, x_{12}\}$. CTC places the following constraints:
\begin{compactenum}
	\item It must be the same length as the input sequence $X$. 
	
	\item It has the same vocabulary as $Y$, plus an additional token $\epsilon$ to denote blanks. 
	
	\item At position $i$, it either (a) repeats the aligned token at $i-1$, (b) assigns the empty token $\epsilon$, or (c) assigns the next letter of the label sequence. 
\end{compactenum} 
For our example, we could have an aligned sequence $A = \{h, h, e, \epsilon, \epsilon, l, l, l, \epsilon, l, l, o\}$. Then we apply the following two steps (can interpret as functions) to map from $A$ to $Y$:
\begin{compactenum}
	\item Merge any repeated [contiguous] characters. 
	
	\item Remove any $\epsilon$ tokens. 
\end{compactenum}

\begin{example}[Number of Valid Alignments]
	{\itshape\small Given $X$ of length $T$ and $Y$ of length $U \le T$ (and no repeating letters), how many valid alignments exist?}
	\tcblower
	
	The differences between alignments fall under two categories:
	\begin{compactenum}
		\item Indices where we transition from one label to the next. 
		\item Indices where we insert the blank token, $\epsilon$. 
	\end{compactenum}
	\vspace{1em}
	
	Stated even simpler, the alignments differ first and foremost by \textit{which elements of $X$ are ``extra'' tokens}, where I'm using ``extra'' to mean either blank or repeat token. Given a set of $T$ tokens, there are $\tbinom{T}{T - U}$ different ways to assign $T - U$ of them as ``extra.'' The tricky part is that we can't just randomly decide to repeat or insert a blank, since a sequence of one or more blanks is \textit{always} followed by a transition to next letter, by definition. And remember, we have defined $Y$ to have no repeated [contiguous] labels. \\
	
	Apparently, the answer is $\tbinom{T + U}{ T - U}$ total valid alignments. 
\end{example}


\bluesec{Loss Function}. When you hear someone say ``the CTC loss,'' they usually mean ``MLE using a CTC posterior.'' In other words, there is no ``CTC loss'' function, but rather there is the standard maximum likelihood objective, but we use a particular form for the posterior $p(Y \mid X)$ over possible output labels $Y$ given raw input sequence $X$:
\graybox{
	p(Y \mid X)
	&= \sum_{\mathcal A \in \mathcal{A}_{X, Y}} \prod_{t=1}^T p_t(a_t \mid X)
}
where $\mathcal A$ is one of the valid alignments from $\mathcal{A}_{X, Y}$. The value of $a_t$ obeys the set of three constraints listed above.

How can we compute the loss efficiently? Let $\vec z \triangleq [\epsilon, y_1, \epsilon, y_2, \ldots, \epsilon, y_U, \epsilon]$, and let $\alpha$ denote the \red{score of the merged alignments} at a given node [in the \red{CTC lattice}]. We compute the forward probabilities $\alpha_t(s)$, defined as the probability of arriving at [prefix of] augmented label sequence $\slice[s]{z}$ given unmerged alignments up to input step $t$:
$$
\alpha_t(s) 
\triangleq p(\slice[s]{z} \mid \slice[t]{x})
$$


\myfig[0.8\textwidth]{figs/ctc_distill.png}

The key insight is that we can compute $\alpha_t$ as long as we know $\alpha_{t-1}$. There are two cases to consider.
\begin{compactenum}
	\item  (1.1) \textbf{$z_s$ is the blank token $\epsilon$.} At the previous RNN output (time $t - 1$), we could've emitted either a blank token $\epsilon$ or the previous token in the augmented label sequence, $z_{s-1}$. In other words, 
	\begin{align}
	\alpha_t(s) 
	&= p(z_s \eq \epsilon \mid  x_t) \cdot
	\left(   \alpha_{t - 1}(s) + \alpha_{t - 1}(s - 1) \right) 
	\end{align}
	
	\item (1.2) \textbf{$z_s$ is the same label as at step $s - 2$}. This occurs when $Y$ has repeated labels next to each other. 
	\begin{align}
	\alpha_t(s) 
	&= p(z_s \eq z_{s-2} \mid x_t) 
	\cdot \left(   \alpha_{t - 1}(s) + \alpha_{t - 1}(s - 1)   \right)
	\end{align}
	In this situation, $\alpha_{t-1}(s)$ corresponds to us just emitting the same token as we did at $t - 1$ or emitting a blank token $\epsilon$, and $\alpha_{t-1}(s - 1)$ corresponds to a transition to/from $\epsilon$ and a label. 
	
	\item (2) \textbf{$z_{s-1}$ is the blank token $\epsilon$ between unique characters.} In addition to the two $\alpha_{t-1}$ terms from before, we now also must consider the possibility that our RNN emitted $z_{s-2}$ at the previous time ($t-1$) and then emitted $z_s$ immediately after at time $t$.  
	\begin{align}
	\alpha_t(s)
	&= p(z_s \mid x_t) \cdot \left(
	\alpha_{t-1}(s-2) + \alpha_{t-1}(s-1) + \alpha_{t-1}(s)
	\right)
	\end{align}
\end{compactenum}





% ===========================================================================================
\lecture{Optimization}{Large-Scale Study of Curiosity Driven Learning}{January 05, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Burda et al., ``Large-Scale Study of Curiosity Driven Learning,'' OpenAI and UC Berkeley (Aug 2018).}

An agent sees observation $x_t$, takes action $a_t$, and transitions to the next state with observation $x_{t + 1}$. Goal: incentivize agent with reward $r_t$ relating to how informative the transition was. The main components in what follows are:
\begin{compactitem}
	\item Observation embedding $\phi(x)$. 
	\item Forward dynamics network for prediction $P(\phi(x_{t+1} \mid x_t, a_t)$. 
	\item Exploration reward (\textit{surprisal}):
	\graybox{
		r_t 
		&= -\log p\left( \phi\left(x_{t+1}\right) \mid x_t, a_t \right)	
	}
\end{compactitem}
The authors choose to model the next state embedding with a Gaussian,
\begin{align}
\phi(x_{t+1}) \mid x_t, a_t
&\sim \mathcal{N}(f(x_t, a_t), \epsilon ) \\
r_t 
&= ||f(x_t, a_t) - \phi(x_{t+1})||_2^2
\end{align}
where $f$ is the learned dynamics model. 

\myspace 
\p \blue{Feature spaces} (2.1). Some possible ways to define $\phi$:
\begin{compactitem}
	\item \textbf{Pixels}: $\phi(x) = x$.
	
	\item \textbf{Random Features}: Literally feeding $\phi(x) = ConvNet(x)$ where ConvNet is \textit{randomly initialized} and fixed. 
	
	\item \textbf{VAE}: Use the mapping to the mean [of the approximated distribution] as the embedding network $\phi$. 
\end{compactitem}


\myspace
\p \blue{Interpretation}. It seems that this works because after awhile, it is boring and predictable to take actions that result in losing a game. The most surprising actions seem to be those that advance us forward, to new and uncharted territory. However, these experiments are all on games that have a very "linear" uni-directional-like sequence of success. I wonder how successful this would be in a game like rocket league, where there is no tight coupling of direction with success and novelties (e.g. moving forward in mario bros). 



% ===========================================================================================
\lecture{Optimization}{The Marginal Value of Adaptive Gradient Methods in Machine Learning}{March 10, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Wilson et al., ``The Marginal Value of Adaptive Gradient Methods in Machine Learning,'' (May 2018).}


\p \blue{TL;DR}. For simple overparameterized problems, adaptive methods (a) often find drastically different solutions than SGD, and (b) tend to give undue influence to spurious features that have no effect on out-of-sample generalization. They also found that tuning the initial learning and decay scheme for Adam yields significant improvements over its default settings in all cases.

\myspace
\p \blue{Background} (2). The gradient updates for general stochastic gradient, stochastic momentum, and adaptive gradient methods, respectively, can be formalized as follows\footnote{I'm defining $\Delta w_{k + 1} \triangleq w_{k + 1} - w_k$.}
\begin{align}
\mtgreen{[regular]} 	\quad	\Delta w_{k + 1} 
&= - \alpha_k \widetilde{\nabla} f(w_k) \\
\mtgreen{[momentum]} \quad	\Delta w_{k + 1} 
&= - \alpha_k \widetilde{\nabla} f\left( w_k + \gamma_k   \Delta w_k    \right)
+ \beta_k \Delta w_k \\
\mtgreen{[adaptive]} \quad \Delta w_{k + 1}
&= - \alpha_k \matr[k]{H}^{-1} \widetilde{\nabla} f\left( w_k + \gamma_k  \Delta w_k    \right)
+ \beta_k \matr[k]{H}^{-1}  \matr[k - 1]{H} \Delta w_k
\end{align}
where $H_k$ is a p.d. matrix involving the entire sequence of iterates $(w_1, \ldots w_k$). For example, regular momentum would be $\gamma_k{=}0$, and Nesterov momentum would be $\gamma_k{=}\beta_k$. In practice, we basically always define $H_k$ as the diagonal matrix:
\begin{align}
H_k := \text{diag}  \left[
\left(
\sum_{i=1}^k \eta_i \vec[i]{g} \odot  \vec[i]{g}
\right)^{1/2}
\right]
\end{align}

\myspace
\p \blue{The Potential Perils of Adaptivity} (3). Consider the binary least-squares classification problem, where we aim to minimize
\begin{align}
R_s[w] &:= \onehalf ||Xw - y||_2^2
\end{align}
where $X \in \R^{n \times d}$ and $y \in \{-1, 1\}^{n}$. 

\begin{definition}[-1em][Lemma 3.1]
	If there exists a scalar $c$ s.t. $X \text{sign}(X^T y) = cy$, then (assuming $w_0 := 0$) AdaGrad, Adam, and RMSProp all converge to the unique solution $w \propto \text{sign}(X^T y)$.
\end{definition}







% ======================================================================
\lecture{Optimization}{Deep Double Descent}{January 25, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Nakkiran et al., ``Deep Double Descent: Where Bigger Models and More Data Hurt'' \textit{OpenAI}, (Dec 2019).}


\bluesec{TL;DR}: For a variety of DL tasks, increasing model size (or training epochs) first leads to \textit{worse} performance, and then gets better. Define a new complexity measure called \green{effective model complexity}. 
\vspace{-0.5em}
\begin{quote}
	{\small\itshape
		Informally, our intuition is that for model-sizes at the interpolation threshold, there is effectively only one model that fits the train data and this interpolating model is very sensitive to noise in the train set and/or model mis-specification. That is, since the model is just barely able to fit the train data, forcing it to fit even slightly-noisy or mis-specified labels will destroy its global structure, and result in high test error.
	}
\end{quote}

\bluesec{Introduction} (1). Bias-variance tradeoff suggests that increasing model complexity results in lower bias and higher variance. After a certain threshold [of increasing model complexity], conventional wisdom says that the model will ``overfit'' with the variance term dominating the test error. Therefore, increasing the complexity beyond this threshould \textit{should} merely result in a larger variance term and thus worse MSE (i.e. \textit{larger models are worse} [after a certain point]). Conventioanal wisdom also tells us that \textit{more data is always better} [for improving the test MSE]. 

\bluesec{Results} (2). Define a \green{training procedure} $\mathcal T$ to be any procedure that takes as input a training set $S = \{(x_i, y_i)\}_{i=1}^{n}$ and outputs a classifier $\mathcal T(S) : x \mapsto y$. 

\begin{definition}[-1em][Effective Model Complexity]
	The \green{EMC} of $\mathcal T$, wrt distribution $\mathcal D$ and $\epsilon > 0$, is defined as:
	\begin{align}
	\mathrm{EMC}_{\mathcal D, \epsilon}(\mathcal T) 
	&:= \max\{   n \mid \E[S \sim \mathcal{D}^n]{\mathrm{Error}_S(\mathcal T (S))  } \le \epsilon \}
	\end{align}
	where $\mathrm{Error}_S(M)$ is the mean error of model $M$ on train samples $S$.
\end{definition}
In other words, the EMC of $\mathcal T$ is the maximum number of training samples for which $\mathcal T$ gets [on average] zero \textit{training error}. The authors then hypothesize the 3 following regimes (assuming $n$ training examples):

\newcommand{\emc}{\mathrm{EMC}_{\mathcal D, \epsilon}(\mathcal T)}
\begin{compactitem}
	\item \textbf{Under-parameterized}: $\emc << n$. Any $\delta \mathcal T$ resulting in larger $\emc$ will \textit{decrease} test error.
	
	\item \textbf{Over-parameterized}: $\emc >> n$. (same as above).
	
	\item \textbf{Critically parameterized}: $\emc \approx n$. Any $\delta \mathcal T$ resulting in larger $\emc$ may \textit{decrease or increase} test error.
\end{compactitem}







% ======================================================================
\lecture{Optimization}{A Tutorial on Bayesian Optimization of Expensive Cost Functions}{October 02, 2020}
% ======================================================================

\citepaper{Brochu, Eric
	Cora, Vlad M.
	de Freitas, Nando}{
	A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning
}{}{Dec 2010}

\bluesec{The Bayesian Optimization Approach} (2). Goal will to be \textit{maximize} a real-valued function $f$:
\begin{align}
	\vec{x}^* 
		&= \argmax_{\vec x} f(\vec x) \\
	|| f(\vec[1]{x}) - f( \vec[2]{x}  ) || 
		&\le C || \vec[1]{x} - \vec[2]{x} ||
		\quad \forall \vec[1]{x}, \vec[2]{x} \in \mathcal A
\end{align}
where the second line states our assumption that $f$ is \green{Lipschitz-continuous}. Bayesian optimization uses a prior $p(f)$ and evidence $p(\mathcal{D)}$ to define a posterior distribution over functions given evidence. The optimization follows the principle of \textit{maximum expected utility}, with expectation taken wrt the posterior. The utility function will be referred to as the \green{acquisition function}. 


\bluesec{Priors over Functions} (2.1). By far the most common choice for the prior over functions is the \green{Gaussian process prior}. A GP is completely specified by its mean and covariance function.
\begin{myquote}
	A GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution.
	\begin{align}
		f(\vec x) \sim \mathcal{GP}\lr{
			m(\vec x), k(\vec x, \vec x')	
	}
	\end{align}
\end{myquote}
Here, we'll use the zero mean function. A common choice for the kernel is the \green{squared exponential function}:
\begin{align}
	k(\vec[i]{x}, \vec[j]{x})
		&= \exp \{  -\onehalf || \vec[i]{x} - \vec[j]{x} ||^2 \}
\end{align}
Now say we observed some data $\mathcal{D}_{1:t}$ of queries/outputs to/from $f$. Before writing out the predictive distribution on the next observation $f_{t+1}$ condition on the previous observations, we denote the joint Gaussian in a convenient form:
\begin{align}
	\begin{bmatrix}
		\vec[1:t]{f} \\
		f_{t+1}
	\end{bmatrix}
	\sim 
	\Gauss{\vec 0,   
		\begin{bmatrix}
			\matr K & \vec k \\
			\vec{k}^T & k(\vec[t+1]{x}, \vec[t+1]{x}) 
		\end{bmatrix}
	}
\end{align}
where $k_i = k(\vec[t+1]{x}, \vec[i]{x})$.  

\Needspace{10\baselineskip}
Given some next query $\vec[t+1]{x}$, the predictive distribution is thus:\marginnote{Remember, this assumes mean zero!}
\graybox{
	P( f_{t+1} \mid \mathcal{D}_{1:t}, \vec[t+1]{x}  )
		&= \mathcal{N}\lr{
			\mu_t(\vec[t+1]{x}), \sigma_t^2(\vec[t+1]{x})
		} \\
	\mu_t(\vec[t+1]{x})
		&= \vec{k}^T \minv{K} \vec[1:t]{f} \\
	\sigma_t^2(\vec[t+1]{x})
		&= k(\vec[t+1]{x}, \vec[t+1]{x}) -  \vec{k}^T \minv{K} \vec k 
}
For a proof, see section 4.3.4.3 (page 120) of Murphy et al.

\bluesec{Choice of Covariance Functions} (2.2). The choice of $k$ determines the smoothness properties of samples drawn from the GP. It is more common to incorporate additional \textit{hyperparameters} $\theta$ into the covariance function, e.g.:
\begin{align}
	k(\vec[i]{x}, \vec[j]{x})
		&= \exp \lr {
			- \onehalf (\vec[i]{x} - \vec[j]{x})^T \diag{ \vec \theta}^{-2} (\vec[i]{x} - \vec[j]{x})
		}
\end{align}
Typically the values of $\theta$ are learned by ``seeding'' with a few random samples and maximizing the log-likelihood of the evidence $\mathcal D$ given $\theta$. It is also common to have a scalar hyperparameter applied to $k$ to control the magnitude of the variance.

\bluesec{Acquisition Functions for Bayesian Optimization} (2.3). Typically, acquisition functions are defined such that high acquisition corresponds to potentially high values of the objective function, whether because the prediction is high, the uncertainty is great, or both. One approach, referred to as \green{probability of improvement} over the incumbent (current best) $f(\vec{x}^+)$, where $\vec{x}^+ = \argmax_{\vec[i]{x} \in \slice[t]{x}} f(\vec[i]{x})$, defines acquisition function
\graybox{
	PI(\vec x)
		&= \Prob{ f(\vec x) \ge f(\vec{x}^+) }
		= \Phi \lr{
			\frac{  \mu(\vec x) - f(\vec{x}^+)  }{    \sigma(\vec x)  }	
	}
}
where $\mu$ and $\sigma$ refer to the current predictive posterior defined earlier, and $\Phi$ is the normal cumulative distribution. Note how this is a pure exploitation strategy. We can add a trade-off parameter $\xi \ge 0$: 
\graybox{
	PI(\vec x)
	&= \Prob{ f(\vec x) \ge f(\vec{x}^+) + \xi }
	= \Phi \lr{
		\frac{  \mu(\vec x) - f(\vec{x}^+)  - \xi }{    \sigma(\vec x)  }	
	}
}
which has the effect of subtracting $\xi/\sigma$ in the input of the cumulative distribution function -- higher uncertainty points are now given relatively better chances of being selected. 

\Needspace{10\baselineskip}
Notice one drawback of PI is that it doesn't tell us anything about the potential \textit{magnitude} of improvement over the incumbent. In theory, we want to choose the next input for which the expected output is closest to the true maximum $f(\vec{x}^*)$:
\begin{align}
	\vec[t+1]{x}
		&= \argmin_{\vec x} \E{ ||  f_{t+1}(\vec x) - f(\vec{x}^*)  || \mid \mathcal{D}_{1:t} } \\ 
		&= \argmin_{\vec x} \int ||  f_{t+1}(\vec x) - f(\vec{x}^*)  || P(f_{t+1} \mid \mathcal{D}_{1:t}) \mathrm{d}f_{t+1}
\end{align}
However, this also has a notable drawback. What if, for example, there a point $\vec[t+1]{x}$ that wasn't expected to evaluate close to $f^*$, but for which dramatically improved the expected deviation from $f^*$ at step $t+2$? In other words, the formulation above suffers from it being a one-step-ahead approach. An alternative formulation based on improving over the current incumbent $f(\vec{x}^+)$ (instead of being based on deviation from true optimum $f(\vec{x}^*)$ is \green{expected improvement}:
\graybox{
	I(\vec x) 
		&= \max\{  0,    f_{t+1}(\vec x) - f(\vec{x}^+)  \} \\
	\vec[t+1]{x}
		&= \argmax_{\vec x} \E{ I(\vec x) } \\
		&= \argmax_{\vec x} \int_{I=0}^{I=\infty} I(\vec x) \mathcal{N}\left( f(\vec{x}^+) + I(\vec x) \mid \mu_t(\vec x) , \sigma^2_t(\vec x)  \right) \mathrm{d}I \\
		&= \argmax_{\vec x} \lr{  \mu_t(\vec x) - f(\vec{x}+)   } \Phi(Z) + \sigma_t(\vec x) \phi(Z) \\
	Z &= \frac{  \mu_t(\vec x) - f(\vec{x}^+) }{ \sigma_t(\vec x)   } 
}
and note that $EI(\vec x)$ is zero when $\sigma_t(\vec x)$ is zero.  Intuition check: to me, it seems that PI(x) gives us the probability sum (marginal) from I=0, to $\infty$ (same integral as EI), whereas $EI(x)$ gives us an expectation over that distribution (instead of just the flat sum). 

\bluesec{Maximizing the Acquisition Function} (2.3.4). Authors use the \green{DIRECT} algorithm to find the maxima of the acquisition function. It is a deterministic, derivative-free optimizer. It uses the existing samples of the objective function to decide how to proceed to DIvide the feasible space into finer RECTangles.

% ---------------
\subsub{DIRECT}
% ---------------
\myspace
\citepaper{Jones et al.}{Lipschitzian Optimization Without the Lipschitz Constant}{General Motors R\&D}{Oct 1993}

\bluesec{Lipschitzian Optimization in One Dimension} (2). This section introduces Shubert's algorithm and its primary shortcomings.
\begin{align}
	|f(x) - f(x')| 
		&\leq K |x - x'| \quad \forall x, x' \in [\ell, u]  \label{eq:direct1}
\end{align}

Let $a, b$ be two points inside $[\ell, u]$. Substitute for $x'$ for each of them to obtain:
\begin{align}
	f(x) \geq f(a) - K(x -a) \\
	f(x) \geq f(b) + K(x - b) 
\end{align}
for $x \in [a, b]$. This gives us a lower bound for $f$ in the interval [a, b], as illustrated below. 
\myfig[0.5\textwidth]{figs/direct_fig1.png}

Since we know (re: are assuming) $f$ is Lipschitz $K$, the true function value must lie above the ``V'' in the figure. Accordingly, the lowest possible function value in the interval $[a, b]$ occurs at the bottom of the V. I'll denote this point as $x_{min}$ with $f_{min} \triangleq f(x_{min})$:
\graybox{
	x_{min} 
		&= \frac{a+b}{2} + \frac{ f(a) - f(b) }{2K} \\
	f_{min}
		&= \frac{  f(a) + f(b) }{2} - K(b - a)
}

\bluesec{DIRECT Algorithm in One Dimension}. Modification of Shubert's alg. First, instead of requiring the function to be evaluated at all $2^n$ vertices of the $n$-dimensional [hypercube] search space, we evaluate at the center point. This requires the following modifications to the lower-bound formulations from the previous section. Let $[a, b]$ be interval with center $c = (a + b)/2$. Substitute $x' := c$ in eq \ref{eq:direct1}. Remember that, here, we are interesting in minimizing $f$, and thus we want to find inequalities of the form $f(x) \geq \ldots$. The two relevant inequalities (out of the possible variations due to the absolute values) are thus
\begin{align}
	f(x) 
		&\geq f(c) + K(x - c) \quad \forall x \leq c \\
	f(x)
		&\geq f(c) - K(x -c) \quad \forall x \geq c
\end{align}
which gives us an inverted v lower bound as illustrated below. 
\myfig[0.5\textwidth]{figs/direct_fig3.png}

with lower bound occurring at the endpoints now:
\graybox{
	\mtext{lower bound} &= f(c) - K(b -a)  / 2 \label{eq:direct-lb}
}

\begin{myquote}[-1em]
	[DIRECT] will partition the space into intervals whose center points are evaluated and will select intervals based on the lower bound given in eq \ref{eq:direct-lb}] \textellipsis the interval is divided into thirds, and the function is evaluated at the center points of the left and right thirds. The original center point simply becomes the center of a smaller interval.
\end{myquote}

\myfig[0.4\textwidth]{figs/direct_fig4.png}

The figure above shows us how we subdivide a \textit{given} interval with center point. 

\Needspace{15\baselineskip}
Next we need to define how, given a set of $m$ intervals $[a_i, b_i], i = 1, \ldots, m$, the DIRECT algorithm selects an interval for further sampling. The figure below plots interval $[a_i, b_i]$ as a point $(x, y) = (b_i - a_i)/2, f(c_i))$. 

\myfig[0.4\textwidth]{figs/direct_fig5.png}

\begin{compactitem}
	\item The horizontal coordinate is the distance from the interval's center to its vertices. It captures the goodness of the interval with respect to global search, that is, goodness based on the amount of unexplored territory in the interval. 
	
	\item The vertical coordinate is the value of the function at the interval's center. It captures the goodness of the interval with respect to local search, that is, goodness based on known function values. 
	
	\item If one passes a line with slope K through any dot in this diagram, the vertical intercept will be the lower bound for the corresponding interval. Hence, 
	the interval with the lowest lower bound can be found by positioning a line with slope K below the cloud of dots and shifting it upwards until it first touches a dot.
\end{compactitem}

Lastly, since the selection criteria so far depends heavily on $K$, DIRECT will instead effectively use \textit{all possible values for $K$}. This just means selecting intervals corresponding to dots on the lower right parts of the convex hull of the cloud of dots, as illustrated below. 

\myfig[0.4\textwidth]{figs/direct_fig6.png}






% ======================================================================
\lecture{Optimization}{Practical Bayesian Optimization of Machine Learning Algorithms}{September 22, 2020}
% ======================================================================

\citepaper{P. Snoek, H. Larochelle, and R. Adams}{
	Practical Bayesian Optimization of Machine Learning Algorithms
}{University of Toronto}{Aug 2012}


\bluesec{Introduction} (1). For some continuous function $f$, BO assumes that f was sampled from a \green{Gaussian Process} (GP) and maintains a posterior distribution for $f$ given observations. Here, the observations are the measure of generalization performance\footnote{I believe the observations are always the output value of $f$, to be clear.} for an associated setting of the input parameters to $f$. In order to choose/propose the next set of parameters to feed to $f$, we can do one of the following popular approaches:
\begin{compactitem}
	\item Optimize the \green{expected improvement} (EI) over the current best result \red{TODO}: formal definition
	
	\item Optimize the GP \green{upper confidence bound} (UCB) \red{TODO}: formal definition.
\end{compactitem}
Main contributions of this paper:
\begin{compactitem}
	\item Identification of good practices for BO of machine learning algorithms.
	
	\item Argue that a fully Bayesian treatment of GP kernel params is of critical importance. 
	
	\item Description of new algorithm that accounts for cost in experiments. 
	
	\item Propose algorithm that can take advantage of multiple cores in parallel.
\end{compactitem}

\bluesec{Bayesian Optimization with GP Priors} (2). Two major choices when performing BO:
\begin{compactenum}
	\item Select a prior over functions that will express assumptions about $f$. 
	\item Choose an \green{acquisition function}, which is used to construct a utility function from the model posterior, allowing us to determine the next point to evaluate. 
\end{compactenum}
For choice (1), we use the GP prior. For (2), first denote the acquisition function $a : \mathcal X \rightarrow \R^+$ that basically gives a score for any input $\vec x$, such that we can get a proposal for the next input $\vec[next]{x}$ to feed to $f$ via $\vec[next]{x} := \argmax_{\vec x} a(\vec x)$. Note that $a(\vec x)$ is shorthand for $a(\vec x; \{\vec[n]{x}, y_n\}, \theta\})$ where $\theta$ are the GP hyperparameters (\red{?}) (same goes for predictive $\mu$ and $\sigma$ below). 


\Needspace{15\baselineskip}
\underline{Acquisition functions}\footnote{Note that we are trying to minimize $f$ in this context.}\marginnote{Impr $\equiv$ Improvement}[4em]
\graybox{
	\mtgreen{[Prob of Impr.]}\quad 
		a_{PI}(\vec x)
			&= \Phi(\gamma(\vec x)) 
		\quad
			\gamma(\vec x) = \frac{f(\vec[best]{x}) - \mu(\vec x)}{\sigma(\vec x)} \\
	\mtgreen{[Expected Impr.]}\quad
		a_{EI}(\vec x)
			&= \sigma(\vec x) \lr{ \gamma\lr{\vec x}  \Phi\lr{  \gamma\lr{ \vec x } } + \mathcal{N}\lr{  \gamma(\vec x) ; 0, 1}  } \\
	\mtgreen{[UCB]}\quad 
		a_{LCB}\lr{ \vec x } 
			&= \mu(\vec x) - \kappa \sigma(\vec x)
}
with a tunable $\kappa$ to balance exploitation and exploration\footnote{$a_{LCB}$ exploits lower confidence bounds (upper, when considering maximization) to construct acquisition functions that minimize regret over the course of their optimization.}.

\underline{Covariance functions}: Below are the \green{automatic relevance determination} (ARD) squared exponential and Mat\'{e}rn 5/2 kernels:
\graybox{
	K_{SE}(\vec x, \vec x')
		&= \theta_0 \exp \left\{    -\onehalf r^2(\vec x, \vec x')   \right\}
	 \qquad
		r^2(\vec x, \vec x') = \sum_{d=1}^D (x_d - x_d')^2 / \theta_d^2 \\
	K_{M52}(\vec x, \vec x')
		&= \theta_0 \lr{   1  + \sqrt{5 r^2(\vec x, \vec x') }  + \frac{5}{3} r^2(\vec x, \vec x') } \exp \left\{ -  \sqrt{5 r^2(\vec x, \vec x'  }    \right\}
}

\underline{GP Hyperparameters}:
\begin{compactitem}
	\item Length scales $\theta_{1:D}$
	\item Covariance amplitude $\theta_0$
	\item Observation noise $\nu$
	\item Constant mean $m$ \red{(?)}
\end{compactitem}
We can either use a point estimate of these parameters by optimizing the marginal likelihood under the GP:
\begin{align}
	\argmax_{\theta, \nu, m} p(\vec{y} \mid \{ \vec[n]{x} \}_{n=1}^N , \theta , \nu, m) 
		&=\argmax_{\theta, \nu, m}  \mathcal{N}(\vec y \mid m \vec 1, \matr[\theta]{\Sigma} + \nu \matr I)
\end{align}
or the [fully-Bayesian] marginalization via the \green{integrated acquisition function}:
\begin{align}
	\hat{a} (\vec x)
		&= \int a(\vec x) p(\theta \mid \{\vec[n]{x}, y_n\}) \mathrm{d} \theta 
\end{align}

\underline{Monte Carlo Acquisition for parallelizing BO}. Consider situation where $N$ evaluations have completed, yielding data $\{ \vec[n]{x}, y_n \}_{n=1}^{N}$, and $J$ evaluations are pending at locations $\{ \vec[j]{x} \}_{j=1}^{J}$ (running on different threads). Ideally, we'd choose $\vec[next]{x}$ based on 
\begin{align}
	\hat{a}\lr{ \vec x ; \{ \vec[j]{x} \}}
		&= \int_{\R^J} a(\vec x; \{\vec[j]{x}, y_j\}) p( y_{1:J} \mid \slice[J]{x} ) \mathrm{d}y_1 \ldots \mathrm{d}y_J
\end{align}
Instead, since we don't have access to all the pending evaluations, we compute Monte Carlo estimates to approximate the above integral.


% ------------------------------------
\myspace 
\subsub{Review of Gaussian Processes} 
\myspace
% ------------------------------------


\begin{example}[Gaussian Process Review]
	A GP is defined as a collection of random variables $f(\vec x)$\footnote{We also refer to $f(\vec x)$ as a real process. Don't get too wrapped up in thinking about how one samples from a space of functions (yet) -- we haven't gotten to that part! For now, just interpret this as a regular collection of random variables.}, any finite number of which have a joint Gaussian distribution. A GP is completely specified by mean function $m(\vec x)$ and covariance function $k(\vec x, \vec{x}')$. 
	\begin{align}
		m(\vec x) 
			&= \E{f(\vec x)} \\
		k(\vec x, \vec x')
			&= \E{ \lr{  f(\vec x) - m(\vec x) }   \lr{ f(\vec x')  - m(\vec x') }  }
	\end{align}
	Again, without getting too wrapped up in the ``meaning'', we \textit{define} the notation (which at this point is still just a placeholder basically) for representing a GP:
	\begin{align}
		f(\vec x) \sim \mathcal{GP}\lr{  m(\vec x), k(\vec x, \vec x') }
	\end{align}

	\textbf{Key Insight}: if we don't know the distribution that $f$ is sampled from\footnote{An example where we \textit{do} know is if we let e.g. $f(x) \triangleq \phi(x)^T w$, with feature map $\phi$ and weights $w \sim \mathcal{N}(0, \sigma_w)$ -- crucially that $w$ is the only source of randomness and is drawn from a Gaussian. In this case, we can just directly compute the exact formula for $k(\vec x, \vec x')$ and are basically done.} we can instead \textit{define} how our covariance function should look, which itself \textit{implies} a distribution over functions $f$ (!!!). 
	
	Consider the widely popular choice: the \green{squared exponential} covariance function:
	\begin{align}
		k(\vec[p]{x}, \vec[q]{x})
			&\triangleq \exp\lr{ - \onehalf | \vec[p]{x} - \vec[q]{x} |^2 }
	\end{align}
	It can be shown that, by defining our covariance function as above, we are implicitly modeling $f$ as being sampled from a Bayesian linear regression model with an infinite number of basis (feature) functions (see prev footnote). 
\end{example}

\bluesec{Sampling Procedure}. Let's assume we are using the squared exponential covariance function from the above review box. We'll also assume $m(\vec x) = \vec 0$. \textbf{Sampling from the GP prior} works as follows:

\Needspace{18\baselineskip}
\begin{compactitem}
	\item Draw a set of samples over input space $\{\vec[i]{x}\}_{i=1}^N$ with each $\vec[i]{x} \in \R^d$. 
	\item Compute the $N \times N$ covariance matrix $\matr K$, where $K_{ij} := \exp\lr{ -\onehalf |  \vec[i]{x} - \vec[j]{x} |^2 }$. 
	\item Our GP prior for $f$ is now well-defined as a multivariate Gaussian over $N$ dimensions (NOT $d$)\footnote{Notice that the functions we sample from this prior are only defined at each of the $N$ query inputs, so we are really only getting pointwise defined functions\textellipsis}. 
	\item We can sample functions $f_*$ from our prior -- these will \textit{each} be random Gaussian vectors of $N$ dimensions with interpretation that the $i$th element equals the sampled $f_*$ at input $\vec[i]{x}$. For example, to sample a function $f(\vec x)$ from the prior:
	\begin{compactenum}
		\item Compute the \green{Cholesky decomposition} $\matr L$ of the symmetric psd covariance matrix $\matr K = \matr L \matr{L}^T$. 
		
		\item Sample $N$ times from the standard normal to get $\vec u \sim \mathcal{N}(0, I)$. 
		
		\item Compute $f(\cdot) = \vec m +  \matr L \vec u$\footnote{Our prior mean function is $\vec m = \vec 0$.}.
	\end{compactenum}
\end{compactitem}
Now, how do incorporate observations and sample from the posterior? Let's pretend that we live in infinite-computation land for a moment, and that we've collected all the possible training inputs $\matr[tr]{X} = \{\vec[i]{x}^{tr}\}_{i=1}^{N_{tr}}$ and [a disjoint set of] test inputs  $\matr[test]{X} = \{\vec[i]{x}^{test}\}_{i=1}^{N_{test}}$. 
\begin{compactenum}
	\item Compute each element of the \textit{combined} $N \times N$ (where $N = N_{tr} + N_{test}$) squared-exponential covariance matrix to obtain a full (massive) GP prior \footnote{Same as before -- nothing new here except how I've organized/defined the sampled input sets)}. 
	
	\item Evaluate $f(\vec[i]{x}^{tr})$ for all $\vec[i]{x} \in \matr[tr]{X}$, resulting in a vector $\vec f \in \R^{N_{tr}}$of training outputs. 
	
	\item Condition our GP prior to obtain the corresponding posterior $\vec[*]{f} \mid \matr[test]{X}, \matr[tr]{X}, \vec{f}$, over function evaluations $\vec[*]{f}$ (outputs) on the test set $\matr[test]{X}$.
\end{compactenum}


\bluesec{Math/other} \red{TODO} reorganize section. The \green{Choleskey decomposition} of a symmetric psd matrix $\matr A$ decomposes $\matr A$ into a product of a lower triangular matrix $\matr L$ and its transpose:
\begin{align}
	\matr{L} \matr{L}^T &= \matr A
\end{align}
where $\matr L$ is called the \purple{Cholesky factor}. This decomposition is useful for solving linear systems ($\matr A \vec x = \vec b$ for vector $\vec x$) with symmetric psd coefficient matrix $\matr A$. 




% ======================================================================
\lecture{Optimization}{Dragonfly}{September 23, 2020}
% ======================================================================

\citepaper{Kandasamy et al.}{
	Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly
}{CMU}{Apr 2020}

\bluesec{Introduction} (1). Given black-box function $f : \mathcal{X} \mapsto \R,$ we wish to find its optimum using [as few as possible] repeated evaluations to $f$. Methods aim to determine the next point $x_t$ for evaluation, given knowledge of $f$ acquired via previous query-observation pairs $\{(x_i, y_i)\}_{i=1}^{t-1}$. They do this by taking the argmax of an \green{acquisition function} $\varphi_t : \mathcal{X} \mapsto \R$:
\begin{align}
	x_t \leftarrow \argmax_{x \in \mathcal X} \varphi_t(x)
\end{align}


\bluesec{Review of Gaussian Processes and Bayesian Optimization} (2). A \green{Gaussian Process} (GP) is characterized by a mean function $\mu : \mathcal X \mapsto \R$ and a kernel (covariance function) $\kappa : \mathcal{X}^2 \mapsto \R$. Suppose we are given $n$ observations $\mathcal{D}_n = \{(x_i, y_i)\}_{i=1}^n$ from this GP, where $y_i = f(x_i) + \epsilon_i \in \R$. Then the \textit{posterior} $f{|}\mathcal{D}_n$ is also a GP with mean $\mu_n$ and covariance $\kappa_n$:
\graybox{
	\mu_n(x) 
	&= k^T \lr{ K + \eta^2 I_n }^{-1} Y \\
	\kappa_n(x, x')
	&= \kappa(x, x') - k^T  \lr{ K + \eta^2 I_n }^{-1} k'
}
\begin{compactitem}
	\item $Y \in R^n$ is a vector with $Y_i = y_i$
	\item $k_i = \kappa(x, x_i) $ 
	\item $k'_i = \kappa(x', x_i)$
	\item Gram matrix $K \in \R^{n \times n}$ is given by $K_{ij} = \kappa (x_i, x_j)$. 
\end{compactitem}
\vspace{1em}

\begin{itemdefinition}{Bayesian Optimization}{A suite of methods for black box optimisation in the Bayesian paradigm.}
	\item Use a prior belief distribution for $f$.
	
	\item To determine the next point $x_t$ for evaluation, construct an acquisition function $\varphi_t : \mathcal X \mapsto \R$ which captures the utility of performing an evaluation at $x$ according to the posterior $f$ conditioned on past evaluations.
\end{itemdefinition}

One popular acquisition function is the \green{Gaussian process upper confidence bound} (GP-UCB)\footnote{Upper confidence bound for $f$.}:
\begin{align}
	\varphi_t (x)
	&= \mu_{t-1}(x) + \sqrt{\beta_t} \sigma_{t-1}(x)
\end{align}
where the posterior standard deviation $\sigma_{t-1}$ is the uncertainty associated with our current estimate of $f$ (after t-1 observations). 
\begin{compactitem}
	\item The $\mu_{t-1}$ term encourages an \textit{exploitative strategy}: we want to query regions where we already believe $f$ is high. 
	
	\item $\sigma_{t-1}$ encourages an \textit{exploratory strategy}: we want to query where we are uncertain about $f$lest we miss high valued regions which have not been queried yet. 
	
	\item $\beta_t$ controls the trade-off between exploration and exploitation.
\end{compactitem}

\bluesec{Choice of Acquisition} (4.1). Authors employ an adaptive sampling strategy which chooses different acquisition functions at different iterations instead of attempting to pick a single best one. 

\bluesec{GP Hyperparameters} (4.2). Authors either employ maximium likelihood or use posterior sampling at each iteration to obtain a set of GP hyperparameters. 


\bluesec{BO Implementation in Dragonfly} (5). 
\begin{compactitem}
	\item \textbf{Domains} of variables. Support Euclidean, integral, discrete and neural network variables. 
	
	\item \textbf{Kernels}. Squared exponential and Mat\'{e}rn kernels. 
	
	\item \textbf{Optimising the Acquisition}. To maximize $\varphi_t$ in purely Euclidean spaces, they use \purple{DiRect} or \purple{PDOO}. For all other cases they perform the following evolutionary algorithm:
	\begin{compactenum}
		\item Begin with initial pool of randomly chosen points in the domain (the random seed queries). Evaluate the acquisition on these queries. 
		\item Generate a set of $N_{mut}$ mutations of this pool. 
		\begin{compactenum}
			\item Stochastically select $N_{mut}$ candidates from this set such that those with higher $\varphi_t$ (acq) values are more likely to be selected. 
			\item Apply a mutation operator to each candidate.
		\end{compactenum}
		\item Evaluate $\varphi_t$ on these $N_{mut}$ mutations. 
		\item Add them to the initial pool. 
		\item Repeat from some prescribed number of steps. 
	\end{compactenum}
	
	\item \textbf{Initialisation}. Bootstrap the BO routine with $n_{init}$ evaluations. For Euclidean/integral vars, these points are chosen via \purple{latin hypercube sampling}. Discrete vars are sampled uniformly at random.
\end{compactitem}


\begin{algorithm}[BO in Dragonfly with M asynchronous workers]
	Let $n_{init}$ be the number of random initial query points to evaluate on. 
	
	\begin{compactitem}
		\item $\mathcal{D}_0 \leftarrow$ evaluate $f$ at $n_{init}$ points.
		
		\item $y_{max} \leftarrow$ maximum $y$ value in $\mathcal{D}_0$. 
		
		\item Repeat the following for some number of iterations (ok how many tho?)
		\begin{compactitem}
			\item Wait for a worker to finish. 
			
			\item $\mathcal{D}_t \leftarrow \mathcal{D}_{t-1} \cup \{(q, y)\}$ where $(q, y)$ are the worker's previous query and observation. 
			
			\item $\mu_{t-1} \leftarrow $ compute posterior GP mean given $\mathcal{D}_j$ using $\theta$.  
			
			\item Compute hallucinated posterior (necessary for simple first impl?)
			
			\item $q' \leftarrow$ determine next query for evaluation using acquisition $\alpha$ and GP $\mathcal{GP}_{t-1}$. 
			
			\item Re-deploy worker with an evaluation at $q'$. 
		\end{compactitem}
	\end{compactitem}
\end{algorithm}






% ======================================================================
\lecture{Optimization}{Decoupled Weight Decay Regularization}{June 04, 2020}
% ======================================================================

\citepaper{I. Loshchilov and F. Hutter}{Decoupled Weight Decay Regularization}{University of Freiburg}{Jan 2019}


\bluesec{Decoupling the Weight Decay from the Gradient-Based Update} (2). Denote learning rate as $\alpha$, rate of weight decay as $\lambda$, and batch-averaged loss at step $t$ as $f_t$. Let $f_t^{reg}(\theta) = f_t(\theta) + \frac{\lambda'}{2} || \theta||_2^2$ denote the loss function with $L2$ regularization, where $\lambda' = \frac{\lambda}{\alpha}$. The formal distinction between \green{weight decay} and \green{L2 regularization} is as follows:
\begin{align}
	\mtgreen{[weight decay]}
	&\qquad \theta_{t+1}
	= (1 - \lambda) \theta_t - \alpha \nabla f_t(\theta_t) \\
	\mtgreen{[L2 reg]}
	&\qquad \theta_{t+1}
	= \theta_t - \alpha \nabla f_t^{reg}(\theta_t)
\end{align}

\begin{definition}[-1em][Proposition 1]
	Weight decay (with $\lambda$) is identical to L2 regularization (with $\lambda' = \frac{\lambda}{\alpha}$) for standard SGD.
\end{definition}

The key realization here is that the optimal value of $\alpha$ may change differently than the optimal weight decay value $\lambda$. The L2 regularization formula, however, has the weight decay strongly coupled with the learning rate, since $\lambda' = \frac{\lambda}{\alpha}$ (a requirement for Proposition 1 to hold). The formula for weight decay above was proposed for decoupling this effect. The exact change to AdamW is shown below. 

\myfig[0.7\textwidth]{figs/adamw_alg2.png}










































% ==================================================================================
% PAPERS AND TUTORIALS
% ==================================================================================
\mysection{Miscellaneous}\label{Miscellaneous}




% ============================================================================================
\lecture{Miscellaneous}{Neural Style}{January 22}

\p \blue{Notation}. 
\begin{compactitem}
	\item \textbf{Content image}: $\vec{p}$
	\item \textbf{Filter responses}: the matrix $P^l \in \mathcal{R}^{N_l \times M_l}$ contains the activations of the filters in layer $l$, where $P_{ij}^l$ would give the activation of the $i$th filter at position $j$ in layer $l$. $N_l$ is number of feature maps, each of size $M_l$ (height $\times$ width of the feature map)\footnote{If not clear, $M_l$ is a scalar, for any given value of $l$.}.
	\item \textbf{Reconstructed image}: $\vec{x}$ (initially random noise). Denote its corresponding filter response matrix at layer $l$ as $P^l$. 
\end{compactitem}

\myspace
\p \blue{Content Reconstruction}. 
\begin{enumerate}
	\item Feed in \green{content image} $~\vec{p}~$ into pre-trained network, saving any desired filter responses during the forward pass. These are interpreted as the various ``encodings'' of the image done by the network. Think of them analogously to ``ground-truth'' labels.
	
	\item Define $\vec x$ as the \green{generated image}, which we first initialize to random noise. We will be changing the pixels of $\vec x$ via gradient descent updates. 
	
	\item Define the \green{loss function}. After each forward pass, evaluate with squared-error loss between the two representations at the layer of interest:
	\graybox{
		\mathcal{L}_{content}(\vec p, \vec x, l) &= \onehalf \sum_{i, j} (F_{ij}^l - P_{ij}^l)^2 \tlab{1} \\
		\pderiv{\mathcal L_{content}}{F_{ij}^l} &= 
			\begin{cases}
			(F^l - P^l)_{ij} & F_{ij}^l > 0 \\
			0 & F_{ij}^l < 0
		\end{cases}\tlab{2}
		}
	where it appears we are assuming ReLU activations (\red{?}).
	
	\item Compute iterative updates to $\vec x$ via \green{gradient descent} until it generates the same response in a certain layer of the CNN as the original image $\vec p$. 
\end{enumerate}

\myspace
\p \blue{Style Representation}. On top of the CNN responses in each layer, the authors built a style representation that computes the correlations between the different [aforementioned] filter responses. The correlation matrix for layer $l$ is denoted in the standard way with a Gram matrix $G^l \in \mathcal{R}^{N_l \times N_l}$, with entries
\begin{align}
G_{ij}^l &= \langle F^l_i, F_j^l \rangle =  \sum_k F_{ik}^l F_{jk}^l \tlab{3}
\end{align}
To generate a texture that matches the style of a given image, do the following.
\begin{enumerate}
	\item Let $\vec a$ denote the original [style] image, with corresponding $A^l$. Let $\vec x$, initialized to random noise, denote the generated [style] image, with corresponding $G^l$.
	
	\item The contribution to the loss of layer $l$, denoted $E_l$, to the total loss, denoted $\mathcal L_{style}$, is given by
	\graybox{
		E_l &= \frac{1}{4 N_l^2 M_l^2} \sum_{ij} (G_{ij}^l - A_{ij}^l)^2 \tlab{4} \\
		\mathcal L_{style}(\vec a, \vec x) &= \sum_{l = 0}^L w_l E_l \tlab{5} \\
		\pderiv{ E_{l}}{F_{ij}^l} &= 
		\begin{cases}
			\frac{1}{N_l^2 M_l^2}	\left( (F^l)^T (G^l - A^l) \right)_{ji} & F_{ij}^l > 0 \\
			0 & F_{ij}^l < 0
		\end{cases}\tlab{6}
	}
	where $w_l$ are [as of yet unspecified] weighting factors of the contribution of layer $l$ to the total loss. 
\end{enumerate}

\myspace
\p \blue{Mixing content with style}. Essentially a joint minimization that combines the previous two main ideas. 
\begin{enumerate}
	\item Begin with the following images: white noise $\vec x$, content image $\vec p$, and style image $\vec a$. 
	
	\item The loss function to minimize is a linear combination of ~\ref{1} and ~\ref{5}:
	\graybox{
		\mathcal L_{total}(\vec p, \vec a, \vec x, l) 
			&= \alpha \mathcal L_{content}(\vec p, \vec x, l) 
			+ \beta \mathcal L_{style}(\vec a, \vec x) \tlab{7}
		}
	Note that we can choose which layers $L_{style}$ uses by tweaking the layer weights $w_l$. For example, the authors chose to set $w_l = 1/5$ for 'conv[1, 2, 4, 5]\_1' and 0 otherwise. For the ratio $\alpha/\beta$, they explored $1 \times 10^{-3}$ and $1 \times 10^{-4}$.
\end{enumerate}








% ============================================================================================
% ============================================================================================
\lecture{Miscellaneous}{TextRank}{May 03}
% ============================================================================================
% ============================================================================================

\myspace
\p \blue{Introduction}. A graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.\marginnote{\textbf{Semantic graph}: one whose structure encodes meaning between the nodes (semantic elements).}[-2em] \green{TextRank} is a graph-based ranking model for graphs extracted from natural language texts. The authors investigate/evaluate TextRank on unsupervised keyword and sentence extraction.

\myspace
\p \blue{The \sout{TextRank} PageRank Model}. In general [graph-based ranking], a vertex can be ranked based on certain properties such as the number of vertices pointing to it (in-degree), how highly-ranked \textit{those} vertices are, etc. Formally, the authors [of \textit{PageRank}] define the score of a vertex $V_i$ as follows:\marginnote{The factor $d$ is usually set to 0.85.}[2em]

\graybox{
	S(V_i) &= (1 - d) + d * \sum_{V_j \in \textrm{In}(V_i)} \frac{1}{\mid \textrm{Out}(V_i) \mid} S(V_j) 
	\quad \text{where} \quad 
	d \in \Re[0, 1]} \label{textrank-formula}
and the damping factor $d$ is interpreted as the probability of jumping from a given vertex\footnote{Note that $d$ is a single parameter for the graph, i.e. it is the same for all vertices.} to another \underline{random} vertex in the graph. In practice, the algorithm is implemented through the following steps:
\begin{compactitem}
	\item[(1)] Initialize all vertices with arbitrary values.\footnote{The authors do not specify what they mean by arbitrary. What range? What sampling distribution? Arbitrary as in uniformly random? \red{EDIT}: The authors claim that the vertex values upon completion are not affected by the choice of initial value. Investigate!}
	
	\item[(2)] Iterate over vertices, computing equation~\ref{textrank-formula} until convergence [of the error rate] below a predefined threshold. The error-rate, defined as the difference between the "true score" and the score computed at iteration $k$, $S^k(V_i)$, is \textit{approximated} as:
	\begin{align}
		\textrm{Error}^k(V_i) \approx S^{k}(V_i) - S^{k - 1}(V_i)
	\end{align}
\end{compactitem}

\myspace 
\p \blue{Weighted Graphs}. In contrast with the PageRank model, here we are concerned with natural language texts, which may include multiple or partial links between the units (vertices). The authors hypothesize that modifying equation~\ref{textrank-formula} to incorporate \textit{weighted} connections may be useful for NLP applications.\marginnote{$w_{ij}$ denotes the connection between vertices $V_i$ and $V_j$.}[2em]

\graybox{
	\textcolor{OliveGreen}{W}S(V_i) &= (1 - d) + d * \sum_{j \in \textrm{In}(V_i)}
		\textcolor{OliveGreen}{
			 \frac{w_{ji}}{ \sum{V_k \in \textrm{Out}(V_j)} w_{jk} }
			W }
	S(V_j) \label{textrank-formula-2}
}
where I've shown the modified part in green. The authors mention they set all weights to random values in the interval 0-10 (no explanation). 

\myspace
\p \blue{Text as a Graph}. In general, the application of graph-based ranking algorithms to natural language texts consists of the following main steps:
\begin{compactitem}
	\item[(1)] Identify text units that best define the task at hand, and add them as vertices in the graph.
	
	\item[(2)] Identify relations that connect such text unit in order to draw edges between vertices in the graph. Edges can be directed or undirected, weighted or unweighted.
	
	\item[(3)] Iterate the algorithm until convergence.\vspace{0.3em}
	
	\item[(4)] Sort [in reversed order] vertices based on final score. Use the values attached to each vertex for ranking/selection decisions.
\end{compactitem}

\myspace\Needspace{30\baselineskip}
\subsub{Keyword Extraction}
\myspace

\p \blue{Graph}. The authors apply TextRank to extract words/phrases that are representative for a given document. The individual graph components are defined as follows:
\begin{compactitem}[-]
	\item \green{Vertex}: sequence of one or more lexical units from the text.
	\begin{compactitem}
		\item In addition, we can restrict which vertices are added to the graph with \underline{syntactic filters}.
		\item Best filter [for the authors]: \textit{nouns and adjectives only}.
	\end{compactitem}
	\item \green{Edge}: two vertices are connected if their corresponding lexical units co-occur within a window of $N$ words\marginnote{Typically $N \in \Z[2, 10]$}[-1em]\footnote{That is \textellipsis  simpler than expected. \textit{Can we do better?}}.
\end{compactitem}

\myspace\Needspace{15\baselineskip}
\p \blue{Procedure}:
\begin{compactitem}
	\item[(1)] \textbf{Pre-Processing}: Tokenize and annotate [with POS] the text.\vspace{0.3em}
	
	\item[(2)] \textbf{Build the Graph}: Add all [single] words to the graph that pass the syntactic filter, and connect [undirected/unweighted] edges as defined earlier (co-occurrence).
	
	\item[(3)] \textbf{Run algorithm}: Initialize all scores to 1. For a convergence threshold of 0.0001, usually takes about 20-30 iterations.
	
	\item[(4)] \textbf{Post-Processing}: 
	\begin{compactitem}
		\item[(i)] Keep the top $T$ vertices (by score), where the authors chose $T = |V|/3$.\footnote{Another approach is to have $T$ be a fixed value, where typically $5 < T < 20$.} Remember that vertices are still individual words.
		
		\item[(ii)] From the new subset of $T$ keywords, collapse any that were adjacent in the original text in a single lexical unit.
	\end{compactitem}
\end{compactitem}

\myspace
\p \blue{Evaluation}. The data set used is a collection of 500 abstracts, each with a set of keywords. Results are evaluated using \green{precision}, \green{recall}, and \green{F-measure}\footnote{
	Brief terminology review:
	\begin{compactitem}
		\item \green{Precision}: fraction of keywords extracted that are in the "true" set of keywords.
		\item \green{Recall}: fraction of "true" keywords that are in the extracted set of keywords.\marginnote{A PR Curve plots precision as a function of recall.}[-2em]
		\item \green{F-score}: combining precision and recall to get a single number for evaluation:
		$$
		F = \frac{2pr}{p + r}
		$$
	\end{compactitem}
	}. % end footnote
The best results were obtained with a co-occurrence window of 2 [on an undirected graph], which yielded:
$$
\text{Precision: } 31.2\% \quad \text{Recall: } 43.1\% \quad \text{F-measure: } 36.2
$$
The authors found that larger window size corresponded with lower precision, and that directed graphs performed worse than undirected graphs.

\myspace
\subsub{Sentence Extraction} 
\myspace 

\p \blue{Graph}. Now we move to ``sentence extraction for automatic summarization.''
\begin{compactitem}
	\item \green{Vertex}: a vertex is added to the graph for each sentence in the text.
	\item \green{Edge}: each weighted edge represents the similarity between two sentences. The authors use the following similarity measure between two sentences $S_i$ and $S_j$:
	\graybox{
		\textrm{Similarity}(S_i, S_j) &= 
			\frac{\mid S_i \cap S_j \mid}{\log(\mid S_i \mid) + \log(\mid S_j \mid)}
		}
		where the numerator is the number of words that occur in both $S_i$ and $S_j$. 
\end{compactitem}
The \blue{procedure} is identical to the algorithm described for keyword extraction, except we run it on full sentences.

\myspace
\p \blue{Evaluation}. The data set used is 567 news articles. For each article, TextRank generates a 100-word summary (i.e. they set $T = 100$). They evaluate with the \href{http://www.isi.edu/licensed-sw/see/rouge/}{\textsc{Rouge}} evaluation toolkit (Ngram statistics). 




% ============================================================================================
\lecture{Miscellaneous}{Survey of Text Clustering Algorithms}{July 03, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Aggarwal et al., ``A Survey of Text Clustering Algorithms,'' (2012).}

\p \blue{Introduction}. The unique characteristics for clustering \textit{text}, as opposed to more traditional (numeric) clustering, are (1) large dimensionality but highly sparse data, (2) words are typically highly correlated, meaning the number of principal components is much smaller than the feature space, and (3) the number of words per document can vary, so we must normalize appropriately.\\

\p Common types of clustering algorithms include agglomerative clustering algorithms, partitioning algorithms, and standard parametric modeling based methods such as the EM-algorithm.

\p \blue{Feature Selection}. 
\begin{compactitem}
	\item \green{Document Frequency-Based}. Using document frequency to filter \textit{out} irrelevant features. Dealing with certain words, like ``the'', should probably be taken a step further and simply removed (stop words). 
	
	\item \green{Term Strength}. A more aggressive technique for stop-word removal. It's used to measure how informative a word/term $t$ is for identifying two related documents, $x$ and $y$. Denoted $s(t)$, it is defined as:\marginnote{See ref 94 of the paper for more.}
	\begin{align}
		s(t) &= \Prob{t \in y \mid t \in x}
	\end{align}
	So, how do we know $x$ and $y$ are related to begin with? One way is a user-defined cosine similarity threshold. Say we gather a set of such \textit{pairs} and randomly identify one of the pair as the ``first'' document of the pair, then we can approximate $s(t)$ as
	\begin{align}
		s(t) &= \frac{ \text{Num pairs in which t occurs in both}   }{ \text{Num pairs in which t occurs in the first of the pair}  }
	\end{align}
	
	\begin{quote}
		\textit{{\small In order to prune features, the term strength may be compared to the expected strength of a term which is randomly distributed in the training documents with the same frequency. If the term strength of t is not at least two standard deviations greater than that of the random word, then it is removed from the collection.}}
	\end{quote}
	
	\item \green{Entropy-Based Ranking}. The quality of a term is measured by the entropy reduction when it is removed [from the collection]. The entropy $E(t)$ of term $t$ in a collection of $n$ documents is:
	\begin{align}
		E(t) &= - \insum \jnsum \left( S_{ij} \cdot \log(S_{ij}) 
		+ (1 - S_{ij}) \cdot \log(1 - S_{ij})\right)\\
		S_{ij} &= 2^{-d_{ij} / \bar d }
	\end{align}
	where
	\begin{compactitem}
		\item $S_{ij} \in (0, 1)$ is the similarity between doc $i$ and $j$.
		\item $d_{ij}$ is the distance between $i$ and $j$ after the term $t$ is removed
		\item $\bar d$ is the average distance between the documents after the term $t$ is removed.
	\end{compactitem} 
\end{compactitem}

\myspace 
\p \blue{LSI-based Methods}. Latent Semantic Indexing\marginnote{See ref 28. of paper for more on LSI.} is based on dimensionality reduction where the new (transformed) features are a linear combination of the originals. This helps magnify the semantic effects in the underlying data. LSI is quite similar to PCA\footnote{The difference between LSI and PCA is that PCA subtracts out the means, which destroys the sparseness of the design matrix.}, except that we use an approximation of the covariance matrix $C$ which is appropriate for the sparse and high-dimensional nature of text data.\\

\p Let $\matr{A} \in \R^{n \times d}$ be term-document matrix, where $\matr[i, j]{A}$ is the (normalized) frequency for term $j$ in document $i$. Then $\matr{A}^T \matr{A} = n \cdot \Sigma$ is the (scaled) approximation to covariance matrix\footnote{Approximation because it is based on our training data, not on true expectations over the underlying data-distribution.}, assuming the data is mean-centered. Quick check/reminder:
\begin{align}
	(\matr{A}^T \matr{A})_{ij} &= \matr[:, i]{A}^T \matr[:, j]{A} \triangleq \vec[i]{a}^T \vec[j]{a} \\
	&\approx n \cdot \E{\rvec[i]{a} \rvec[j]{a}}
\end{align}
where the expectation is technically over the underlying data distribution, which gives e.g. $P(a_i = x)$, the probability the $i$th word in our vocabulary having frequency $x$. Apparently, since the data is sparse, we don't have to worry much about it actually being mean-centered (\red{why?}). \\

\p As usual, we using the eigenvectors of $\matr{A}^T \matr{A}$ with the largest variance in order to represent the text\footnote{In typical collections, only about 300 to 400 eigenvectors are required for the representation.}. In addition:
\begin{quote}
	\textit{{\small One excellent characteristic of LSI is that the truncation of the dimensions
			removes the noise effects of synonymy and polysemy, and the similarity computations are more closely affected by the semantic concepts in the data. }}
\end{quote}

\myspace
\p \blue{Non-negative Matrix Factorization}. Another latent-space method (like LSI), but particularly suitable for clustering. The main characteristics of the NMF scheme:
\begin{compactitem}
	\item In LSI, the new basis system consists of a set of orthonormal vectors. This is \textit{not} the case for NMF.
	
	\item In NMF, the vectors in the basis system \textbf{\underline{directly correspond to cluster topics.}} Therefore, the cluster membership for a document may be determined by examining the largest component of the document along any of the [basis] vectors.
\end{compactitem}
\p Assume we want to create $k$ clusters, using our $n$ documents and vocabulary size $d$. The goal of NMF is to find matrices $\matr{U} \in \R^{n \times k}$ and $\matr{V} \in \R^{d \times k}$ that minimize:\marginnote{$$u_{ij} \ge 0$$ $$v_{ij} \ge 0$$}[2em]
\graybox{
	J &= \frac{1}{2} || \matr{A} - \matr{U} \matr{V}^T ||_F^2 \\
	&= \frac{1}{2} \left(
		tr(\matr{A} \matr{A}^T)
		- 2 tr(\matr{A} \matr{V} \matr{U}^T)
		+ tr(\matr{U} \matr{V}^T \matr{V} \matr{U}^T)
	\right)
	}
Note that the columns of $\matr{V}$ provide the $k$ basis vectors which correspond to the $k$ different clusters. We can interpret this as trying to factorize $\matr{A} \approx \matr{U} \matr{V}^T$. For each row, $\vec{a}$, of $\matr{A}$ (document vector), this is 
\begin{align}
	\vec{a} &\approx \vec{u} \cdot \matr{V}^T \\
	&= \sum_{i = 1}^{k} \vec[i]{u} \matr[i]{V}^T
\end{align}
Therefore, the document vector $\vec{a}$ can be rewritten as an approximate linear (non-negative) combination of the basis vector which corresponds to the $k$ columns of $\matr{V}^T$. \\

\p Langrange-multiplier stuff: Our optimization problem can be solved using the Lagrange method.
\begin{compactitem}
	\item Variables to optimize: All elements of both $\matr{U} = [u_{ij}]$ and $\matr{V} = [v_{ij}]$
	\item Constraint: non-negativity, i.e. $\forall i,j$, $u_{ij} \ge 0$ and $v_{ij} \ge 0$. 
	\item Multipliers: Denote as \underline{matrices} $\alpha$ and $\beta$, with same dimensions as $\matr{U}$ and $\matr{V}$, respectively. 
	\item Lagrangian: I'll just show it here first, and then explain in this footnote\footnote{
		 Recall that in Lagrangian minimization, $\mathcal{L}$ takes the form of [the-function-to-be-minimized] + $\lambda$ ([constraint-function] - [expected-value-of-constraint-at-optimum]). So the second term is expected to tend toward zero (i.e. critical point) at the optimal values. In our case, since our optimal value is sort-of (\red{?}) at 0 for any value of $u_{ij}$ and/or $v_{ij}$, we just have a sum over [langrange-mult] $\times$ [variable]. 
		}:\marginnote{Any matrix multiplication with a $\cdot$ is just a reminder to think of the matrices as column vectors.}[1em]
	\begin{align}
		\mathcal{L} &= J + tr(\alpha \cdot \matr{U}^T) + tr(\beta \cdot \matr{V}^T) \\
		\text{where} \quad tr(\alpha \cdot \matr{U}^T) 
			&= \insum \alpha_i \cdot \vec[i]{u}
			= \insum \jnsum \alpha_{ij} u_{ij}
	\end{align}
	You should think of $\alpha$ as a column vector of length $n$, and $\matr{U}^T$ as a row vector of length $n$. The reason we prefer $\mathcal{L}$ over just $J$ is because now we have an \textit{unconstrained} optimization problem.
	
	\item Optimization: Set the partials of $\mathcal{L}$ w.r.t both $U$ and $V$ (separately) to zero\footnote{Recall that the Lagrangian consists entirely of traces (re: scalars). Derivatives of traces with respect to matrices output the same dimension as that matrix, and derivatives are taken element-wise as always.}:
	\begin{align}
		\pderiv{\mathcal{L}}{U} &= - \matr{A} \cdot \matr{V} 
			+ \matr{U} \cdot \matr{V}^T \cdot \matr{V}
			+ \matr{\alpha} = 0 \\
		\pderiv{\mathcal{L}}{V} &= - \matr{A}^T \cdot \matr{U} 
		+ \matr{V} \cdot \matr{U}^T \cdot \matr{U}
		+ \matr{\beta} = 0
	\end{align} 
	Since, ultimately, these just say [some matrix] = 0, we can multiply both sides (element-wise) by a constant ($x \times 0 = 0$). \textit{Using}\footnote{i.e. the equations that follow are \textit{not} the KT conditions, they just \textit{use}/exploit them\textellipsis} the \green{Kuhn-Tucker conditions} $\alpha_{ij} \cdot u_{ij} = 0$ and
	$\beta_{ij} \cdot v_{ij} = 0$, we get:
	\begin{align}
		(\matr{A} \cdot \matr{V})_{ij} \cdot u_{ij}
			- (\matr{U} \cdot \matr{V}^T \cdot \matr{V})_{ij} \cdot u_{ij}
			 &= 0 \\
		(\matr{A}^T \cdot \matr{U} )_{ij} \cdot v_{ij}
			- (\matr{V} \cdot \matr{U}^T \cdot \matr{U})_{ij} \cdot v_{ij}
		&= 0
	\end{align}
	
	\item Update rules:
	\graybox{
		u_{ij} &= \frac{  (\matr{A} \cdot \matr{V})_{ij} \cdot u_{ij}  }{  
			(\matr{U} \cdot \matr{V}^T \cdot \matr{V})_{ij}} \\
		v_{ij} &= \frac{ (\matr{A}^T \cdot \matr{U} )_{ij} \cdot v_{ij}  }{ (\matr{V} \cdot \matr{U}^T \cdot \matr{U})_{ij}   }
		}
\end{compactitem}

\myspace\Needspace{15\baselineskip}
\subsub{Distance-based Clustering Algorithms}
\myspace

\p One challenge in clustering short segments of text (e.g., tweets) is that exact keyword matching may not work well. One general strategy for solving this problem is to expand text representation by exploiting related text documents, which is related to smoothing of a document language model in information retrieval\marginnote{See ref. 66 in the paper for computing similarities of short text segments.}[-2em].

\myspace
\p \blue{Agglomerative and Hierarchical Clustering}. ``Agglomerative'' refers to the process of bottom-up clustering to build a tree -- at the bottom are leaves (documents) and internal nodes correspond to the merged groups of clusters. The different methods for merging groups of documents for the different agglomerative methods are as follows:
\begin{compactitem}
	\item \green{Single Linkage Clustering}. Defines similarity between two groups (clusters) of documents as the largest similarity between any pair of documents from these two groups. First, (1) compute all similarity pairs [between documents; ignore cluster labels], then (2) sort in decreasing order, and (3) walk through the list in that order, merging clusters if the pair belong to different clusters. One drawback is \textit{chaining}: the resulting clusters assume transitivity of similarity\footnote{Here, transitivity of similarity means if $A$ is similar to $B$, and $B$ is similar to $C$, then $A$ is similar to $C$. This is not guaranteed by any means for textual similarity, and so we can end up with $A$ and $Z$ in the same cluster, even though they aren't similar at all. }.
	
	\item \green{Group-Average Linkage Clustering}. Similarity between two clusters is the \textit{average} similarity over all unique pairwise combinations of documents from one cluster to the other. One way to speed up this computation with an approximation is to just compute the similarity between the mean vector of either cluster. 
	
	\item \green{Complete Linkage Clustering}. Similarity between two clusters is the \textit{worst-case} similarity between any pair of documents. 
\end{compactitem}

\myspace
\p \blue{Distance-Based Partitioning Algorithms}. 
\begin{compactitem}
	\item \green{K-Medoid Clustering}. Use a set of points from training data as anchors (medoids) around which the clusters are built. Key idea is we are using an optimal set of representative documents \textit{from the original corpus}. The set of $k$ reps is successively improved via randomized inter-changes. In each iteration, we replace a randomly picked rep in the current set of medoids with a randomly picked rep from the collection, if it improves the clustering objective function. This approach is applied until convergence is achieved.\marginnote{K-Medoid isn't great for clustering text, especially short texts.}[-2em]
	
	\item \green{K-Means Clustering}. Successively (1) assigning points to the nearest cluster centroid and then (2) re-computing the centroid of each cluster. Repeat until convergence. Requires typically few iterations (about 5 for many large data sets). Disadvantage: sensitive to initial set of seeds (initial cluster centroids). One method for improving the initial set of seeds is to use some supervision - e.g. initialize with $k$ pre-defined topic vectors (see ref. 4 in paper for more). 
\end{compactitem}

\myspace
\p \blue{Hybrid Approach: Scatter-Gather Method}. Use a hierarchical clustering
algorithm on a sample of the corpus in order to find a robust initial set of seeds\marginnote{Scatter-Gather is discussed in detail in ref. 25 of the paper}. This robust set of seeds is used in conjunction with a standard k-means clustering algorithm in order to determine good clusters.\red{TODO:} resume note-taking; page 19/52 of PDF.

\myspace
% ====================================
\subsub{Probabilistic Document Clustering and Topic Models}
% ====================================
\myspace

\p \blue{Overview}. Primary assumptions in any topic modeling approach:\marginnote{From pg. 31/52 of paper.}
\begin{compactitem}
	\item The $n$ documents in the corpus are assumed to each have a probability of belonging to one of $k$ topics. Denote the probability of document $D_i$ belonging to topic $T_j$ as $\Prob{T_j \mid D_i}$. This allows for \textit{soft cluster membership} in terms of probabilities.
	
	\item Each topic is associated with a probability vector, which quantifies the probability of the different terms in the lexicon for that topic. For example, consider a document that belongs completely to topic $T_j$. We denote the probability of term $t_l$ occurring in that document as $\Prob{t_l \mid T_j}$. 
\end{compactitem}
The two main methods for topic modeling are \green{Probabilistic Latent Semantic Indexing} (PLSA) and \green{Latent Dirichlet Allocation} (LDA).

\myspace
\p \blue{PLSA}. We note that the two aforementioned probabilities, $\Prob{T_j \mid D_i}$ and $\Prob{t_l \mid T_j}$ allow us to calculate $\Prob{t_l \mid D_i}$: the probability that term $t_l$ occurs in some document $D_i$:
\begin{align}
	\Prob{t_l \mid D_i} &= \sum_{j = 1}^{k} \Prob{t_l \mid T_j} \cdot \Prob{T_j \mid D_i} \label{wacka-flocka}
\end{align}
which should be interpreted as a weighted average\footnote{This is actually pretty bad notation, and borderline incorrect. $\Prob{T_j \mid D_i}$ is NOT a conditional probability! It is our prior! It is literally $\Prob{\text{ClusterOf}(D_i) = T_j}$.}. From here, we can generate a $n \times d$ matrix of probabilities. \\


\p Recall that we also have our $n \times d$ term-document matrix $\matr{X}$, where $\matr[i, l]{X}$ gives the number of times term $l$ occurred in document $D_i$. This allows us to do maximum likelihood! Our negative log-likelihood, $J$ can be derived as follows:\marginnote{Interpret $\Prob{\matr{X}}$ as the joint probability of observing the words in our data and with their assoc. frequencies.}[1em]
\begin{align}
	J &= - \log\left(  \Prob{\matr{X}} \right) \\
	&= - \log\left( \prod_{i, l} \Prob{ t_l \mid D_i   }^{ \matr[i, l]{X} } \right) \\
	&= - \sum_{i, l} \matr[i, l]{X} \cdot \log\left(  \Prob{ t_l \mid D_i   } \right) 
\end{align}
and we can plug-in eqn~\ref{wacka-flocka} to for evaluating $\Prob{ t_l \mid D_i}$. We want to optimize the value of $J$, subject to the constraints:
\begin{align}
	(\forall T_j): ~ \sum_l \Prob{t_l \mid T_j} = 1 
	\qquad\quad 
	(\forall D_i): ~ \sum_j \Prob{T_j \mid D_i} = 1
\end{align}
This can be solved with a Lagrangian method, similar to the process for NMF described earlier. See page 33/52 of the paper for details.

\myspace 
\p \blue{Latent Dirichlet Allocation} (LDA). The term-topic probabilities and topic-document probabilities are modeled with a \textit{Dirichlet distribution} as a prior\footnote{LDA is the Bayesian version of PLSI}. Typically preferred over PLSI because \underline{PLSI more prone to overfitting}.

\myspace\myspace\Needspace{25\baselineskip}
% ====================================
\subsub{Online Clustering with Text Streams}
% ====================================
\begin{center}
\linespread{0.5}\tiny Reference List: \myref{3}: A Framework for Clustering Massive Text and Categorical Data Streams; \purple{[112]}: Efficient Streaming Text Clustering; \myref{48}: Bursty feature representation
for clustering text streams; \myref{61}: Clustering Text Data Streams (Liu et al.)
\end{center}
\myspace

\p \blue{Overview}. Maintaining text clusters in real time.\marginnote{See ref. 112 for more on OSKM} One method is the \green{Online Spherical K-Means Algorithm} (OSKM)\footnote{Authors only provide a very brief description, which I'll just copy here:
\vspace{-1em}
\begin{quote}
	{\tiny \textit{This technique
			divides up the incoming stream into small segments, each of which can
			be processed effectively in main memory. A set of k-means iterations
			are applied to each such data segment in order to cluster them. The
			advantage of using a segment-wise approach for clustering is that since
			each segment can be held in main memory, we can process each data
			point multiple times as long as it is held in main memory. In addition,
			the centroids from the previous segment are used in the next iteration
			for clustering purposes. A decay factor is introduced in order to age-
			out the old documents, so that the new documents are considered more
			important from a clustering perspective.} }
\end{quote}}.

\myspace
\p \blue{Condensed Droplets Algorithm}. I'm calling it that because they don't call it anything -- it is the algorithm in \myref{3}. 
\begin{compactitem}
	\item \textbf{Fading function}: $f(t) = 2^{-\lambda \cdot t}$. A time-dependent weight for each data point (text stream). Non-monotonic decreasing; decays uniformly with time. 
	
	\item \textbf{Decay rate}: $\lambda = 1/t_0$. Inverse of the half-life of the data stream.
\end{compactitem}
When a cluster is created by a new point, it is allowed to remain as a trend-setting outlier for at least one half-life. During that period, if at least one more data point arrives, then the cluster becomes an active and mature cluster. If not, the trend-setting outlier is recognized as a true anomaly and is removed from the list of current clusters (\textit{cluster death}). Specifically, this happens when the (weighted) number of points in the [single-point] cluster is 0.5. The same criterion is used to define the death of
mature clusters. The statistics of the data points are referred to as \green{condensed droplets}, which represent the word distributions within a cluster, and can be used in order to compute the similarity of an incoming data point to the cluster. Main idea of algorithm is as follows:
\begin{compactenum}
	\item Initialize empty set of clusters $\mathcal{C} = \{\}$. As new data points arrive, unit clusters
	containing individual data points are created. Once a maximum number k of such clusters have been created, we can begin the process of online cluster maintenance.
	
	\item For a new data point $X$, compute its similarity to each cluster $C_j$, denoted as $S(X, C_j)$.
	\begin{compactitem}[-]
		\item If $S(X, C_{best}) > \text{thresh}_{outlier}$, or if there are no inactive clusters left\footnote{We specify some max allowed number of clusters $k$.},  insert $X$ to the cluster with maximum similarity.
		\item Otherwise, a new cluster is created\footnote{The new cluster replaces the least recently updated inactive cluster.} containing the solitary data point $X$.
	\end{compactitem} 
\end{compactenum}

\myspace
\p \blue{Misc.}
\begin{compactitem}
	\item Mandatory read: reference \myref{61}. Details phrase extraction/\green{topic signatures}. The use of using phrases instead of individual words is referred to as \green{semantic smoothing}. 
	\item For \textit{dynamic} (and more recent) topic modeling, see reference \myref{107} of the paper, titled ``A probabilistic model for online document clustering with application to novelty detection.''
\end{compactitem}


\Needspace{15\baselineskip}
\myspace
\p \blue{Semi-Supervised Clustering}. Useful when we have any prior knowledge about the kinds of clusters available in the underlying data. Some approaches:
\begin{compactitem}
	\item Incorporate this knowledge when seeding the cluster centroids for $k$-means clustering. 
	
	\item Iterative EM approach: unlabeled documents are assigned labels using a naive Bayes approach on the currently labeled documents. These newly labeled documents are then again used for re-training a Bayes
	classifier. Iterate to convergence.
	
	\item Graph-based approach: graph nodes are documents and the edges are similarities between the connected documents (nodes). We can incorporate prior knowledge by adding certain edges between nodes that we know are similar. A \textit{normalized cut algorithm} is then applied to this graph in order to create the final clustering.
\end{compactitem}
We can also use partially supervised methods in conjunction with pre-existing categorical \textit{hierarchies}. 






% ============================================================================================
\lecture{Miscellaneous}{Clustering Massive Text Streams}{July 10, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Aggarwal et al., ``A Framework for Clustering Massive Text and Categorical Data
	Streams,'' (2006).}

\p \blue{Overview}. Authors present an online approach for clustering massive text and categorical data streams with the use of a statistical summarization methodology. First, we will go over the process of storing and maintaining the data structures necessary for the clustering algorithm. Then, we will discuss the differences which arise from using different kinds of data, and the empirical results.

\myspace
\p \blue{Maintaining Cluster Statistics}. The data stream consists of $d$-dimensional records, where each dimension corresponds to the numeric frequency of a given word in the vector space representation. Each data point is weighted by the \green{fading function} $f(t)$, a non-monotonic decreasing function which decays uniformly with time $t$. The authors define the \green{half-life} of a data point (e.g. a tweet) as:
\begin{align}
	t_0 ~~ \text{s.t.} ~~ f(t_0) = \frac{1}{2} f(0)
\end{align}
and, similarly, the \green{decay-rate} as its inverse, $\lambda = 1/t_0$. Thus we have $f(t) = 2^{-\lambda \cdot t}$. \\

\p To achieve greater accuracy in the clustering process, we require a high level of granularity in the underlying data structures. To do this, we will use a process in which condensed clusters of data points are maintained, referred to as \green{cluster droplets}. We define them differently for the case of text and categorical data, beginning with categorical:
\begin{compactitem}
	\item \textbf{Categorical}. A cluster droplet $\mathcal{D}(t, \mathcal{C})$ for a set of
	categorical data points $\mathcal{C}$ at time $t$ is defined as the tuple:
	\begin{align}
		\mathcal{D}(t, \mathcal{C}) \triangleq (\bar{DF2},  \bar{DF1}, n, w(t), l)
	\end{align}
	where
	\begin{compactitem}
		\item Entry $k$ of the vector $\bar{DF2}$ is the (weighted) number of points in cluster $\mathcal{C}$ where the $i$th dimension had value $x$ and the $j$ dimension had value $y$. In other words, all pairwise combinations of values in the categorical vector\footnote{This is intentionally written hand-wavy because I'm really concerned with \textit{text} streams and don't want to give this much space.}. $\sum_{i = 1}^{d} \sum_{j \ne i}^{d} v_i v_j$ entries total\footnote{$v_i$ is the number of values the $i$th categorical dimension can take on.}.
		
		\item Similarly, $\bar{DF1}$ consists of the (weighted) counts that some dimension $i$ took on the value $x$. $\sum_{i = 1}^{d}v_i $ entries total.
		
		\item $w(t)$ is the sum of the weights of the data points at time $t$. 
		
		\item $l$ is the time stamp of the last time a data point was added to the cluster.
	\end{compactitem}
	
	
	\item \textbf{Text}. Can be viewed as an example of a \underline{sparse} numeric data set.  A cluster droplet $\mathcal{D}(t, \mathcal{C})$ for a set of text data points $\mathcal{C}$ at time $t$ is defined as the tuple:
	\begin{align}
	\mathcal{D}(t, \mathcal{C}) \triangleq (\bar{DF2},  \bar{DF1}, n, w(t), l)
	\end{align}
	where
	\begin{compactitem}
		\item $\bar{DF2}$ contains $3 \cdot wb \cdot (wb - 1) / 2$ entries, where $wb$ is the number of \underline{distinct words} \underline{in the \textit{cluster} $\mathcal{C}$}.
		
		\item $\bar{DF1}$ contains $2 \cdot wb$ entries.
		
		\item $n$ is the number of data points in the cluster $\mathcal{C}$. 
	\end{compactitem}
\end{compactitem}

\myspace
\p \blue{Cluster Droplet Maintenance}. 
\begin{compactenum}
	\item We first start of with $k$ trivial clusters (the first $k$ data points that arrived). 
	
	\item When a new point $\bar{X}$ arrives, the cosine similarity to each cluster's $\bar{DF1}$ is computed. 
	
	\item $\bar{X}$ is inserted into the cluster for which this is a maximum, so long as the associated $S(\bar{X}, \bar{DF1}) > \mathrm{thresh}$, a predefined threshold. If not above the threshold \textit{and} some \green{inactive cluster} exists, a new cluster is created containing the solitary point $\bar{X}$, which replaces the inactive cluster. If not above threshold but no inactive clusters, then we just insert it into the max similarity cluster anyway.
	
	\item If $\bar{X}$ was inserted (i.e. didn't replace an inactive cluster), then we need to:
	\begin{compactenum}
		\item Update the statistics to reflect the decay of the data points at the current moment in time\footnote{In other words, the statistics for a cluster do not decay, until a new point is added to it.}. This is done by multiplying entries in the droplet vectors by $2^{-\lambda \cdot (t - l)}$.
		
		\item Add the statistics for each newly arriving data point to the cluster statistics.
	\end{compactenum}
\end{compactenum}





% ============================================================================================
\lecture{Miscellaneous}{Hierarchical Attention Networks}{September 06, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Yang et al., ``Hierarchical Attention Networks for Document Classification.''}

\blue{Overview}. Authors introduce the Hierarchical Attention Network (HAN) that is designed to capture insights regarding (1) the hierarchical structure of documents (words -> sentences -> documents), and (2) the context dependence between words and sentences. The latter is implemented by including two levels of attention mechanisms, one at the word level and one at the sentence level.

\myspace
\p \blue{Hierarchical Attention Networks}. Below is an illustration of the network. The first stage is familiar to sequence to sequence models - a bidirectional encoder for outputting sentence-level representations of a sequence of words. The HAN goes a step further by feeding this another bidirectional encoder for outputting document-level representations for sequences of sentences. 

\myfig[0.4\textwidth]{HAN.png}

The authors choose the GRU as their underlying RNN. For ease of reference, the defining equations of the GRU are shown below:
\begin{align}
h_t &= (1 - z_t) \odot h_{t - 1} + z_t \odot \tilde h_t \\
z_t &= \sigma \left( W_z x_t + U_z h_{t - 1} + b_z \right) \\
\tilde h_t &= \tanh \left( W_h x_t + r_t \odot (U_h h_{t - 1}) + b_h \right) \\
r_t &= \sigma \left( W_r x_t + U_r h_{t - 1} + b_r \right)
\end{align}

\myspace
\p \blue{Hierarchical Attention}. Here I'll overview the main stages of information flow.
\begin{compactenum}
	\item \textbf{Word Encoder}. Let the $t$th word in the $i$th sentence be denoted $w_{it}$. They embed the vectors with a word embedding matrix $W_e$, $x_{it} = W_e w_{it}$, and then feed $x_{it}$ through a bidirectional GRU to ultimately obtain $h_{it} := [\overrightarrow h_{it}; \overleftarrow{h_{it}}]$.
	
	\item \textbf{Word Attention}. Extracts words that are important to the meaning of the sentence and aggregates the representation of these informative words to form a sentence vector. 
	\graybox{
		u_{it} &= \tanh(W_w h_{it} + b_w) \\
		\alpha_{it} &= \frac{\exp(u_{it}^T u_w)}{\sum_t \exp(u_{it}^T u_w)} \\
		s_i &= \sum_t \alpha_{it} h_{it}
	}
	Note the \textit{context vector} $u_w$, which is shared for all words\footnote{To emphasize, there is only a single context vector $u_w$ in the network, period. The subscript just tells us that it is the word-level context vector, to distinguish it from the sentence-level context vector in the later stage.} and randomly initialized and jointly learned during the training process. 
	
	\item \textbf{Sentence Encoder}. Similar to the word encoder, but uses the sentence vectors $s_i$ as the input for the $i$th sentence in the document. Note that the output of this encoder, $h_i$ contains information from the neighboring sentences too (bidirectional) but focuses on sentence $i$. 
	
	\item \textbf{Sentence Attention}. For rewarding sentences that are clues to correctly classify a document. Similar to before, we now use a sentence level context vector $u_s$ to measure the importance of the sentences.
	\graybox{
		u_{i} &= \tanh(W_s h_{i} + b_s) \\
		\alpha_{i} &= \frac{\exp(u_{i}^T u_s)}{\sum_i \exp(u_{i}^T u_s)} \\
		v &= \sum_t \alpha_{i} h_{i}
	}
	where v is the document vector that summarizes all the information of sentences in a document.
\end{compactenum}

As usual, we convert $v$ to a normalized probability vector by feeding through a softmax:
\begin{align}
p = \text{softmax}(W_c v + b_c)
\end{align}

\myspace
\p \blue{Configuration and Training}. Quick overview of some parameters chosen by the authors:
\begin{compactitem}
	\item \textbf{Tokenization}: Stanford CoreNLP. Vocabulary consists of words occurring more than 5 times, all others are replaced with UNK token.
	
	\item \textbf{Word Embeddings}: train word2vec on the training and validation splits. Dimension of 200.
	
	\item \textbf{GRU}. Dimension of 50 (so 100 because bidirectional). 
	
	\item \textbf{Context vectors}. Both $u_w$ and $u_s$ have dimension of 100. 
	
	\item \textbf{Training}: batch size of 64, grouping documents of similar length into a batch. SGD with momentum of 0.9.
\end{compactitem}






% ============================================================================================
\lecture{Miscellaneous}{Neural Architecture Search with Reinforcement Learning}{April 01, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize B. Zoph and Q. Le, ``Neural Architecture Search with Reinforcement Learning,'' (2017).}

\myfig[0.4\textwidth]{figs/nas.png}

\p \blue{Controller RNN}. Generates architectures with a predefined number of layers, which is increased manually as training progresses. At convergence, validation accuracy of the generated network is recorded. Then, the controller parameters $\theta_c$ are optimized to maximized the expected validation accuracy over a batch of generated architectures. 

\myspace
\p \blue{Reinforcement Learning} to learn the controller parameters $\theta_c$. Let $a_{1:T}$ denote a list of actions taken by the controller\footnote{Note that $T$ is not necessarily the number of layers, since a single generated layer can correspond to multiple actions (e.g. stride height, stride width, num filters, etc.).}, which defines a generated architecture. We denote the resulting validation accuracy by $R$, which is the reward signal for our RL task. Concretely, we want our controller to maximize its expected reward, $J(\theta_c)$:
\graybox{
	J(\theta_c) &= \E[P(a_{1:T};~ \theta_c)]{R}	
}
Since the quantity $\nabla_{\theta_c} R$ is non-differentiable\footnote{$R$ is a function of the action sequence $a_{1:T}$ and the parameters $\theta_c$, and implicitly depends on the samples used for the validation set. Clearly, we do not have access to an analytical form of $R$, and computing numerical gradients via small perturbations of $theta_c$ is computationally intractable.}, we use a \green{policy gradient} method to iteratively update $\theta_c$. All this means is that we instead compute gradients over the softmax outputs (the action probabilities), and use the value of $R$ as a simple weight factor.
\begin{align}
	\nabla_{\theta_c} J(\theta_c) 
	&= R \sum_{t = 1}^{T}   \E[P(a_{1:T};~ \theta_c)]{ \nabla_{\theta_c} \log P(a_t \mid a_{1:t-1}; \theta_c) }	\label{nas_1} \\
	&\approx \inv{m} \sum_{k = 1}^{m} R_k \sum_{t = 1}^{T} \nabla_{\theta_c} \log P(a_t \mid a_{1:t-1}; \theta_c) \label{nas_2}
\end{align}
where the second equation is the empirical approximation (batch-average instead of expectation) over a batch of size $m$, an unbiased estimator for our gradient\footnote{It is unbiased for the same reason that any average over samples $x$ drawn from a distribution $P(x)$ is an unbiased estimator for $\E[P]{x}$.}. Also note that we \textit{do} have access to the distribution $P(a_{1:T}; \theta_c)$ since it is \textit{defined} to be the joint softmax probabilities of our controller, given its parameter values $\theta_c$ (i.e. this is not a $p_{data}$ vs $p_{model}$ situation). The approximation \ref{nas_1} is an unbiased estimator for \ref{nas_2}, but has high variance. To reduce the variance of our estimator, the authors employ a baseline function $b$ that does not depend on the current action:
\begin{align}
 \inv{m} \sum_{k = 1}^{m} \sum_{t = 1}^{T} \nabla_{\theta_c} \log P(a_t \mid a_{1:t-1}; \theta_c) (R_k - b)
\end{align}
























% ===========================================================================================
\lecture{Miscellaneous}{Neural Ordinary Differential Equations}{December 16, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize R. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, ``Neural Ordinary Differential Equations,'' University of Toronto  (Oct 2018).}

\p \blue{Introduction} (1). Let $T$ denote the number of layers of our network. In the limit of $T \rightarrow \infty$ and small $\delta \vec{h}(t)$ between each ``layer'', we can parameterize the dynamics via an ODE:
\begin{align}
	\deriv{\vec{h}(t)}{t}
		&= f\left(
			\vec{h}\left(t \right), t, \theta	 
		\right) \label{eq:neural-ode-1}
\end{align}
Benefits of defining models using ODE solvers:
\begin{compactitem}
	\item \textbf{Memory}. Constant as a function of depth, since we don't need to store intermediate values from the forward pass. 
	
	\item \textbf{Adaptive computation}. Modern ODE solvers adapt evaluation strategy on the fly. 
	
	\item \textbf{Parameter efficiency}. Nearby ``layers'' have shared parameters. 
\end{compactitem}

\begin{example}[Review: ODE]
	Remember the basic idea with ODEs like the one shown above. Our goal is to solve for $\vec{h}(t)$. 
	\begin{align}
		d\vec{h}(t) 
			&= f\left(\vec{h}\left(t \right), t, \theta	 \right) \mathrm{d}t \\
		\int d\vec{h}(t) 
			&= \int f\left(\vec{h}\left(t \right), t, \theta	 \right) \mathrm{d}t \\
		\vec{h}(t) + c_1
			&= \int f\left(\vec{h}\left(t \right), t, \theta	 \right) \mathrm{d}t \\
	\end{align}
	and so the solution of an ODE is often represented as an integral. 
\end{example}


\p \blue{Reverse-mode automatic differentiation of ODE solutions} (2). Our goal is to optimize 
\begin{align}
	L(\vec{z}(t_1))
		&= L \left(
			\int_{t0}^{t_1} f(\vec{z}(t), t, \theta) \mathrm{d}t 
		\right)
\end{align}

Given our starting definition (eq \ref{eq:neural-ode-1}), we can say
\begin{align}
	\vec z (t + \epsilon) 
		&= z(t) + \int_t^{t + \epsilon} f\left(\vec{z}\left(t \right), t, \theta	 \right) \mathrm{d}t 
		:= T_\epsilon (\vec z (t), t)  
\end{align}
which we can use to define the \green{adjoint} $a(t)$:
\begin{align}
	\mgreen{a(t)}
		&\triangleq - \pderiv{L}{\vec z (t)} = -  \pderiv{L}{\vec z (t + \epsilon)} \deriv{\vec z (t + \epsilon)}{\vec z (t)} \\
		&= \mgreen{a(t + \epsilon)} \pderiv{T_\epsilon (\vec z (t), t)}{\vec z (t)} \\
	\deriv{\mgreen{a(t)}}{t}
		&= - \mgreen{a(t)}^T \pderiv{ f(\vec z (t), t, \theta) }{\vec z}
\end{align}
where $\deriv{a(t)}{t}$ can be derived using the limit definition of a derivative. We'll now outline the algorithm for computing gradients. We use a black box ODE solver as a subroutine that solves a first-order ODE initial value problem. As such, it accepts an initial state, its first derivative, the start time, the stop time, and parameters $\theta$ as arguments. 

\begin{algorithm}[Reverse-mode derivative]
	{\small\itshape 
		Given start time $t_0$, stop time $t_1$, final state $\vec{z}(t_1)$, parameters $\theta$, and gradient $\pderiv{L}{\vec{z}(t_1)}$ compute all gradients of $L$. 
}
	
	\tcblower 
	
	\begin{compactenum}
		\item Compute $t_1$ gradient:
		\begin{align}
			\pderiv{L}{t_1}
				&= \pderiv{L}{\vec{z}(t_1)}^T \pderiv{\vec{z}(t_1)}{t_1}
				=  \pderiv{L}{\vec{z}(t_1)}^T f(\vec z (t_1), t_1, \theta)
		\end{align}
		
		\item Initialize the augmented state:
		\begin{align}
			s_0
				&:= \left[
					\vec z (t_1),~
					\vec a (t_1),~
					\vec 0,~
					- \pderiv{L}{t_1}
				\right]
		\end{align}
		
		
		\item Define augmented sate dynamics:
		\begin{align}
			\deriv{ s }{ t }
				&\triangleq \left[
					f(\vec z (t), t, \theta),~
					-\vec a (t)^T \pderiv{f}{\vec z},~
					-\vec a (t)^T \pderiv{f}{\theta},~
					-\vec a (t)^T \pderiv{f}{t}
				\right]
		\end{align}
		
		\item Solve \textbf{reverse-time}\footnote{Notice how our ``initial state'' actually corresponds to $t_1$, and we pass in $t_1$ and $t_0$ in the opposite order we typically do.} ODE:
		\begin{align}
			\left[
				\vec z (t_0),~
				\pderiv{L}{\vec z (t_0)},~
				\pderiv{L}{\theta},~
				\pderiv{L}{t_0}
			\right]
				&= \text{ODESolve}\left(
					s_0, \deriv{s}{t}, t_1, t_0, \theta
				\right)
		\end{align}
		
		\item Return $	\pderiv{L}{\vec z (t_0)},~\pderiv{L}{\theta},~\pderiv{L}{t_0}, \pderiv{L}{t_1} $
	\end{compactenum}
\end{algorithm}










% ===========================================================================================
\lecture{Miscellaneous}{A Framework for Intelligence and Cortical Function}{January 01, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize J. Hawkins et al., ``A Framework for Intelligence and Cortical Function Based on Grid Cells in the NeoCortex,'' Numata Inc. (Oct 2018).}


\p \blue{Introduction}. Authors propose new framework based on location processing that provides supporting evidence to the theory that \textbf{all regions of the neocortex are fundamentally the same}. We've known that \green{grid cells} exist in the hippocampal complex of mammals, but only recently have seen evidence that they may be present in the neocortex.

\myspace
\p \blue{How Grid Cells Represent Location}. Grid cells in the \purple{entorhinal cortex}\footnote{The entorhinal cortex is located in the medial temporal lobe and functions as a hub in a widespread network for memory, navigation and the perception of time.} represent space and location. The main concepts, in order such that they build on one another, are as follows:
\begin{compactitem}
	\item A \textbf{single grid cell} is a neuron that fires [when the agent is] at [one of many] multiple locations in a physical environment\footnote{For example, there may be a grid cell in my brain that fires when I'm at certain locations inside my room. Those locations tend to form a lattice of sorts.}.
	
	\item A \textbf{grid cell module} is a set of grid cells that activate with the \textit{same lattice spacing and orientation} but at shifted locations within an environment. 
	
	\item \textbf{Multiple grid cell modules} that differ in tile spacing and/or orientation can provide \textit{unique location} information\footnote{A single module alone cannot, because it repeats periodically. In other words, it can only provide relative location information.}.
\end{compactitem}
Crucially, the number of unique locations that can be represented by a set of grid cell modules \textbf{scales exponentially} with the number of modules. Every learned environment is associated with a set of unique locations (firing patterns of the grid cells).

\myspace\Needspace{10\baselineskip}
\p \blue{Grid Cells in the Neocortex}. The authors propose that we learn the structures of objects (like pencils, cups, etc) via grid cells in the \textit{neocortex}. Specifically, they propose:
\begin{compactenum}
	\item Every cortical column has neurons that perform a function similar to grid cells.
	
	\item Cortical columns learn models of \textit{objects} similarly to how grid/place cells learn models of \textit{environments}.
\end{compactenum}





% ===========================================================================================
\lecture{Miscellaneous}{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}{March 17, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Y. Gal and Z. Ghahramani, ``A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,'' \textit{University of Cambridge} (Oct 2016).}

\p \blue{Background} (3). In Bayesian regression, we want to infer the parameters $\vec \omega$ of some function $\vec y = f^{\vec \omega}(\vec x)$. We define a prior, $p(\vec \omega)$, and a likelihood,
\begin{align}
	p(y{=}d \mid \vec x , \vec \omega)
		&= \text{Cat}_d\left(
			\text{softmax}\left(
				f^{\vec \omega}(\vec x)
			\right)
		\right)
\end{align}
for a classification setting. Given a dataset $\matr X, \matr Y$, and some new point $\vec{x}^*$, we can predict its output via
\begin{align}
	p(\vec{y}^* \mid \vec{x}^*, \matr X, \matr Y)
		&= \int p(\vec{y}^* \mid \vec{x}^*, \vec \omega) p(\vec \omega \mid \matr X, \matr Y) \mathrm{d}\vec\omega 
\end{align}
In a \green{Bayesian neural network}, we place the prior over the NNs weights (typically Gaussians). The posterior $p(\vec \omega \mid \matr X, \matr Y)$ is usually intractable, so we resort to \green{variational inference} to approximate it. We define our approximating distribution $\vec q (\vec \omega)$ and aim to minimize the KLD:
\graybox{
	\text{KL}\left(
		q(\vec \omega) || p(\vec \omega \mid \matr X, \matr Y)
	\right)
		&\propto 
			- \int q(\vec \omega) \log p(\matr Y \mid \matr X, \vec \omega) \mathrm{d}\vec \omega
			+ \text{KL}(q(\vec \omega) || p(\vec \omega)) \\
		&= \sum_{i=1}^{N} \int q(\vec \omega) \log p(\vec[i]{y} \mid f^{\vec \omega}(\vec[i]{x})) \mathrm{d}\vec\omega
			+ \text{KL}(q(\vec\omega) || p (\vec\omega))
}


\myspace
\p \blue{Variational Inference in RNNs} (4). The authors use MC integration to approximate the integral. The use only a single sample $\hat{\vec \omega} \sim q(\vec \omega)$ for each of the $N$ summations, resulting in an unbiased estimator. Plugging this in, we obtain our objective:
\graybox{
	\mathcal L &\approx 
		- \sum_{i=1}^{N} \log p \left(
			\vec[i]{y} \mid f_{\vec y}^{ \hat{\vec[i]{\omega}} } \left(
				f_{\vec h}^{  \hat{\vec[i]{\omega}} }  \left(      \vec[i, T]{x}, \vec[T - 1]{h}  \right)
			\right)
		\right) + \text{KL}(q( \vec\omega )    || p(\vec \omega))
}
The crucial observations here are:
\begin{compactitem}
	\item For each sequence $\vec[i]{x}$, we sample a new realization $\hat{\vec[i]{\omega}}$. 
	\item For each of the $T$ symbols in $\vec[i]{x}$, we use that \textit{same} realization. 
\end{compactitem}
We define our approximating distribution to factorize over the weight matrices and their rows in $\vec\omega$. For each weight matrix row $\vec[k]{w}$, we have
\graybox{
	q(\vec[k]{w})
		&\triangleq p \mathcal{N}(\vec[k]{w}; \vec{0}, \sigma^2 I)
			+ (1 - p) \mathcal{N}(\vec[k]{w}; \vec[k]{m}, \sigma^2 I)
}
with $\vec[k]{m}$ \green{variational parameter} (row vector). 








% ===========================================================================================
\lecture{Miscellaneous}{Protection Against Reconstruction and Its Applications in Private Federated Learning}{April 26, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize A. Bhowmick et al., ``Protection Against Reconstruction and Its Applications in Private Federated Learning,'' \textit{Apple, Inc.} (Dec 2018).}


\p \blue{Introduction} (1). In many scenarios, it is possible to reconstruct model inputs $x$ given just $\nabla_{\theta} \ell (\theta; x, y)$. \green{Differential privacy} is one approach for obscuring the gradients such that guarantees can be made regarding risk of compromising user data $x$. \green{Locally private} algorithms, however, are preferred to DP when the user wants to keep their data private even from the data collector. The authors want to find a way to perform SGD while providing both local privacy to individual data $X_i$ and stronger guarantees on the global privacy of the output $\hat \theta_n$ of their procedure. \\

Formally, say we have two users' data $x$ and $x'$ (both in $\mathcal X$) and some randomized mechanism $M : \mathcal X \mapsto \mathcal Z$. We say that $M$ is \green{$\varepsilon$-local differentially private} if $\forall x, x' \in \mathcal X$ and sets $S \subset \mathcal Z$:
\graybox{
	\dfrac{
		\Prob{ M(x) \in S }
	}{
		\Prob{ M(x') \in S }
} \le e^{\varepsilon}
}
Clearly, the RHS will need to be pretty big for this to be achievable. The authors claim that allowing $\varepsilon >> 1$ ``may [still] provide meaningful privacy protections.''

\bluesec{Privacy Protections} (2). The focus here is on the \green{curious onlooker}: an individual (e.g. Apple PFL engineer) who can observe all updates to a model and communication from individual devices. Let $X$ denote some user data. Let $\Delta W$ denote the weights difference after some model update using the data $X$. Let $Z$ be the result of the randomized mapping $\Delta W \mapsto Z$. Our setting can be described with the Markov chain $X \rightarrow \Delta W \rightarrow Z$. The onlooker observes $Z$ and wants to estimate some function $f(X)$ on the private data.\\

\textbf{Separated private mechanisms} (2.2). The authors propose, instead of a simple mapping $\Delta W \rightarrow Z$, to split it up into two parts: $Z_1 = M_1(U)$ and $Z_2 = M_2(R)$, where 
\begin{align}
	U &= \frac{\Delta W}{||\Delta W||_2} \\
	R &= ||\Delta W||_2
\end{align}

\begin{definition}[-1em][Separated Differential Privacy]
	A pair of mechanisms $M_1, M_2$ mapping from $\mathcal U \times \mathcal R$ to $\mathcal Z_1 \times \mathcal Z_2$ is \green{$(\varepsilon_1, \varepsilon_2)$-separated differentially private} if  $M_1$ is $\varepsilon_1$-locally differentially private and $M_2$ is $\varepsilon_2$-locally differentially private.
\end{definition}



\bluesec{Privatizing Unit $\ell_2$ Vectors with High Accuracy} (4.1). Given some vector $u \in \mathbb{S}^{d-1}$\footnote{Here, this denotes an n-sphere: $$ \mathbb{S}^n \triangleq \{ x \in \R^{n+1} : ||x|| = r  \} $$}, we want to generate an $\varepsilon$-differentially private vector $Z$ such that
\begin{align}
	\E{Z \mid u} = u \qquad \forall u \in \mathbb{S}^{d-1}
\end{align}

\begin{algorithm}[Privatized Unit Vector: \texttt{PrivUnit}$_2$]
	Sample random vector $V$:
	\begin{align}
		V \sim \begin{cases}
			U\left( 
				\{  v \in \mathbb{S}^{d-1} \mid \langle v, u \rangle \mred{ \geq } \gamma  \} 
				\right) & \text{with probability } p \\
						U\left( 
			\{  v \in \mathbb{S}^{d-1} \mid \langle v, u \rangle \mred{ < } \gamma  \}
			\right) & \text{otherwise}
		\end{cases}
	\end{align}
	where $\gamma \in [0, 1]$ and p $\geq \onehalf$ together control \textit{accuracy}  and \textit{privacy}.\\ 
	
	Let $\alpha = \tfrac{d-1}{2}$, $\tau = \tfrac{1+\gamma}{2}$, and
	\begin{align}
		m = \frac{  (1-\gamma^2)\alpha  }{  2^{d-2} (d-1)  } \left[
			\frac{p}{B(\alpha, \alpha) - B(\tau; \alpha, \alpha)}
			- \frac{1 - p}{B(\tau; \alpha, \alpha)}
		\right]
	\end{align}
	where $B(x, \alpha, \beta)$ is the incomplete beta function (see paper pg 17 for details). \\
	
	Return $Z = \inv m \cdot V$
\end{algorithm}


\bluesec{Privatizing the Magnitude} (4.3). We also need to privatize the weight delta norms. We want to return values $r \in [0, r_{max}]$ for some $r_{max} < \infty$. 






% ===========================================================================================
\lecture{Miscellaneous}{Product quantization for nearest neighbor search}{August 03, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize J\'{e}gou, ``Product quantization for nearest neighbor search,'' (2011)}


\bluesec{Vector Quantization}. Denote the \textit{index set} $\mathcal I = [0..k-1]$ and the set of reproduction values (a.k.a. \green{centroids}) $c_i$ as $\mathcal C = \{ \vec[i]{c} \in \R^D : i \in \mathcal I \}$. We refer to $\mathcal C$ as the \green{codebook} of size $k$.  A \green{quantizer} is a function $q: \vec x \mapsto \vec q(\vec x)$, where $x \in \R^D$ and $\vec q(\vec x) \in \mathcal C$. We typically evaluate the quality of a quantizer with mean squared error of the reconstruction:
\begin{align}
	MSE(q) &= \E[\vec x \sim p(\vec x)]{|| \vec x - \vec q (\vec x) ||_2^2 }
\end{align}
\vspace{1em}

In order for an optimizer to be optimal, it must satisfy the \green{LLoyd optimality conditions}:
\begin{align}
	\mtgreen{(1)}&
		\quad
		\vec q (\vec x) = \argmin_{c_i \in mathcal C} ||\vec x - \vec[i]{c}||_2 \\
	\mtgreen{(2)}&
		\quad
		\vec[i]{c} = \E[\vec x]{\vec x \mid i} \triangleq \int_{\mathcal V_i} \vec x p(\vec x) \mathrm{d} \vec x
\end{align}
a.k.a. literally just K-means. 

\bluesec{Product Quantization}. Input vector $\vec x \in \mathcal R^D$ is split into $m$ distinct subvectors $\vec[j]{u} \in \R^{D/m}$, where $j \in [1..m]$. Note that $D$ must be an integer multiple of $m$ (i.e. $D = a m$ for some $a \in \mathbb Z$).

\graybox{
	\vec x = 
		\underbrace{\slice[D^*]{x}}_{\vec[1]{u}(\vec x)}, \ldots, 
		\underbrace{\slice[D][D-D^*+1]{x}}_{\vec[m]{u}(\vec x)}
	\quad \rightarrow \quad
	\vec[1]{q} ( \vec[1]{u}(\vec x) ), \ldots, \vec[m]{q} ( \vec[m]{u}(\vec x) )
}
Note that each subquantizer $q_j$ has its own index set $\mathcal{I}_j$ and codebook $\mathcal C_j$. Therefore, the final reproduction value of a product quantizer is identified by an element of the product set $\mathcal I = \mathcal I_1 \times \cdots \times \mathcal I_m$. The associated final codebook is $\mathcal C = \mathcal C_1 \times \cdots \times \mathcal C_m$. 




% ===========================================================================================
\lecture{Miscellaneous}{Did the Model Understand the Question?}{August 10, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Mudrakarta et al., ``Did the Model Understand the Question?'' \textit{Univ. Chicago \& Google Brain} (May 2018).  arXiv:1805.05492v1}

\bluesec{TL;DR}. Basically all QA-related networks are dumb and don't learn what we think they learn. 
\begin{compactitem}
	\item Networks tend to make predictions based a tiny subset of the input words. Due to this, altering the non-important words in ways that may drastically change the meaning of the question can have virtually no impact on the network's prediction. 
	
	\item Networks assign high-importance to words like ``there'', ``what'', ``how'', etc. These are actually low-information words that the network should not heavily rely on.
	
	\item Networks rely on the image far more than the question.  
\end{compactitem}

\bluesec{Integrated Gradients (IG)} (3). Purpose: ``isolate question words that a DL system uses to produce an answer.''
\begin{align}
	F(\vec x \eq \seq[n]{x}) &\in [0, 1] \\
	A_F(\vec x, \vec x')
		&= \seq[n]{a} \in \R^n
\end{align}
where $\vec x'$ is some baseline input we use to compute the relative attribution of input $x$. The authors set $\vec x'$ as the ``empty question'' (sequence of padding values)\footnote{The use the same context though (e.g. the associated image for VQA). Only the question is changed.}.

\begin{definition}
	Given an input $x$ and baseline $x'$, the \green{integrated gradient} along the $i$th dimension is as follows.
	\graybox{
		IG_i(x, x')
			&\triangleq (x_i - x_i') \times \int_{\alpha \eq 0}^{1} \pderiv{F(x' + \alpha \times (x - x') ) }{ x_i } \mathrm{d}\alpha 
	}
\end{definition}
Interpretation: seems like IG gives us a better idea of the \textit{total} ``attribution'' of each input dimension $x_i$ relative to baseline $x_i'$ along the line connecting $x_i$ and $x_i'$, instead of just the immediate derivative around $x_i$. Although, the fact that infinitesimal contributions could cancel each other out seems problematic (positive and negative gradients along the interpolation). 











% ======================================================================
\lecture{Miscellaneous}{Solving Rubik's Cube with a Robot Hand}{January 18, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Akkaya et al., ``Solving Rubik's Cube with a Robot Hand''' \textit{OpenAI}, (Oct 2019).}

\bluesec{TL;DR}: \green{Automatic Domain Randomization} (ADR) + robot platform = solving rubik's cube from simulation alone. The two main structural components are (1) the hand and (2) a few cameras that observe the pose/state of the hand/cube.

\bluesec{ADR} (5). They denote an ``environment'' as $e_{\lambda}$, parameterized by $\lambda \in \R^d$. In other words, there are $d$ parameters/knobs they can fiddle with in simulation to change various characteristics of the given environment\footnote{I guess it comes down to semantics whether we interpret a $\Delta \lambda$ as a new environment or just the same environment with different characteristics.}.\marginnote{$$\lambda \in \R^d$$$$\phi \in \R^{d'}$$}. Here, the authors choose $d' = 2d$. Let $\phi^L, \phi^H \in \R^d$ be some partition of $\phi$.
\begin{align}
	P_{\phi}(\lambda)
		&= \prod_{i=1}^{d} P_{\phi}(\lambda_i) = \prod_{i=1}^{d} U(\phi_i^L, \phi_i^H) \\
	\mathcal H(P_{\phi})
		&= - \inv{d} \int P_{\phi}(\lambda) \ln P_{\phi}(\lambda) \mathrm{d} \lambda
		= \inv{d} \sum_{i=1}^d \ln \lr{ \phi_i^H - \phi_i^L }
\end{align}
where they define a normalized entropy $\mathcal H$ in nats/dimension. The pairs $(\phi_i^L, \phi_i^H)$ are referred to as \green{boundary values}. 

\begin{algorithm}[Automatic Domain Randomization]
	TL;DR: algorithm for updating parameter ranges $\phi_i$ in distribution $P_{\phi}$.\\
	 
	Init with \green{performance buffers} $\{ D_i^L, D_i^H \}_{i=1}^d$ (presumably empty at start), \green{thresholds} $m, t_L, t_H$ ($t_L < t_H$). Repeat the following until ``training is complete'':
	\begin{enumerate}
		\item Sample $\lambda \sim P_{\phi}$. 
		
		\item Randomly sample index $1 \le i \le d$, and number $x \sim U(0, 1)$. 
		
		\item If $x < 0.5$, set $D_i \eq D_i^L$, $\lambda_i \eq \phi_i^L$, else  $D_i \eq D_i^H$, $\lambda_i \eq \phi_i^H$. 
		
		\item Collect model performance $p$ on environment parameterized by $\lambda$. 
		
		\item $D_i \leftarrow D_i \cup \{ p  \}$. 
		
		\item If $\text{Length}(D_i) \ge m$:
		\begin{enumerate}
			\item $\bar p \leftarrow \textsc{Average}(D_i)$
			\item \textsc{CLEAR}($D_i$)
			\item if $\bar p \ge t_H$ increase $\phi_i$ by $\Delta$. If $\bar p \le t_L$, decrease $\phi_i$ by $\Delta$. (not sure if this means increase range scale, or shift range left/right (seems like the former))
		\end{enumerate}
	\end{enumerate}

	In English: we sample environment params $\lambda$ at each iteration, but then set a random dimension of $\lambda$ to a boundary value. We measure model performance and append to performance buffer index associated with the boundary value. After we have enough performance measurements for a given boundary value index $i$, we increase the associated pair $(\phi_i^L, \phi_i^H)$.
\end{algorithm}

To generate training data, they use the ADR algorithm above in conjunction with sampling from the [changing each time ADR is run] distribution $P_{\phi}$ and running the model (running the model results in new training data\footnote{\red{TODO}: how? how does running a model generate data? what is the data anyway?}). 
\myfig[0.6\textwidth]{figs/adr.png}






% ======================================================================
\lecture{Miscellaneous}{SmartChoices}{August 06, 2020}
% ======================================================================

\citepaper{V. Carbune, T. Coppey, A. Daryin, T. Deselaers, N. Sarda, J. Yagnik}{SmartChoices: Hybridizing Programming and Machine Learning}{Google Research}{June 2019}


\bluesec{Software Development with SmartChoices} (2). Below is an example for creating a SmartChoice object, along with a common sequence of method invocations:

\lstset{language=Python}
\begin{lstlisting}
choice = SmartChoice(
	output_def=(float, shape=[1], range=[0, 1]),
	observation_defs={k: (float, [1], [0, 10] for k in ['low', 'high', 'target']}, 
	initial_function=lambda observations: 0.5)
choice.Observe({'high': 0.56, 'target': 0.43})
value = choice.Predict()
# ...Do something with value that allows us to compute reward...
choice.Feedback(reward=10)
\end{lstlisting}

SmartChoice aims to maiximize the sum of reward values received over time (possibly discounted). 


\bluesec{SmartChoices in Algorithms} (4). To evaluate the impact of SmartChoices, the authors measure \green{cumulative regret} over training episodes\footnote{Recall that an episode in RL is defined as a sequence of the form ($s_0; a_1, r_1, s_1; a_2, r_2, s_2; \ldots s_F$), starting with a start state and terminating with an end state.}




% ======================================================================
\lecture{Miscellaneous}{The Contextual Bandits Problem}{August 09, 2020}
% ======================================================================

\citepaper{Robert Schapire}{The Contextual Bandits Problem}{Lecture at Microsoft Research}{March 2017}

(\href{https://youtu.be/N5x48g2sp8M}{Link to talk})


\begin{algorithm}[Formal Model]
	Repeat the folowing for $t \in [1..T]$:
	\begin{compactenum}
		\item Learner observes \green{context} $x_t$
		\item \green{Reward} vector $\vec[t]{r} \in [0, 1]^K$ chosen (but \textbf{not} observed)
		\item Learner selects \green{action} $a_t \in \{1, \ldots, K\}$
		\item Learner receives observed \green{reward} $r_t(a_t)$ 
	\end{compactenum}

	\textbf{Goal}: maximize the total reward: $\sum_{t=1}^T r_t(a_t)$ \textit{relative to the best policy} $\pi \in \Pi$, i.e. want small \green{regret}:
	\begin{align}
		\max_{\pi \in \Pi} \inv{T} \sum_{t=1}^T r_t\lr{ \pi\lr{ x_t } } 
		- \inv{T} \sum_{t=1}^T r_t (a_t)
	\end{align}
	
	For now, we assume pairs $(x_t, \vec[t]{r})$ are chosen at random iid. 
\end{algorithm}

Some miscellaneous additional points: 
\begin{compactitem}
	\item Want to learn a good \green{policy} $\pi: x \mapsto a$.
	\item Before learning, we still have to decide the general form/space of possible policies that can be learned (by deciding general model architecture, etc). 
	\item A \green{no-regret} learning algorithm is one that satisfies $\text{regret} \rightarrow 0$ as $T \rightarrow \infty$. 
\end{compactitem}
 
\bluesec{Starting Point: Full-Information Setting}. Same as bandit, but learner can see rewards for \textit{all} actions (for the given context). Note that the learner does not see the rewards for all actions until it \textit{after} it chooses an action -- it first takes an action, then receives a reward and also is shown the rewards for all the other actions it could've taken. Similarly, after our learner makes a series of actions for various contexts, it can look back and compute the rewards it \textit{would've} received for any other policy\footnote{\red{Question}: is it more common to reflect after a series of N actions, or to do so after every action?}.

\begin{algorithm}[Follow-the-Leader algorithm]
	Optimal-regret algorithm for the full-information setting (and iid $(x_t \vec[t]{r})$. At each round $t$, do:
	\begin{compactenum}
		\item Find the \textit{empirically best} $\pi \in \Pi$, the policy with the maximum average reward over all previously-observed contexts. 
		\item Use this policy to choose the next action: $a_t = \pi(x_t)$. 
	\end{compactenum}

	Has optimal regret $\mathcal{O}\lr{\sqrt{\ln | \Pi | / T}}$. 
\end{algorithm}
The main challenge of FTL is determining how to find the empirical best policy, which is the task of finding some single function $\pi: x \mapsto a$ that would've resulting in maximum average reward over all previous observations\footnote{Note that this is identical to the general task of finding a classifier that performs optimally on some training data}. Considering that the context and action spaces can be large, this is not trivial. 


\bluesec{Non-Stochastic (Adversarial) Setting}. If the $(x_t, \vec[t]{r})$ are \textit{not} iid, then FTL will not work, especially if the contexts are provided by an adversary (e.g. in game playing). The \green{hedge algorithm} (below) has optimal regret even in adversarial setting, but time/space are linear $| \Pi |$. 

\begin{algorithm}[Hedge Algorithm]
	Maintain a real-valued \textit{weight} for every $\pi \in \Pi$. On each round $t$:
	\begin{compactenum}
		\item Choose \textit{random} policy $\pi$ with probability proportional to weights
		\item Use action chosen by $\pi$ 
		\item \textit{Increase} weight of each policy according the reward it would've received
	\end{compactenum}
\end{algorithm}

\bluesec{Epoch-Greedy Algorithm}. Back to the bandit setting (and stochastic/iid) where we only observe rewards for the actions we take. Although we \textit{could} attempt to just use the same algorithms as before -- but only on actions/rewards we took of course -- we end up with a lot of missing/biased information (since we can only evaluate other policies on some $x_t$ when they take the same action as the policy we used). One simple improvement is to use the same algorithms as before (FTL) with probability $1 - \epsilon$, and to choose an action uniformly at random with probability $\epsilon$. 

\bluesec{De-biasing Biased Estimates}. Need to address the problem of selection bias. A simple trick is \green{inverse-propensity weighting}:

\begin{definition}[-1em][Inverse-Propensity Weighting]
	\textbf{Goal}: estimate $\E{X}$ for some RV $X$. With probability $p$, we observe $X$ once, and with probability (1 - p), we don't get to observe $X$ at all (on each trial). 
	
	\textbf{Trick}: Define new RV $\hat{X} = X / p$ if observed, else 0. Then $\E{\hat X} = p \E{X / p} + (1 - p) \E{0} = X$ -- unbiased!
\end{definition}
We can use this technique to get unbiased estimates for the rewards of \textit{all} actions (not just observed). \red{Problem}: variance can be extremely large!



\bluesec{Mini-Monster Algorithm}\footnote{A.K.A. ILOVETOCONBANDITS}. Each round, finds a weighted combination of policies satisfying:
\begin{compactenum}
	\item Low (estimated) regret
	\item Low (estimated) variance
\end{compactenum}
















% ======================================================================
\lecture{Miscellaneous}{The principles of adaptation in organisms and machines}{September 13, 2020}
% ======================================================================

\citepaper{Hideaki Shimazaki}{
	The principles of adaptation in organisms and machines I: machine learning, information theory, and thermodynamics
}{Graduate School of Informatics, Kyoto University}{Feb 2019}


\bluesec{Hierarchical Models} (2.1). The author defines the following [hierarchical] generative model for data $\rvec{Y}$ from the outside world:
\begin{align}
	p(\vec y , \vec x, \vec w)
		&= p(\vec y \mid \vec x, \vec w) p(\vec x \mid \vec w) p(\vec w) \\
    \mtgreen{[generative model]}\qquad	
		&\approx \mgreen{ p(\vec y, \vec x; w) }
		= \morange{ p(\vec y \mid \vec x; w)} \mpurple{ p(\vec x; w) }
\end{align}
factorized using the \orange{observation model} and \purple{prior} model, where the second line is the approximation the author makes henceforth: treat $\vec w$ as a fixed hyperparameter instead of a value sampled from a distribution\footnote{Note also that in sec 2.2. he ends up treating $\vec w$ as trainable model parameters.}. $\rvec X$ represents the underlying causative factors of the data $\rvec Y$, while $\rvec W$ is basically a blanket set of factors containing any environmental factors/contexts underlying either/both of $\rvec X$ and $\rvec Y$.  We can further simplify by letting $\vec w = (\vec \lambda, \vec \phi)$, defined such that we have the following graphical model:

\begin{drawing}
	% X and Y nodes.
	\node[latent] (X) {X};
	\node[latent, below=0.5cm of X] (Y) {Y};
	
	% beta, omega, and phi const nodes.
	\node[const, left=of X, yshift=1.0em] (lambda) {$\lambda$}; 
	\node[const, left=of X, yshift=-1.5em] (phi) {$\phi$};
	
	% X -> Y edge.
	\edge{X} {Y};
	
	% Const edges.
	\edge{lambda} {X};
	\edge{phi} {Y};
\end{drawing}

In generative modeling, the typical goal is to learn the marginalized likelihood, which we can then use to estimate underlying causes from data\footnote{I'll use the notation $p(\vec y \mid \vec w)$ interchangeably with $p(\vec y ; w)$, etc.}:
\begin{align}
	\mtred{[marginal likelihood]} \qquad
		\mred{ p(\vec y \mid \vec w) }
		&= \int \mgreen{p(\vec y, \vec x \mid \vec w)} \mathrm{d} \vec x \\
	\mtpink{[posterior distribution]} \qquad
		\mpink{ p(\vec x \mid \matr Y, \vec w) }
		&= \frac{
				 \morange{ p(\matr Y \mid \vec x, \vec w)}  \mpurple{ p(\vec x \mid \vec w) }
	}{
		\mred{  p(\matr Y \mid \vec w) }
		}
\end{align}


\bluesec{Learning} (2.2). Author defines \green{Type II maximum likelihood estimation}: the process of determining parameters that maximize the marginal likelihood \underline{function} $p(\matr Y \mid  \vec w)$:
\begin{align}
	\matr{W}^* 
		&= \argmax_{\vec w} \log \mred{ p(\matr Y \mid \vec w) } \\
		&= \argmax_{\vec w} \log  \int \mgreen{p(\matr Y, \vec x \mid \vec w)} \mathrm{d} \vec x \\
\end{align}
The method for constructing $\mpink{ p(\vec x \mid \matr Y, \vec w) }$ using the generative model with $\vec w := \matr{W}^*$ is called the \green{empirical Bayes method}. As usual, the author points out the difficulty of obtaining the exact form of $\morange{ p(\matr Y \mid \vec x, \vec w)}$ and $ \mpurple{ p(\vec x \mid \vec w) }$ (and thus also $	\mred{ p(\vec y \mid \vec w) }$ and $ p(\vec x \mid \matr Y, \vec w) $), and proceeds to introduce approximate methods. We'll use a slightly modified version of the graphical model from before: 

\begin{drawing}
	% X and Y nodes.
	\node[latent] (X) {X};
	\node[latent, below=0.5cm of X] (Y) {Y};
	
	% beta, omega, and phi const nodes.
	\node[const, above=of X] (beta) {$\beta$}; 
	\node[const, left=of beta, yshift=-1.5em] (omega) {$\omega$};
	\node[const, left=of X, yshift=-1.5em] (phi) {$\phi$};
	
	% X -> Y edge.
	\edge{X} {Y};
	
	% Const edges.
	\edge{beta} {X};
	\edge{omega} {X};
	\edge{phi} {Y};
	
	\plate {p} 	{(beta)(X)(Y)} {$N$};
\end{drawing}

We'll also define some \red{crucial assumptions} that will hold henceforth:
\begin{compactitem}
	\item Assumption 1: Observation model parameters $\vec \phi$ are learned from an \underline{entire set} of observed data $\{\matr[1]{Y}, \ldots, \matr[n]{Y}\}$. 
	\item Assumption 2: Using a factorized set of prior distribution parameters $\vec \lambda \rightarrow \{\vec \beta, \vec \omega\}$, assume \textellipsis
	\begin{compactitem}
		\item $\vec \omega$ are optimized using \underline{all samples}.
		\item $\vec \beta$ are optimized for \underline{each sample}.
	\end{compactitem}
\end{compactitem}

\bluesec{Approximate Inference} (3). Recall that variational inference is the process of learning a parameterized function $q(\vec x \mid \matr Y) \approx p(\vec x \mid \matr Y, \vec w)$ to approximate the posterior.  Quick review of the main equations:
\begin{align}
	\log p(\matr Y \mid \vec w)
		&= \mathcal{L}[q, p] + \dkl{q}{p} \\
	 \mathcal{L}[q, p] 
	 	&\triangleq \int q(\vec x \mid \matr Y) \log \frac{ \mgreen{p(\matr Y , \vec x \mid \vec w)} }{  q(\vec x \mid \matr Y) } \mathrm{d} \vec x \\ 
	 	&= \mathcal{Q}[q, p] + H[q] \\
	 \mathcal{Q}[q, p]
	 	&\triangleq \int q(\vec x \mid \matr Y)  \log \mgreen{p(\matr Y , \vec x \mid \vec w)}  \mathrm{d} \vec x
\end{align}
with an illustration of the EM-algorithm below:

\myfig[0.8\textwidth]{figs/principles_of_adapt_fig4.png}


\Needspace{15\baselineskip}
\bluesec{Summary of Interpretations}. The author provides a lot of interpretations of formulas scattered throughout -- below I've summarized them:
\begin{compactitem}
	\item \pink{Posterior} $p(\vec x \mid \matr Y, \vec w)$
	\begin{compactitem}
		\item Neural activity $\matr X$ caused by stimulus $\matr Y$. 
		\item Constructed by modulating the internal neural activity represented as the prior distribution by observing data.
	\end{compactitem}
	\item \orange{Observation Model} $p(\vec y \mid \vec x, \vec \phi)$:
	\begin{compactitem}
		\item Represents how data is expressed by combinations of neuronal activity. 
	\end{compactitem}

	\item \purple{Prior} $p(\vec x \mid \vec \lambda)$:
	\begin{compactitem}
		\item Constraints on neural activity. 
		\item Spontaneous activity of neurons. 
	\end{compactitem}
\end{compactitem}

\bluesec{Further Reading}. Some quotes and their citations from the paper that I want to investigate further. 
\begin{compactitem}
	\item ``It is likely that the brain implements functions similar to Bayesian inference''  -- K. Doya, Bayesian brain: Probabilistic approaches to neural coding. MIT press, 2007.
	
	\item ``Algorithms for learning approximate inference model in the statistics and machine learning is instructive when we consider models of the brain.'' K. Friston, “Learning and inference in the brain,” Neural Networks. 
	
	\item ``Parameters that represent spontaneous activity of neurons may be learned from many samples.'' P. Berkes, G. Orban, M. Lengyel, and J. Fiser, “Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment,” Science, vol. 331, no. 6013, pp. 83-87,  2011.
	
	\item ``Early sensory neurons receive feedback/recurrent modulation via top-down or lateral connections, which plays an essential role in perceptual experiences, attention, and reward modulation.'' V. A. Lamme and P. R. Roelfsema, “The distinct modes of vision offered by feedforward and recurrent processing,” Trends in neurosciences, vol. 23, no. 11, pp. 571–579, 2000.
\end{compactitem}










\end{document}








