% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% P  R  E  A  M  B  L  E
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
\documentclass[11pt]{article}
\usepackage{amsbsy, amsmath, amssymb, authblk}

%\usepackage{array} 
%\usepackage{algorithm2e}

\usepackage{booktabs, bm}
\usepackage[small,labelfont=bf,up,singlelinecheck=false]{caption}
\usepackage{cancel}
\usepackage{comment}
%\usepackage{fancyhdr}
%\usepackage[default]{lato}
\usepackage[T1]{fontenc}
\usepackage[bottom]{footmisc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage[utf8]{inputenc}
%	\inputencoding{latin1}
%	\inputencoding{utf8}
%\usepackage{lettrine}
%\usepackage[sc]{mathpazo}
\usepackage{lmodern} % Nice fonts?
%\usepackage{mathrsfs}
\usepackage{mathtools} 
%\usepackage{marvosym} % silly bullet-point symbols (misc symbols)
%\usepackage{microtype}
\usepackage{minitoc}         % left in case it is needed elsewhere
\setcounter{secttocdepth}{5} % idem
\usepackage{etoc} % for toc before each section.
%\usepackage{multicol}
\usepackage{needspace}
\usepackage{paralist}
%\usepackage{polynom} 			% typesetting polynomial long division
%\usepackage{setspace}
%	\onehalfspacing 
\usepackage{stmaryrd}  % \llbracket, \rrbracket
\usepackage{tocloft}
\usepackage{xparse} % DeclareDocumentCommand
\usepackage[compact]{titlesec} 		% compact shrinks whitespace around section headings.
\usepackage{ulem} 				% for strikeout \sout command.
%\usepackage{verbatim}

% Muh packagez :)
\usepackage{../Packages/MathCommands}
\usepackage{../Packages/BrandonColors}
\usepackage{../Packages/BrandonBoxes}
\usepackage{../Packages/NoteTaker}
%\usepackage{../Packages/MachineLearningUtils}


%\usepackage{program}
% DL BOOK CONVENTIONS
\renewcommand\vec[2][]{\bm{#2}_{#1}}

\DeclareDocumentCommand{\slice}
	{ O{t} O{1} m }
	{\vec[\langle #2 \ldots #1 \rangle]{#3}}

\newcommand\myfig[2][0.3\textwidth]{\begin{figure}[h!]\centering\includegraphics[width=#1]{#2}\end{figure}}
\newcommand\myspace[1][]{\vspace{#1\bigskipamount}}
\newcommand\p{\Needspace{10\baselineskip} \noindent}
\newcommand\tlab[1]{\tag{#1}\label{#1}}
\newcommand\Var[1]{\mathrm{Var}\left[#1\right]}


%\usepackage{program}

\usepackage{layout} % Type \layout() anywhere to see values of layout frame.
%\usepackage{showframe} % Displays layout frame on all pages
\usepackage{marginnote}
\renewcommand*{\marginfont}{\scriptsize}

\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\usepackage{tikz}
\usetikzlibrary{arrows, automata, shapes, snakes, positioning}
\usetikzlibrary{bayesnet}


\titleformat*{\subsubsection}{\small\scshape}
\newcommand\subsub[1]{\Needspace{15\baselineskip}\hrule\subsubsection{#1}\hrule}

 \renewcommand\dotseq[2]
 {#1^{(1)}, \ldots, #1^{(#2)}}
 \renewcommand\rdotseq[2]
 {#1^{(#2)}, \ldots, #1^{(1)}} % reversed
	
\newcommand\QA[2]{\item \red{Q}: #1
	\begin{compactitem}
		\item \green{A}: #2
	\end{compactitem}}
	
\newcommand\myref[1]{\purple{[#1]}}

\definecolor{forgeblue}{HTML}{018C9F}
% Gray table borders
\makeatletter
\def\rulecolor#1#{\CT@arc{#1}}
\def\CT@arc#1#2{%
	\ifdim\baselineskip=\z@\noalign\fi
	{\gdef\CT@arc@{\color#1{#2}}}}
\let\CT@arc@\relax
\rulecolor{forgeblue}
\makeatother


%\setlength{\parskip}{1pt}
%\setlength{\columnseprule}{0.1pt}
%\setlength{\columnsep}{0.6cm}
%\setlength\tabcolsep{0.1cm}
\renewcommand{\arraystretch}{1.2}


\DeclareDocumentEnvironment{definition}{O{-0.5em} o}{
	\IfNoValueTF{#2}{}{\textbf{#2}}
	\vspace*{#1}
	\begin{quote}
		\itshape\small}
	{\end{quote}}

\makeatletter
\newcommand*\dotp{\mathpalette\dotp@{.5}}
\newcommand*\dotp@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

% bluesec
\newcommand\bluesec[1]{\myspace \p \blue{#1}}

% Title
\title{\vspace{-10mm}\fontsize{24pt}{8pt}\selectfont\textbf{Fall 2016 Course Notes}\vspace*{-4mm}}
% Author
\author{Brandon McKinzie}
% Date
\date{}

% --------------------------------------------------------------
% --------------------------------------------------------------


\renewcommand\cftsecfont{\small\bfseries}
\renewcommand\cftsubsecfont{\scriptsize}
\renewcommand\cftsubsubsecfont{\scriptsize}

\renewcommand\cftsecafterpnum{\vskip-5pt}
\renewcommand\cftsubsecafterpnum{\vskip-7pt}
\renewcommand\cftsubsubsecafterpnum{\vskip-7pt}

\begin{document}

\dosecttoc
\tableofcontents

% ========================= SECTIONSWITCH ========================= 

% ==================================================================================
% APPLIED MATH AND ML BASICS
% ==================================================================================
\mysection{Math and Machine Learning Basics}\label{Math and Machine Learning Basics}

\lecture{Math and Machine Learning Basics}{Linear Algebra (Quick Review) (Ch. 2)}{January 23, 2017}

\begin{itemize}
	\item For $A^{-1}$ to exist, $Ax=b$ must have \underline{exactly} one solution for every value of $b$.\marginnote{Unless stated otherwise, assume $A \in \R^{m\times n}$} Determining whether a solution exists $\forall b \in \R^m$ means requiring that the \green{column space} (range) of $A$ be all of $\R^m$. It is helpful to see $Ax$ expanded out explicitly in this way:
	\begin{align}
 		\matr{A} \vec{x} &= \sum_i x_i \matr[:,i]{A} 
 			= x_1 \begin{pmatrix} A_{1,1} \\ \vdots \\ A_{m,1} \end{pmatrix}
 				+ \cdots 
 				+ x_m \begin{pmatrix} A_{1,n} \\ \vdots \\ A_{m,n} \end{pmatrix}
 			\tlab{2.27}
	\end{align}
	\begin{compactitem}[$\rightarrow$]
		\item Necessary: $\matr{A}$ must have at least $m$ columns ($n \ge m$). (``wide''). 
		\item Necessary \textit{and} sufficient: matrix must contain at least one set of $m$ linearly independent columns. 
		\item Invertibility: In addition to above, need matrix to be \textit{square} (re: at most $m$ columns $\land$ at least $m$ columns). 
	\end{compactitem}

	\item A square matrix with linearly dependent columns is known as \green{singular}. A (necessarily square) matrix is singular if and only if one or more eigenvalues are zero. 

	% Norms
	\item A \green{norm} is any function $f$ that satisfies the following properties:\marginnote{$||x||_\infty = \max_i |x_i|$}
	\begin{align}
		f(\vec{x}) = 0 &\Rightarrow \vec{x} = \vec{0} \\
		f(\vec{x} + \vec{y}) &\le f(\vec{x}) + f(\vec{y}) \\
		\forall \alpha \in \R, ~ f(\alpha \vec{x}) &= |\alpha|f(\vec{x})
	\end{align}
	
	\item An \green{orthogonal matrix} is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal:\marginnote{Note that orthonorm cols implies orthonorm rows (if square). To prove, consider the relationship between $\matr{A}^T\matr{A}$ and $\matr{A}\matr{A}^T$}
	\begin{align}
		\matr{A}^T\matr{A} &= \matr{A}\matr{A}^T = \matr{I} \tlab{2.37} \\
		\matr{A}^{-1} &= \matr{A}^T \tlab{2.38}
	\end{align}
	
	\item Suppose square matrix $\matr{A} \in \R^{n \times n}$ has $n$ linearly independent eigenvectors $\{\vec{v}^{(1)}, \ldots, \vec{v}^{(n)}\}$. The \green{eigendecomposition} of $\matr{A}$ is then given by\marginnote{\underline{All} real-symmetric $\matr{A}$ have an eigendecomposition, \textbf{but it might not be unique!}}[2cm]\footnote{This appear to imply that unless the columns of $V$ are also normalized, can't guarantee that its inverse equals its transpose? (since that is the only difference between it and an orthogonal matrix)}
	\begin{align}
		\matr{A} &= \matr{V} ~  \diag{\vec{\lambda}} ~ \matr{V}^{-1} \tlab{2.40}
	\end{align}
	In the special case where $\matr{A}$ is real-symmetric, $\matr{A} = \matr{Q}\matr{\Lambda}\matr{Q}^T$. \purple{Interpretation:} $\matr{A}\vec{x}$ can be decomposed into the following three steps:
	\begin{compactitem}
		\item[1)] \textbf{Change of basis}: The vector $(\matr{Q}^T\vec{x})$ can be thought of as how $\vec{x}$ would appear in the basis of eigenvectors of $\matr{A}$. 
		\item[2)] \textbf{Scale}: Next, we scale each component $(\matr{Q}^T\vec{x})_i$ by an amount $\lambda_i$, yielding the new vector $(\matr{\Lambda}(\matr{Q}^T\vec{x}))$. \marginnote{A common convention to sort the entries of $\Lambda$ in descending order. }
		\item[3)] \textbf{Change of basis}: Finally, we rotate this new vector back from the eigen-basis into its original basis, yielding the transformed result of $\matr{Q}\matr{\Lambda}\matr{Q}^T\vec{x}$. 
	\end{compactitem}

	\item \green{Positive definite}: all $\lambda$ are positive; \green{positive semidefinite}: all $\lambda$ are positive or zero.
	\begin{compactitem}[$\rightarrow$]
		\item PSD: $\forall \vec{x}, ~~ \vec{x}^T \matr{A} \vec{x} \ge 0$
		\item PD: $\vec{x}^T \matr{A} \vec{x} = 0 ~ \Rightarrow ~ \vec{x} = \vec{0}$.\footnote{I proved this and it made me happy inside. Check it out. 
			Let $\matr{A}$ be positive definite. Then 
			\begin{align}
				\vec{x}^T \matr{A} \vec{x} &= \vec{x}^T \matr{Q} \matr{\Lambda} \matr{Q}^T \vec{x} \\
				&= \sum_i (\matr{Q}^T \vec{x})_i \lambda_i (\matr{Q}^T \vec{x})_i \\
				&= \sum_i \lambda_i (\matr{Q}^T \vec{x})^2_i 
			\end{align}
			Since all terms in the summation are non-negative and all $\lambda_i > 0$, we have that $\vec{x}^T \matr{A} \vec{x} = 0$ if and only if $(\matr{Q}^T \vec{x})_i = 0 = \vec{q}^{(i)} \cdot \vec{x}$ for all $i$. Since the set of eigenvectors $\{\vec{q}^{(i)}\}$ form an orthonormal basis, we have that $\vec{x}$ must be the zero vector. }
	\end{compactitem}

	% ------------  SVD -------------
	\item Any real matrix $\matr{A} \in \R^{m \times n}$ has a \green{singular value decomposition} of the form, \marginnote{\begin{align}
		\matr{U} &\in \R^{m \times m} \\ 
		\matr{D} &\in \R^{m \times n} \\
		\matr{V} &\in \R^{n \times n} 
		\end{align}}
	\begin{align}
		\matr{A} &= \matr{U} \matr{D} \matr{V}^T
	\end{align}
	where both $\matr{U}$ and $\matr{V}$ are orthogonal matrices, and $\matr{D}$ is diagonal. 
	\begin{compactitem}
		\item The \textbf{singular values} are the diagonal entries $\matr[ii]{D}$. 
		\item The \textbf{left(right)-singular vectors} are the columns of $\matr{U}$($\matr{V}$). 
		\item Eigenvectors of $\matr{A}\matr{A}^T$ are the L-S vectors. Eigenvectors of $\matr{A}^T\matr{A}$ are the R-S vectors. The eigenvalues of both $\matr{A}\matr{A}^T$ and $\matr{A}^T\matr{A}$ are given by the singular values squared. 
	\end{compactitem}

	\item The Moore-Penrose \green{pseudoinverse}, denoted $\matr{A}^+$, enables us to find an ``inverse'' of sorts for a (possibly) non-square matrix $\matr{A}$.\marginnote{$\matr{A}^+$ is useful, e.g., when we want to solve $\matr{A} \vec{x} = \vec{y}$ by left-multiplying each side to obtain $\vec{x} = \matr{B} \vec{y}$.\\ It is far more likely for solution(s) to exist when $\matr{A}$ is wider than it is tall.} Most algorithms compute $\matr{A}^+$ via
	\begin{align}
		\matr{A}^+ = \matr{V} \matr{D}^{+} \matr{U}^T 
	\end{align}
	
	
	\item The \green{determinant} of a matrix is $\det(\matr{A}) = \prod_i \lambda_i$. Conceptually, $|\det(\matr{A})|$ tells how much [multiplication by] $\matr{A}$ expands/contracts space. If $\det(\matr{A}) = 1$, the transformation preserves volume.
\end{itemize}

\subsub{Example: Principal Component Analysis}

\myspace
\p \blue{Task}. Say we want to apply lossy compression (less memory, but may lose precision) to a collection of $m$ points $\{\vec{x}^{(1)}, \ldots, \vec{x}^{(m)} \}$. We will do this by converting each $\vec{x}^{(i)} \in \R^n$ to some $\vec{c}^{(i)} \in \R^l$ ($l < n$), i.e. finding functions $f$ and $g$ such that:
\begin{align}
	f(\vec{x}) = \vec{c} \quad \text{and} \quad \vec{x} \approx g(f(\vec{x}))
\end{align}


\myspace
\p \blue{Decoding function ($g$)}. As is, we still have a rather general task to solve. \textit{PCA} is defined by choosing $g(\vec{c}) = \matr{D}\vec{c}$, with $\matr{D} \in \R^{n \times l}$, where all columns of $\matr{D}$ are both (1) orthogonal and (2) unit norm.

\myspace
\p \blue{Encoding function ($f$)}. Now we need a way of mapping $\vec{x}$ to $\vec{c}$ such that $g(\vec{c})$ will give us back a vector optimally close to $\vec{x}$. We've already defined $g$, so this amount to finding the optimal $\vec{c}*$ such that:
\begin{align}
	\vec{c}* &= \argmin_{\vec{c}} || \vec{x} - g(\vec{c}) ||_2^2 \\
	(\vec{x} - g(\vec{c}))^T (\vec{x} - g(\vec{c})) &= \vec{x}^T\vec{x} - 2 \vec{x}^T g(\vec{c}) + g(\vec{c})^T g(\vec{c}) \\
	\vec{c}* &= \argmin_{\vec{c}} \left[ -2 \vec{x}^T \matr{D} \vec{c} + \vec{c}^T \vec{c} \right] \\
	&= \matr{D}^T \vec{x} = f(\vec{x})
\end{align}
which means the PCA \textit{reconstruction operation} is defined as $r(\vec{x}) = \matr{D} \matr{D}^T \vec{x}$.

\myspace
\p \blue{Optimal $\matr{D}$}. It is important to notice that we've been able to determine e.g. the optimal $\vec{c}*$ \textit{for some} $\vec{x}$ because each $\vec{x}$ has a (allowably) different $\vec{c}*$. However, we use \textit{the same} matrix $\matr{D}$ for all our samples $\vec{x}^{(i)}$, and thus must optimize it over all points in our collection. With that out of the way, we just do what we always do: minimize over the $L^2$ distance between points and their reconstruction. Formally, we minimize the Frobenius norm of the matrix of errors:
\begin{align}
\matr{D}* &= \argmin_{\matr{D}} \sqrt{ 
		\sum_{i,j} \left( \vec[j]{x}^{(i)}  -   r(\vec{x}^{(i)})_j \right)^2
	  } \quad s.t. \quad \matr{D}^T \matr{D} = \matr{I}
\end{align} 

\p Consider the case of $l = 1$ which means $\matr{D} = \vec{d} \in \R^n$. In this case, after [insert math here], we obtain
\begin{align}
	\vec{d}* &= \argmax_{\vec{d}} Tr\left( \vec{d}^T \matr{X}^T \matr{X} \vec{d} \right)
	\quad s.t. \quad \vec{d}^T \vec{d} = 1
\end{align}
where, as usual, $\matr{X} \in \R^{m,n}$. It should be clear that the optimal $\vec{d}$ is just the largest eigenvector of $\matr{X}^T \matr{X}$. 


% =================================================================================
% Probability 
% =================================================================================
\lecture{Math and Machine Learning Basics}{Probability \& Information Theory (Quick Review) (Ch. 3)}{January 24}

\p \blue{Expectation}. For some function $f(x)$, $\E[x \sim P]{f(x)}$ is the mean value that $f$ takes on when $x$ is drawn from $P$. The formula for discrete and continuous variables, respectively is as follows:
\begin{align}
\E[x \sim P]{f(x)} &= \sum_x P(x) f(x) \tlab{3.9} \\
\E[x \sim P]{f(x)} &= \int p(x) f(x) \mathrm{d}x \tlab{3.10} 
\end{align}

\myspace 
\p \blue{Variance}. A measure of how much the values of a function of a random variable $x$ vary as we sample different values of $x$ from its distribution.
\begin{align}
\Var{f(x)} = \E{(f(x) - \E{f(x)})^2} \tlab{3.11}
\end{align}

\myspace
\p \blue{Covariance}. Gives some sense of how much two values are \textit{linearly} related to each other, as well as the \textit{scale} of these variables.
\begin{align}
\Cov{f(x)}{g(x)} = \E{~\left(f(x) - \E{f(x)}\right)~\left(g(x) - \E{g(x)}\right)~} \tlab{3.13}
\end{align}
\begin{compactitem}[$\rightarrow$]
	\item Large $|\Cov{f}{g}|$ means the function values change a lot and both functions are far from their means at the same time.
	\item \green{Correlation} normalizes the contribution of each variable in order to measure only how much the variables are related.
\end{compactitem}

\myspace
\p \blue{Covariance Matrix} of a random vector $\rvec{x} \in \R^{n}$ is an $n \times n$ matrix, such that
\begin{align}
\mathrm{Cov}\left[\rvec{x}\right]_{i,j}   = \Cov{x_i}{x_j} \tlab{3.14}
\end{align}
and if we want the ``sample'' covariance matrix taken over $m$ data point samples, then
\begin{align}
\Sigma &:= \frac{1}{m} \sum_{k = 1}^{m} (x_k - \bar x)(x_k - \bar x)^T
\end{align}
where $m$ is the number of data points.

\myspace
\p \blue{Measure Theory}. 
\begin{compactitem}
	\item A set of points that is negligibly small is said to have \green{measure zero}. In practical terms, think of such a set as occupying no volume in the space we are measuring (interested in). \marginnote{In $\R^2$, a line has measure zero.}
	\item A property that holds \green{almost everywhere} holds throughout all space except for on a set of measure zero.
\end{compactitem}

\myspace
\p \blue{Functions of RVs}. 
\begin{compactitem}
	\item \red{Common mistake:} Suppose $\rvec{y} = g(\rvec{x})$, and $g$ is invertible/continuous/differentiable. It is NOT true that $p_y(\rvec{y}) = p_x(g^{-1}(\rvec{y}))$. This fails to account for the distortion of [probability] space introduced by $g$. Rather, 
	\graybox{
		p_x(\rvec{x}) &= p_y(g(\rvec{x})) \bigg| \pderiv{g(\rvec{x})}{\rvec{x}}\bigg| \tlab{3.47}
	}
\end{compactitem}


\myspace
\p \blue{Information Theory}. Denote the \green{self-information} of an event $\rvec{x} = x$ to be 
\begin{align}
	I(x) \triangleq - \log P(x)
\end{align}
where $\log$ is always assumed to be the natural logarithm. We can quantify the amount of uncertainty in an entire probability distribution using the \green{Shannon entropy},
\begin{align}
	H(\rvec{x}) &= \E[\rvec{x} \sim P]{I(x)} = - \E[\rvec{x} \sim P]{\log P(x)}
\end{align}
which gives the expected amount of information in an event drawn from that distribution. Taking it a step further, say we have two separate probability distributions $P(\rvec{x})$ and $Q(\rvec{x})$. We can measure how different these distributions are with the \green{Kullback-Leibler (KL) divergence}:
\begin{align}
	D_{KL}\left( P || Q \right) &\triangleq \E[\rvec{x} \sim P]{\log \frac{P(x)}{Q(x)} } = \E[\rvec{x} \sim P]{\log P(x) - \log Q(x)}
\end{align}
Note that the expectation is taken over $P$, thus making $D_{KL}$ not symmetric (and thus not a true distance measure), since $D_{KL}(P || Q) \ne D_{KL}(Q || P)$. Finally, a closely related quantity is the \green{cross-entropy}, $H(P, Q)$, defined as:
\graybox{
	H(P, Q) &\triangleq H(P) + D_{KL}(P || Q) \\
	&= - \E[\rvec{x} \sim P]{\log Q(x)}
	}







% =================================================================================
% Numerical Computation 
% =================================================================================
\lecture{Math and Machine Learning Basics}{Numerical Computation (Ch. 4)}{January 24, 2017}

\p \blue{Some terminology}. \green{Underflow} is when numbers near zero are rounded to zero. Similarly, \green{overflow} is when large [magnitude] numbers are approximated as $\pm \infty$. \green{Conditioning} refers to how rapidly a function changes w.r.t. small changes in its inputs. Consider the function $f(\vec{x}) = \matr{A}^{-1}\vec{x}$. When $\matr{A}$ has an eigenvalue decomposition, its \textit{condition number} is
\begin{align}
\max_{i, j} \bigg| \frac{\lambda_i}{\lambda_j} \bigg| \tlab{4.2}
\end{align}
which is the ratio of the magnitude of the largest and smallest eigenvalue. When this is large, matrix inversion is sensitive to error in the input [of f(x)]. \\

\myspace
\blue{Gradient-based optimization}. Recall from basic calculus that the \green{directional derivative} of $f(\vec x)$ in direction $\vec{\hat{u}}$ (a unit vector) is defined as the slope of the function $f$ in direction $\vec{\hat u}$. By definition of the derivative, this is given by (with $\vec v := \vec x + \alpha \vec{\hat u}$)
\begin{align}
	\lim_{\alpha \rightarrow 0} \frac{f(\vec x + \alpha \vec{\hat u}) - f(\vec{x})   }{\alpha} &= \pderiv{f(\vec{x} + \alpha \vec{\hat u})}{\alpha} \bigg|_{\alpha=0} \\
	&= \sum_i \pderiv{f(\vec v)}{v_i}\pderiv{v_i}{\alpha} ~ \bigg|_{\alpha=0}  \\
	&= \sum_i (\nabla_{\vec v} f(\vec v))_i ~  u_i ~ \bigg|_{\alpha=0} \\
	&= \vec{\hat u}^T \nabla_{\vec v} f(\vec v) \bigg|_{\alpha=0} \\
	&= \vec{\hat u}^T \nabla_{\vec x} f(\vec x) \label{directional-deriv}
\end{align}
where it's important to recognize the distinction between $\lim_{\alpha \rightarrow 0}$ and \textit{setting} $\alpha$ to zero, which is denoted by $\big|_{\alpha=0}$. If we want to \textit{find} the direction $\vec{\hat u}$ such that this directional derivative is a minimum, i.e. 
\begin{align}
	\vec{\hat u}^* &= \argmin_{\vec{\hat u}, \vec{\hat u}^T \vec{\hat u} = 1} \vec{\hat u}^T \nabla_{\vec x} f(\vec x) \\
	&=  \argmin_{\vec{\hat u}, \vec{\hat u}^T \vec{\hat u} = 1} ||\vec{\hat u}||_2 ~ ||\nabla_{\vec x} f(\vec x)||_2 ~ \cos(\vec{\theta}) \\
	&= \cos(\vec{\theta})
\end{align}
and we see that $\vec{\hat u}$ points in the opposite direction as the gradient. 

\myspace
\p \blue{Jacobian and Hessian Matrices}. For when we want partial derivatives of some function f\marginnote{$f: \R^m \rightarrow \R^n$} whose input and output are both vectors. The \textbf{Jacobian matrix}\marginnote{$\matr{J} \in \R^{n \times m}$ where $J_{i,j} = \pderiv{}{x_j} f(\vec{x})_i$}[1em] contains all such partial derivatives. Sometimes we want to know about second derivatives too, since this tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. The \textbf{Hessian matrix} $\matr{H}(f)(\vec{x})$ is defined such that \marginnote{The Hessian is the Jacobian of the gradient.}
\begin{align}
\matr{H}(f)(\vec{x})_{i,j} &= \pderiv{^2}{x_i\partial x_j} f(\vec{x}) \tlab{4.6}
\end{align}
The second derivative in a specific direction $\vec{\hat{d}}$ is given by $\hat{\vec{d}}^T \matr{H} \hat{\vec{d}}$\footnote{
	In the same manner that I derived equation \ref{directional-deriv}, we can derive the second derivative in a specified direction $\vec{\hat d}$:
	\begin{align}
		\ppderiv{}{\alpha} f(\vec{x} + \alpha \vec{\hat d}) \big|_{\alpha=0}
		&= \pderiv{}{\alpha} \vec{\hat d}^T \nabla_{\vec{v}} f(\vec{v}) \big|_{\alpha=0} \\
		&= \sum_i d_i \pderiv{}{\alpha} \pderiv{f(\vec v)}{v_i} ~ \big|_{\alpha=0} \\
		&= \sum_i d_i \pderiv{}{v_i} \pderiv{f(\vec v)}{\alpha} ~ \big|_{\alpha=0} \\
		&= \sum_i \sum_j d_i \pderiv{^2f(\vec v) }{v_i v_j} d_j ~ \big|_{\alpha=0} \\
		&= \vec{\hat d}^T \matr{H} \vec{\hat d}
	\end{align}
}. It tells us how well we can expect a gradient descent step to perform. How so? Well, it shows up in the second-order approximation to the function $f(\vec{x})$ about our current spot, which we can denote $\vec{x}^{(0)}$. The standard gradient descent step will move us from $\vec{x}^{(0)} \rightarrow \vec{x}^{(0)} - \epsilon g$, where $g$ is the gradient evaluated at $\vec{x}^{(0)}$. Plugging this in to the 2nd order approximation shows us how $\matr{H}$ can give information related to how ``good'' of a step that really was. Mathematically, 
\begin{align}
	f(\vec{x}) &\approx f(\vec{x}^{(0)}) + (\vec{x} - \vec{x}^{(0)})^T \vec{g} + \onehalf (\vec{x} - \vec{x}^{(0)})^T \matr{H} (\vec{x} - \vec{x}^{(0)}) \tlab{4.8} \\
	f(\vec{x}^{(0)} - \epsilon \vec{g}) &\approx f(\vec{x}^{(0)}) - \epsilon \vec{g}^T\vec{g} + \onehalf \epsilon^2 \vec{g}^T \matr{H} \vec{g} \tlab{4.9}
\end{align}
If $\vec{g}^T \matr{H} \vec{g}$ is positive, then we can easily solve for the optimal $\epsilon = \epsilon^*$ that decreases the Taylor series approximation as
\begin{align}
\epsilon^* &= \frac{\vec{g}^T\vec{g} }{\vec{g}^T \matr{H} \vec{g} } \tlab{4.10}
\end{align}
which can be as low as $1/\lambda_{max}$ (the worst case), and as high as $1/\lambda_{min}$ with the $\lambda$ being the eigenvalues of the Hessian. The best (and perhaps only) way to take what we learned about the ``second derivative test'' in single-variable calculus and apply it to the multidimensional case with $\matr{H}$ is by using the \textit{eigendecomposition of} $\matr{H}$. Why? Because we can examine the eigen\underline{values} of the Hessian to determine whether the critical point $\vec{x}^{(0)}$ is a local maximum, local minimum, or saddle point\footnote{Emphasis on ``values'' in ``eigen\underline{values}'' because it's important not to get tripped up here about what the eigenvectors of the Hessian mean. The reason for the decomposition is that it gives us an orthonormal basis (out of which we can get any direction) and therefore the magnitude of the second derivative along each of these directions as the eigenvalues.}. If all eigenvalues are positive (remember that this is equivalent to saying that the Hessian is \textbf{positive definite}!)\marginnote{The condition number of the Hessian at a given point can give us an idea about how much the second derivatives (along different directions) differ from each other}, the point is a local minimum.

\myspace
\p \blue{Constrained optimization}: minimizing/maximizing a function $f(\vec x)$ constrained to only values of $\vec x$ in some set $\mathbb{S}$. One way of approaching such a problem is to re-design the unconstrained optimization problem such that the re-designed problem's solution satisfies the constraints. For example, to minimize $f(\vec x)$ for $\vec x \in \R^2$ with constraint $||\vec x||_2 = 1$, we can minimize $g(\theta) = f([\cos\theta, \sin\theta]^T)$ wrt $\theta$, then return $[\cos\theta, \sin\theta]^T$ as the solution to the original problem.  \\

The \green{Karush-Kuhn-Tucker} (KKT) approach, a generalization of \green{Lagrange multipliers}, provides a general approach for re-designing the optimization problem, with procedure as follows:
\begin{compactenum}
	\item Find $m$ functions $g^{(i)}$ and $n$ functions $h^{(j)}$ such that your set of allowed values $\mathbb{S}$ can be written
	\begin{align}
		\mathbb{S} = \{ \vec x \mid \forall i, g^{(i)}(\vec x) = 0 \text{ and } \forall j, h^{(j)}(\vec x) \le 0   \}
	\end{align}
	The equations involving $g^{(i)}$ are called the \green{equality constraints} and the inequalities involving $h^{(j)}$ are called the \green{inequality constraints}. 
	
	\item Introduce new variables $\lambda_i$ (for the equality constraints) and $\alpha_j$ (for the inequality constraints). These are called the KKT multipliers. The generalized Lagrangian is then defined as
	\graybox{
		\mathcal{L}(\vec x, \vec{\lambda}, \vec{\alpha})
		&= f(\vec x) + \sum_i \lambda_i g^{(i)}(\vec x) + \sum_j \alpha_j h^{(j)}(\vec x)	
	}
	
	\item Solve the re-designed unconstrained optimization problem:
	\begin{align}
		\min_{\vec x} \max_{\vec{\lambda}} \max_{\vec{\alpha}, \vec{\alpha} \ge 0}
		\mathcal{L}(\vec x, \vec{\lambda}, \vec{\alpha}) \label{KKT-opt}
	\end{align}
	which has the same optimal objective function value and set of optimal points $\vec x$ as the original constrained problem, $\min_{\vec x \in \mathbb{S}} f(\vec x)$. Any time the constraints are satisfied, the expression $\max_{\vec{\lambda}} \max_{\vec{\alpha}, \vec{\alpha} \ge 0}
	\mathcal{L}(\vec x, \vec{\lambda}, \vec{\alpha})$ evaluates to $f(\vec x)$, and any time a constraint is violated, the same expression evaluates to $\infty$. 
	
\end{compactenum}

% =================================================================================
% Numerical Computation 
% =================================================================================
\lecture{Math and Machine Learning Basics}{Machine Learning Basics (Ch. 5)}{January 25, 2017}

\p \blue{Capacity, Overfitting, and Underfitting}. Difference between ML and optimization is that, in addition to wanting low training error, we want \green{generalization error} (test error) to be low as well. The ideal model is an oracle that simply knows the true probability distribution $p(\vec{x}, y)$ that generates the data. The error incurred by such an oracle, due things like inherently stochastic mappings from $\vec{x}$ to $y$ or other variables, is called the \green{Bayes error}. The \green{no free lunch theorem} states that, averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points. Therefore, the goal of ML research is to understand what kinds of distributions are relevant to the ``real world'' that an AI agent experiences, and what kinds of ML algorithms perform well on data drawn from the relevant data-generating distributions.

\myspace\myspace
% --------------------------------------------
\subsub{Estimators, Bias and Variance (5.4)}
% --------------------------------------------

\myspace
\p \blue{Point Estimation}: attempt to provide ``best'' prediction of some quantity, such as some parameter or even a whole function. Formally, a point estimator or \textit{statistic} is any function of the data:
\begin{align}
\hat{\theta}_m = g\left(x^{(1)}, \ldots, x^{(m)}\right) \tlab{5.19}
\end{align}
where, since the data is drawn from a random process, $\hat{\theta}$ is a random variable. \textbf{Function estimation} is identical in form, where we want to estimate some $f(x)$ with $\hat f$, a point estimator in \textit{function space}. 

\myspace
\p \blue{Bias}. Defined below, where the expectation is taken over the data-generating distribution\footnote{May want to double-check this, but I'm fairly certain this is what the book meant when it said ``data,'' based on later examples.}. Bias measures the expected deviation from the true value of the func/param.
\graybox{
	\mathrm{bias}\left[\hat{\theta}_m\right] = \E{\hat{\theta}_m} - \theta \tlab{5.20}
}
\red{TODO:} Figure out how to derive $\E{\hat{\theta}_m^2}$ for Gaussian distribution \href{http://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable}{[helpful link]}. 

\myspace\Needspace{19\baselineskip}
\p \blue{Bias-Variance Tradeoff}. 
\begin{compactitem}[$\rightarrow$]
	\item \textbf{Conceptual Info}. Two sources of error for an estimator are (1) bias and (2) variance, which are both defined as deviations from a certain value. Bias gives deviation from the \textit{true} value, while variance gives the [expected] deviation from this \textit{expected} value. 
	
	\item \textbf{Summary of main formulas}. 
	\begin{align}
	\mathrm{bias}\left[\hat{\theta}_m\right] &= \E{\hat{\theta}_m} - \theta \\
	\Var{\hat\theta_m} &= \E{\left(\hat{\theta}_m - \E{\hat{\theta}_m} \right)^2}
	\end{align}
	
	\item \textbf{MSE decomposition}. The MSE of the estimates is given by\footnote{Derivation:
		\begin{align}
		\mathrm{MSE} &= \E{\hat{\theta}^2 + \theta^2 - 2 \theta \hat{\theta}} \\
		&= \E{\hat{\theta}^2} + \theta^2 - 2\theta\E{\hat{\theta}} \\
		&= (\E{\hat{\theta}}^2 - \E{\hat{\theta}}^2)
		+ \E{\hat{\theta}^2} + \theta^2 - 2\theta\E{\hat{\theta}} \\
		&= \left(\E{\hat{\theta}}^2  + \theta^2 - 2\theta\E{\hat{\theta}}  \right) + \left(\E{\hat{\theta}^2} -  \E{\hat{\theta}}^2\right) \\
		&= \mathrm{Bias}(\hat{\theta})^2 + \Var{\hat{\theta}_m} 
		\end{align}
	}
	\graybox{
		\mathrm{MSE} &= \E{(\hat{\theta}_m - \theta)^2} \tlab{5.53} \\
		&= \mathrm{Bias}(\hat{\theta})^2 + \Var{\hat{\theta}_m} \tlab{5.54}
	}
	and desirable estimators are those with low MSE. 
\end{compactitem}

\myspace
\p \blue{Consistency}. As the number of training data points increases, we want the estimators to converge to the true values. Specifically, below are the definitions for \textit{weak} and \textit{strong} consistency, respectively. 
\begin{align}
\begin{split}
\mathrm{plim}_{m \rightarrow \infty} \hat{\theta}_m &= 0 \\
p\left(\lim_{m \rightarrow \infty} \hat{\theta}_m = \theta \right) &= 1
\end{split}\tlab{5.55}
\end{align}
where the symbol $\mathrm{plim}$ means $P(|\hat{\theta}_m - \theta| > \epsilon) \rightarrow 0$ as $m \rightarrow \infty$. 


\myspace\myspace
\subsub{Maximum Likelihood Estimation (5.5)}

\p Consider set of $m$ examples $\set{X} = \{\vec{x}^{(1)}, \ldots, \vec{x}^{(m)} \}$ drawn independently from the true (but unknown) $p_{data}(\rvec{x})$. Let $p_{model}(\rvec{x}; \vec{\theta})$ be parametric family of probability distributions over the same space indexed by $\vec{\theta}$. The maximum likelihood estimator for $\vec{\theta}$ can be expressed as
\graybox{
	\vec[ML]{\theta} &= \argmax_{\vec{\theta}} \E[\rvec{x} \sim \hat{p}_{data}]{\log p_{model}(\vec{x}; \vec{\theta})} \tlab{5.59}
}
where we've chosen to express with $\log$ for underflow/gradient reasons. One interpretation of ML is to view it as minimizing the dissimilarity, as measured by the KL divergence\footnote{
	The KL divergence is given by
	\begin{align}
	D_{KL} (\hat{p}_{data} || p_{model}) 
	= \E[\rvec{x} \sim \hat{p}_{data}]{\log \hat p_{data}(\vec{x}) - \log p_{model}(\vec{x}) } \tlab{5.60}
	\end{align}
}, between $\hat{p}_{data}$ and $p_{model}$. 
\begin{quote}
	Any loss consisting of a negative log-likelihood is a \textbf{cross-entropy} between the $\hat p_{data}$ distribution and the $p_{model}$ distribution.
\end{quote}


\p \underline{Thoughts}: Let's look at $D_{KL}$ in some more detail. First, I'll rewrite it with the explicit definition of $\E[\vec{x} \sim \hat{p}_{data}]{\log\left( \hat{p}_{data}(\vec{x}) \right)}$:
\begin{align}
	D_{KL} (\hat{p}_{data} || p_{model}) 
	&= \E[\vec{x} \sim \hat{p}_{data}]{
			\log\left( \hat{p}_{data}(\vec{x}) \right)
		  - \log\left( p_{model}(\vec{x}) \right)
	} \\
	&= \left( \frac{1}{N} \left( \sum_{i = 1}^{N} \log\left( \text{Counts}(\vec[i]{x})\right) \right)
		- \log N \right)
	- \E[\vec{x} \sim \hat{p}_{data}]{ \log\left( p_{model}(\vec{x}) \right)}
\end{align}
Note also that our goal is to find parameters $\vec{\theta}$ such that $D_{KL}$ is minimized. It is for \textit{this} reason, that we wish to optimize over $\vec{\theta}$, that minimizing $D_{KL}$ amounts to maximizing the quantity, $\E[\vec{x} \sim \hat{p}_{data}]{ \log\left( p_{model}(\vec{x}) \right)}$. Sure, I can agree this is \textit{true}, but \textbf{why is our goal to minimize $D_{KL}$, as opposed to minimizing $\mid D_{KL} \mid$?} I'm assuming it is because optimizing w.r.t. an absolute value is challenging numerically.




\myspace
\p \blue{Conditional Log-Likelihood and MSE}. We can readily generalize $\vec[ML]{\theta}$ to estimate a conditional probability $p(\rvec{y} \mid \rvec{x}; \vec{\theta})$ in order to predict $\rvec{y}$ given $\rvec{x}$, since\marginnote{We are assuming the examples are i.i.d. here.}
\begin{align}
\vec[ML]{\theta} = \argmax_{\vec{\theta}} \sum_{i = 1}^{m} \log P(\vec{y}^{(i)} \mid \vec{x}^{(i)} ; \vec{\theta}) \tlab{5.63}
\end{align}
where $\vec{x}^{(i)}$ are fed as \textit{inputs} to the model; this is why we can formulate MLE as a conditional probability. 

\myspace\myspace
\subsub{Bayesian Statistics (5.6)}

Distinction between frequentist and bayesian approach:
\begin{compactitem}
	\item \textbf{Frequentist:} Estimate $\vec{\theta}$ $\longrightarrow$ make predictions thereafter based on this estimate.
	\item \textbf{Bayesian:} Consider all possible values of $\vec{\theta}$ when making predictions.
\end{compactitem}

\myspace
\p \blue{The prior}. Before observing the data, we represent our knowledge of $\vec{\theta}$ using the \textbf{prior probability distribution} $p(\vec{\theta})$\marginnote{It is common to choose a high-entropy prior, e.g. uniform.}. Unlike maximum likelihood, which makes predictions using a \textit{point estimate} of $\vec{\theta}$ (a single value), the Bayesian approach uses Bayes' rule to make predictions using the \textit{full distribution} over $\vec{\theta}$. In other words, rather than focusing on the most accurate value estimate of $\vec{\theta}$, we instead focus on pinning down a range of possible $\vec{\theta}$ values and how likely we believe each of these values to be. \\

So what happens to $\vec{\theta}$ after we observe the data? We update it using Bayes' rule\footnote{In practice, we typically compute the denominator by simply normalizing the probability distribution, i.e. it is effectively the partition function.}:
\begin{align}
	p(\vec{\theta} \mid x^{(1)}, \ldots, x^{(m)}) 
	&= \frac{ p(x^{(1)}, \ldots, x^{(m)} \mid \vec{\theta} ) p(\vec{\theta}) }{ p(x^{(1)}, \ldots, x^{(m)}) } 
\end{align}
Note that we still haven't mentioned how to actually make \textit{predictions}. Since we no longer have just one value for $\vec{\theta}$, but rather we have a posterior \textit{distribution} $p(\vec{\theta} \mid x^{(1)}, \ldots, x^{(m)})$, we must integrate over this to get the predicted likelihood of the next sample $x^{(m+1)}$:
\begin{align}
	p(x^{(m+1)} \mid x^{(1)}, \ldots, x^{(m)}) 
	&= \int p(x^{(m+1)} \mid \vec{\theta} ) p(\vec{\theta} \mid x^{(1)}, \ldots, x^{(m)}) \mathrm{d}\vec{\theta} \\
	&= \E[\vec{\theta} \sim p(\vec{\theta} \mid x^{(1)}, \ldots, x^{(m)})]{p(x^{(m+1)} \mid \vec{\theta} )}
\end{align}

\myspace
\p \blue{Linear Regression: MLE vs. Bayesian}. Both want to model the conditional distribution $p(y \mid \vec{x})$ (the conditional likelihood). To derive the standard linear regression algorithm, we \textit{define}\marginnote{Assume $\sigma^2$ is some fixed constant chosen by the user.}[2em]
\begin{align}
p(y \mid \vec{x}) &= \mathcal{N} \left(y; ~ \hat y(\vec{x}; \vec{w}), ~ \sigma^2 \right) \label{cl-lr}\\
\hat y(\vec{x}; \vec{w}) &= \vec{w}^T \vec{x}
\end{align}

\Needspace{10\baselineskip}
\begin{compactitem}
	\item \textbf{Maximum Likelihood Approach}: We can use the definition above (and the i.i.d. assumption) to evaluate the conditional log-likelihood as
		\begin{align}
		\sum_{i = 1}^{m} \log p(y^{(i)} \mid \vec{x}^{(i)} ; \vec{\theta}) 
		= -m\log\sigma - \frac{m}{2} \log(2\pi) - \sum_{i = 1}^{m} \frac{|| \hat y^{(i)} - y^{(i)} ||^2}{2\sigma^2} \tlab{5.65}
		\end{align}
		where only the last term has any dependence on $\vec{w}$. Therefore, to obtain $\vec[ML]{w}$ we take the derivative of the last term w.r.t. $\vec{w}$, set that to zero, and solve for $\vec{w}$. We see that finding the $\vec{w}$ that maximizes the conditional log-likelihood is equivalent to finding the $\vec{w}$ that minimizes the training MSE\marginnote{Recall that the training MSE is $\frac{1}{m} \sum_{i = 1}^{m} || \hat y^{(i)} - y^{(i)} ||^2$}[-2em]. 
		
	\item \textbf{Bayesian Approach}: Our conditional likelihood is already given in equation \ref{cl-lr}. Next, we must define a prior distribution over $\vec{w}$. As is common, we choose a Gaussian prior to express our high degree of uncertainty about $\vec{\theta}$ (implying we'll choose a relatively large variance):\marginnote{Typically assume $ \matr[0]{\Lambda} = \diag{\vec[0]{\lambda}} $}[1em]
	\begin{align}
		p(\vec{w}) := \mathcal{N}(\vec{w}; \vec[0]{\mu}, \matr[0]{\Lambda} )
	\end{align}
	We can then compute [the unnormalized] $p(\vec{w} \mid \matr{X}, \vec{y}) \propto p(\vec{y} \mid \matr{X}, \vec{w}) p(\vec{w})$ [and then normalize it].
\end{compactitem}

\myspace
\p \blue{Maximum A Posteriori (MAP) Estimation}. Often we either prefer a point estimate for $\theta$, or we find out that computing the posterior distribution is intractable and a point estimate offers a tractable estimation. The obvious way of obtaining this while still taking the Bayesian route is to just argmax the posterior and use that as your point estimate:
\begin{align}
	\vec[MAP]{\theta} &= \argmax_{\vec{\theta}} p(\vec{\theta} \mid \vec{x}) = \argmax_{\vec{\theta}} \log p\left( \vec{x} \mid \vec{\theta} \right) + \log p(\vec{\theta})
\end{align}
where the second form shows how this is basically maximum likelihood with incorporation of the prior. We don't want just any $\vec{\theta}$ that maximizes the likelihood of our data if there is virtually no chance of that value of $\vec{\theta}$ in the first place.

\myspace\Needspace{16\baselineskip}
\subsub{Supervised Learning Algorithms (5.7)}

\myspace
\p \blue{Logistic Regression}. We've already seen that linear regression corresponds to the family
\begin{align}
	p(y \mid \vec{x}) = \mathcal{N} \left(y; ~ \vec{\theta}^T\vec{x}, ~ \matr{I} \right) \tlab{5.80}
\end{align}
which we can generalize to the binary \textbf{classification} scenario by interpreting as the probability of class 1. One way of doing this while ensuring the output is between 0 and 1 is to use the logistic sigmoid function:\marginnote{Equation ~\ref{5.81} is the definition of logistic regression}[2em]
\graybox{
	p(y = 1 ~ \mid ~ \vec{x}; \vec{\theta}) = \sigma(\vec{\theta}^T\vec{x}) \tlab{5.81}
}
Unfortunately, \underline{there is no closed-form solution for $\vec{\theta}$}, so we must search via maximizing the log-likelihood.

\myspace
\p \blue{Support Vector Machines}. Driving by a linear function $\vec{w}^T\vec{x} + \vec{b}$ like logistic regression, but instead of outputting probabilities it outputs a class identity, which depends on the sign of $\vec{w}^T\vec{x} + \vec{b}$. SVMs make use of the \green{kernel trick}, the ``trick'' being that we can rewrite $\vec{w}^T\vec{x} + \vec{b}$ completely in terms of dot products between examples. The general form of our prediction function becomes\marginnote{If our kernel function is just $k(x, x^{(i)}) = x^Tx^{(i)}$ then we've just rewritten $\vec{w}$ in the form $\vec{w} \rightarrow \matr{X}^T \vec{\alpha}$}
\graybox{
	f(\vec{x}) = b + \sum_i \alpha_i k(\vec{x}, \vec{x}^{(i)}) \tlab{5.83}
}
where the \textit{kernel} [function] takes the general form $k(\vec{x}, \vec{x}^{(i)}) = \phi(\vec{x}) \cdot  \phi(\vec{x}^{(i)})$. A major drawback to kernel machines (methods) in general is that the cost of evaluating the decision function $f(\vec{x})$ is linear in the number of training examples. SVMs, however, are able to mitigate this by learning an $\alpha$ with mostly zeros. The training examples with \textit{nonzero} $\alpha_i$ are known as \green{support vectors}. 







% ==================================================================================
% DEEP NETWORKS: MODERN PRACTICES
% ==================================================================================
\mysection{Deep Networks: Modern Practices}\label{Modern Practices}




\lecture{Modern Practices}{Deep Feedforward Networks (Ch. 6)}{January 26}

The strategy/purpose of [feedforward] deep learning is to \textit{learn the set of features/representation describing $\vec{x}$} with a mapping $\phi$ before applying a linear model. In this approach, we have a model 
\[
	y = f(\vec{x}; ~ \vec{\theta}, \vec{w}) = \phi(\vec{x}; \vec{\theta})^T \vec{w}
\]
with $\phi$ defining a hidden layer.

\myspace
\p \blue{ReLUs and their generalizations}. Some nice properties of ReLUs are\textellipsis\marginnote{Recall the ReLU activation function: $g(z) = \max\{0, z\}$}[1em]
\begin{compactitem}
	\item Derivatives through a ReLU remain large and consistent whenever the unit is active. 
	\item Second derivative is 0 a.e.\marginnote{a.e. is short for ``almost everywhere''} and the derivative is 1 everywhere the unit is active, meaning the gradient direction is more useful for learning than it would be with activation functions that introduce 2nd-order effects (see equation ~\ref{4.9})
\end{compactitem}

\myspace
\p \blue{Generalizing to aid gradients when $\vec{z} < 0$}. Three such generalizations are based on using a nonzero slope $\alpha_i$ when $z_i < 0$:
\begin{align}
	h_i = g(\vec{z}, \vec{\alpha})_i = \max(0, z_i) + \alpha_i \min(0, z_i)
\end{align}
\begin{compactitem}[$\rightarrow$]
	\item Absolute value rectification: fix $\alpha_i = -1$ to obtain $g(z) = |z|$. 
	\item Leaky ReLU: fix $\alpha_i$ to a small value like 0.01.
	\item Parametric ReLU (PReLU): treats $\alpha_i$ like a learnable parameter.
\end{compactitem}

\myspace
\p \blue{Logistic sigmoid and hyperbolic tangent}. Sigmoid activations on hidden units is a bad idea, since they're only sensitive to their inputs near zero, with small gradients everywhere else. If sigmoid activations must be used, $\tanh$ is probably a better substitute, since it resembles the identity (i.e. a linear function) near zero. 

% ===============================================
\myspace\Needspace{15\baselineskip}
\subsub{Back-Propagation (6.5)}
% ===============================================

\myspace
\p \blue{The chain rule}. Suppose $\vec{z} = f(\vec{y})$ where $\vec{y} = g(\vec{x})$ (see margin for dimensions)\marginnote{
\[\vec{x} \in \R^{m} \] \[\vec{y} \in \R^{n} \] \[z: \R^{n}\rightarrow \R \] \[g: \R^{m}\rightarrow \R^n \]}. Then\footnote{Note that we can view $z = f(\vec{y})$ as a multi-variable function of the dimensions of $\vec{y}$, $$ z = f(y_1, y_2, \ldots, y_n)$$},
\begin{align}
\begin{split}
	\pderiv{z}{x_i} = (\nabla_{\vec{x}} z)_i 
		&= \sum_{j = 1}^{n} \pderiv{z}{y_j}\pderiv{y_j}{x_i} 
		= \sum_{j = 1}^{n}  (\nabla_{\vec{y}} z)_j \pderiv{y_j}{x_i}
		= \sum_{j = 1}^{n}  (\nabla_{\vec{y}} z)_j  ~  (\nabla_{\vec{x}} y_j)_i
\end{split}\tlab{6.45} \\
\rightarrow
\begin{split}
 \nabla_{\vec{x}} z &= \left( \pderiv{\vec{y}}{\vec{x}} \right)^T \nabla_{\vec{y}} z  
 	= \matr[y=g(\vec{x})]{J}^T \nabla_{\vec{y}} z  
\end{split}\tlab{6.46}
\end{align}
\begin{quote}
	\textbf{From this we see that the gradient of a variable $\vec{x}$ can be obtained by multiplying a Jacobian matrix $\pderiv{\vec{y}}{\vec{x}}$ by a gradient $ \nabla_{\vec{y}} z$.}
\end{quote}









\lecture{Modern Practices}{Regularization for Deep Learning (Ch. 7)}{January 12, 2017}

Recall the definition of regularization: ``any modification we make to a learning algorithm that is intented to reduce its generalization error but not its training error.'' 


\myspace
\p \blue{Limiting Model Capacity}. Recall that \textbf{Capacity} [of a model] is the ability to fit a wide variety of functions. Low cap models may struggle to fit training set, while high cap models may overfit by simply memorizing the training set. We can limit model capacity by adding a parameter norm penalty $\Omega(\theta)$ to the objective function $J$:
\graybox{
	\widetilde{J}(\theta; X, y) = J(\theta; X, y) + \alpha \Omega(\theta) \quad\text{where}\quad \alpha \in [0, \infty) \tlab{7.1}
	}
where we typically choose $\Omega$ to only penalize the \textit{weights} and leave biases unregularized.

\myspace
\p \blue{L2-Regularization}. Defined as setting $\Omega(\theta)= \frac{1}{2}||w||_2^2$. Assume that $J(w)$ is quadratic, with minimum at $w^*$. Since quadratic, we can approximate $J$ with a second-order expansion about $w^*$. 
\begin{align}
\hat J(w) &= J(w^*) + \frac{1}{2}(w - w^*)^T H (w - w^*) \tlab{7.6} \\
\nabla_w \hat J(w) &= H(w - w^*) \tlab{7.7}
\end{align}
where $H_{ij} = \pderiv{^2J}{w_iw_j}\big|_{w^*}$. If we add in the [derivative of] the weight decay and set to zero, we obtain the solution
\graybox{
	\widetilde{w} &= (H + \alpha I)^{-1} Hw^* \tlab{7.10}\\
	&= Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^* \tlab{7.13}
	}
which shows that the effect of regularization is to rescale the $i$ eigenvectors of $H$ by $\frac{\lambda_i}{\lambda_i + \alpha}$. This means that eigenvectors with $\lambda_i >> \alpha$ are relatively unchanged, but the eigenvectors with $\lambda_i << \alpha$ are shrunk to nearly zero. In other words, only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact.


\myspace 
\p \blue{Sparse Representations}. Weight decay acts by placing a penalty directly on the model parameters. Another strategy is to place a penalty on the \textit{activations} of the units, encouraging their activations to be sparse. It's important to distinguish the difference between sparse parameters and sparse \textit{representations}. In the former, if we take the example of some $\vec{y} = \matr{B} \vec{h}$, there are many zero entries in some parameter matrix $\matr{B}$ while, in the latter, there are many zero entries in the representation vector $\vec{h}$. The modification to the loss function, analogous to ~\ref{7.1}, takes the form
\graybox{
	\widetilde{J}(\vec\theta; \matr X, \vec y) = J(\vec\theta; \matr X, \vec y) + \alpha \Omega(\vec{h}) \quad\text{where}\quad \alpha \in [0, \infty) \tlab{7.48}
}


\myspace
\p \blue{Adversarial Training} Even for networks that perform at human level accuracy have a nearly 100 percent error rate on examples that are intentionally constructed to search for an input $\vec{x}'$ near a data point $\vec{x}$ such that the model output for $\vec{x}'$ is very different than the output for $\vec{x}$\marginnote{In many cases, $\vec{x}'$ can be so similar to $\vec{x}$ that a human cannot tell the difference!}.
\begin{align}
\vec{x}' ~ \longleftarrow  ~ \vec{x} + \epsilon \cdot \mathrm{sign}\left(\nabla_{ \vec{x} } J(\vec\theta; \vec x, \vec y) \right) 
\end{align}
In the context of regularization, one can reduce the error rate on the original i.i.d. test set via \green{adversarial training} -- training on adversarially perturbed training examples.





% ============================================================================
% OPTIMIZATION
% ============================================================================
\lecture{Modern Practices}{Optimization for Training Deep Models (Ch. 8)}{Feb 17, 2017}

\blue{Empirical Risk Minimization}. The ultimate goal of any machine learning algorithm is to reduce the expected generalization error, also called the \green{risk}:\marginnote{risk definitions}[2em]
\begin{align}
	J^*(\vec \theta) &= \E[(\vec x, y) \sim p_{data}]{ L(f(\vec x; \vec \theta), y)  }
\end{align}
with emphasis that the risk is over the \textit{true} underlying data distribution $p_{data}$. If we knew $p_{data}$, this would be an \underline{optimization} problem. Since we don't, and only have a set of training samples, it is a \underline{machine learning} problem. However, we can still just minimize the \green{empirical risk}, replacing $p_{data}$ in the equation above with $\hat p_{data}$\footnote{This amount to a simple average over the loss function at each training point.}. \\

So, how is minimizing the empirical risk any different than familiar gradient descent approaches?\marginnote{ERM $\ne$ GD} Aren't they designed to do just that? Well, sort of, but it's technically not the same. When we say ``minimize the empirical risk'' in the context of optimization, we mean this very literally. Gradient descent methods emphatically do \textit{not} just go and \textit{set} the weights to values such that the empirical risk reaches its lowest possible value -- that's not machine learning. Furthermore, many useful loss function such as 0-1 loss\footnote{The 0-1 loss function is defined as
	\begin{align}
		L(\hat y, y) = I(\hat y \ne y)
	\end{align}
} do not have useful derivatives. 

\myspace
\p \blue{Surrogate Loss Functions and Early Stopping}. In cases such as 0-1 loss, where minimization is intractable, one typically optimizes a \green{surrogate loss function} instead, such as the  negative log-likelihood of the correct class. Also, an important difference between pure optimization and our training algorithms is that the latter usually don't halt at a local minimum. Instead, we usually must define some early stopping condition to terminate training before overfitting begins to occur.  
\vspace{-0.5em}
\begin{quote}
	{\small\itshape
		We want to minimize the risk, but we don't have access to $p_{data}$, so \textellipsis \\
		We want to minimize the empirical risk, but it's prone to overfitting and our loss function's derivative may be zero/undefined, so \textellipsis \\
		We minimize a surrogate loss function iteratively over minibatches until early stopping is triggered.
	}
\end{quote}

\myspace
\p \blue{Batch and Minibatch Algorithms}. Computing $\nabla_{\vec \theta} J(\vec \theta)$ as an expectation over the entire training set is expensive, so we typically compute the expectation over a small subset of the examples. Recall that the standard deviation, or standard error $SE(\mu_m)$, of the mean taken over some subset of $m \le n$ samples, $\mu_m = \inv{m} \sum_{i \sim Rand(0,~n,~ size=m)} x^{(i)}$, is given by $\sigma / \sqrt{m}$, where $\sigma$ is the true [sample] standard deviation of the full $n$ data samples. In other words, to improve such a gradient by a factor of 10 requires 100 times more samples-per-batch (and thus ~100 times more computation). For this reason, most optimization algorithms actually \textit{converge} much faster if they can rapidly compute approximate estimates of the gradient (re: smaller batches) rather than slowly computing the exact gradient. \\

\p The key points to consider when choosing your batch size:
\begin{compactenum}
	\item Larger batches = more accurate estimates of the gradient, but with less than linear returns.
	
	\item If examples in the batch are processed in parallel (as is typical), then memory roughly scales with batch size.
	
	\item Small batches can offer a regularizing effect. Generalization error is often best for a batch size of 1. However, this requires a low learning rate to maintain stability and thus a longer overall training runtime. 
\end{compactenum}
Also, note that online SGD, where we never reuse data points, but simply update parameters as new data comes in, gives an unbiased estimator of the exact gradient of the generalization error (the risk). Once data samples are reused (e.g. when training with multiple epochs), the gradient estimates become biased. The interesting point here is that the availability of increasingly massive datasets is making single-epoch\footnote{Or even less, i.e. not using all of the training data.} training more common. In such cases, \textit{overfitting is no longer an issue}, but rather underfitting and computational efficiency. 

\p \blue{Ill-conditioning} of the Hessian matrix $\matr{H}$ can cause SGD to get ``stuck'' in the sense that even very small steps increase the cost function. Recall that a second-order Taylor series expansion of the cost function predicts that an SGD step of $-\epsilon \vec{g}$ will add 
\begin{align}
	 \onehalf \epsilon^2 \vec{g}^T \matr{H} \vec{g}  - \epsilon \vec{g}^T\vec{g} \label{delta-sgd}
\end{align}
to the cost. If $\matr{H}$ has a large condition number (re: if $\matr{H}$ is ill-conditioned), then the range of possible values, $[1/\lambda_{max}, 1/\lambda_{min}]$, for $ \vec{g}^T \matr{H} \vec{g} $ can become very large. In particular, if $ \vec{g}^T \matr{H} \vec{g} $ exceeds $\epsilon \vec{g}^T \vec{g}$, then the SGD step will \textit{increase} the cost!

\myspace
\p \blue{Training algorithms}. Below, I list some popular training algorithms and their update equations. 

\begin{itemize}
	\item \textbf{SGD}.
	\begin{align}
		\vec g &\leftarrow \inv{m} \nabla_{\vec \theta} \sum_i^m L\left(  f(\vec{x}^{(i)}; \vec \theta), \vec{y}^{(i)}   \right) \\
		\vec{\theta} &\leftarrow \vec \theta - \epsilon \vec g
	\end{align}
	
	\item \textbf{Momentum}.
	\begin{align}
		\vec{g} &\leftarrow \inv{m} \nabla_{\vec \theta} \sum_i^m L\left(  f(\vec{x}^{(i)}; \vec \theta), \vec{y}^{(i)}   \right) \\
		\vec{v} &\leftarrow \alpha \vec{v} - \epsilon \vec{g} \\
		\vec{\theta} &\leftarrow \vec \theta + \vec{v} 
	\end{align}
	
	\item \textbf{Nesterov Momentum}. Gradient computations instead evaluated after current velocity is applied.
	\begin{align}
		\vec{g} &\leftarrow \inv{m} \nabla_{\vec \theta} \sum_i^m L\left(  f(\vec{x}^{(i)}; \vec{\theta} + \alpha \vec{v}), \vec{y}^{(i)} \right) \\
		\vec{v} &\leftarrow \alpha \vec{v} - \epsilon \vec{g} \\
		\vec{\theta} &\leftarrow \vec \theta + \vec{v} 
	\end{align}
	
	\item \textbf{AdaGrad}. Different learning rate for each model parameter. Individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all historical squared values of the gradient. Empirically, can result in premature and excessive decrease in the effective learning rate. 
	\begin{align}
		\vec g &\leftarrow \inv{m} \nabla_{\vec \theta} \sum_i^m L\left(  f(\vec{x}^{(i)}; \vec \theta), \vec{y}^{(i)}   \right) \\
		\vec{r} &\leftarrow \vec{r} + \vec{g} \odot \vec{g} \\
		\vec{\theta} &\leftarrow \vec \theta - \frac{\epsilon}{\delta + \sqrt{\vec{r}}}  \odot\vec g
	\end{align}
	where the gradient accumulation variable $\vec{r}$ is initialized to the zero vector, and the fraction and square root in the last equation is applied element-wise.
	
	\item \textbf{RMSProp}. Modifies AdaGrad by changing the gradient accumulation into an exponentially weighted moving average.
	\begin{align}
		\vec g &\leftarrow \inv{m} \nabla_{\vec \theta} \sum_i^m L\left(  f(\vec{x}^{(i)}; \vec \theta), \vec{y}^{(i)}   \right) \\
		\vec{r} &\leftarrow \rho \vec{r} + (1 - \rho) \vec{g} \odot \vec{g} \\
		\vec{\theta} &\leftarrow \vec \theta - \frac{\epsilon}{\delta + \sqrt{\vec{r}}}  \odot\vec g
	\end{align}
	It is also common to modify RMSProp to use Nesterov momentum. 
	
	\item \textbf{Adam}. So-named to mean ``adaptive moments.'' We now call $\vec{r}$ the 2nd moment (variance) variable, and introduce $\vec{s}$ as the 1st moment (mean) variable, where the moments are for the [true] gradient; the new variables act as estimates of the moments [since we estimate the gradient with a simple average over a minibatch]. Note that these moments are uncentered. 
	\begin{align}
		\vec g &\leftarrow \inv{m} \nabla_{\vec \theta} \sum_i^m L\left(  f(\vec{x}^{(i)}; \vec \theta), \vec{y}^{(i)}   \right) \\
		\vec{s} &\leftarrow \inv{1 - \rho_1^{t-1}} \left[  \rho_1 \vec{s} + (1 - \rho_1) \vec{g} \right] \\
		\vec{r} &\leftarrow  \inv{1 - \rho_2^{t-1}} \left[  \rho_2 \vec{r} + (1 - \rho_2) \vec{g} \odot \vec{g} \right] \\
		\vec{\theta} &\leftarrow \vec \theta - \frac{\epsilon}{\delta + \sqrt{\vec{r}}} \odot \vec{s}
	\end{align}
	where the factors proportional to the $\rho$ values serve to correct for bias in the moment estimators.
\end{itemize}




% ============================================================================
% CONV NET
% ============================================================================
\lecture{Modern Practices}{Convolutional Neural Networks (Ch. 9)}{January 24, 2017}

\p We use a 2-D image $I$ as our input (and therefore require a 2-D kernel $K$). Note that most neural networks do not technically implement convolution\footnote{Technically the convolution output is defined as
	\begin{align}
	S(i, j) &= (I * K)(i, j) = \sum_m \sum_n I(m, n)K(i -m, j -n) \tlab{9.4} \\
	&= (K * I)(i, j) =  \sum_m \sum_n I(i - m, j - n)K(m, n) \tlab{9.5} 
	\end{align}
	where ~\ref{9.5} can be asserted due to commutativity of convolution. }, but instead implement a related function called the \textit{cross-correlation}, defined as 
\graybox{
	S(i, j) = (I * K)(i, j) = \sum_m \sum_n I(i + m, j + n)K(m, n) \tlab{9.6}
}
Convolution leverages the following three important ideas:
\begin{compactitem}
	\item \textbf{Sparse interactions[/connectivity/weights]}. Individual input units only interact/connect with a subset of the output units. Accomplished by making the kernel smaller than the input. It's important to recognize that the receptive field of the units in the deeper layers of a convolutional network is \textit{larger} than the receptive field of the units in the shallow layers, as seen below.
	\myfig{SparseConv.PNG}
	
	\item \textbf{Parameter sharing}.
	
	\item \textbf{Equivariance} (to translation). Changes in inputs [to a function] cause output to change in the same way. Specifically, $f$ is equivariant to $g$ if $f(g(x)) = g(f(x))$. For convolution, $g$ would be some function that translates the input.
\end{compactitem}

\myspace
\p \blue{Pooling}. Helps make the representation approximately \textbf{invariant} to small translations of the input. The use of pooling can be viewed as adding an infinitely strong prior\footnote{Where the distribution of this prior is over all possible functions learnable by the model.} that the function the layer learns must be invariant to small translations.

\myspace
\p \blue{Additional common tricks}\footnote{Collected on my own. In other words, not from the deep learning book, but rather a bunch of disconnected resources over time.}. 
\begin{compactitem}
	\item \green{Local Response Normalization (LRN)}\footnote{From section 3.3 of Krizhevsky et al. (2012). AlexNet paper.}. Purpose is to aid generalization ability. Let $a^i_{x, y}$ denote the activity of a neuron computed by applying kernel $i$ at position $(x, y)$ and then applying the ReLU nonlinearity. The response-normalized activity $b^i_{x,y}$ is given by the expression
	\begin{align}
		b^i_{x,y} &= \dfrac{a^i_{x,y}}{
			\left(k + \alpha \sum_j \left(a^j_{x,y} \right)^2	 \right)^{\beta}
		}
	\end{align}
	where $j$ runs from $\left[i - n/2\right]_+$ to $\min(N-1, ~ i + n/2)$, and $N$ is the total number of kernels in the given layer\footnote{In other words, the summation is over the adjacent kernel maps, with [total] window size $n$ (manually chosen). The min/max just says $n/2$ to the left (right) unless that would be past the leftmost (rightmost) kernel map.}. Authors used $k = 2$, $n = 5$, $\alpha = 10^{-4}$, $\beta = 0.75$. 
	
	\item \green{Batch Normalization}\footnote{From ``Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'' by Ioffe et al.}. BN Allows us to use much higher learning rates and be less careful about initialization. Algorithm defined in image below, where each element of the batch, $x_i \equiv x_i^{(k)}$ (where we drop the $k$ for notational simplicity), represents the $k$th activation output from the previous layer [for the $i$th sample in the batch] and about to be fed as input to the current layer.
	\myfig[0.5\textwidth]{figs/batch_norm.png}
	
	 Note that one can model each layer's activations as arising from some distribution. When we feed data to a network, we model the data as coming from some data-generating distribution. Similarly, we can model the activations that occur when feeding the data through as coming from some activation-layer-generating distribution. The problem with this model is that the process of updating our weights during training changes the distribution of activations for each layer, which can make the learning task more difficult. Batch normalization's goal is to reduce this \textit{internal covariate shift}, and is motivated by the practice of normalizing our data to have zero mean and unit variance.
	
\end{compactitem}

\myspace
\p \blue{Intuition of some math}. 
\begin{compactitem}
	\QA{How to intuitively understand the commutativity of convolution?}{You must first realize that, \textit{independent of which formula we're thinking of}, as one index into either the image or kernel increases (decreases), the other decreases (increases); they increment in opposite directions. The difference between the two formulas (9.4 and 9.5 in textbook), is just that we start at different ends of the summations (when you actually substitute in the numbers). Note that this doesn't require any symmetry on the boundaries of the summation about zero; it truly is a property of the convolution.}
	
	\QA{What do the authors mean by ``we have flipped the kernel''?}{Not much, and it's poor wording. They didn't \textit{do} anything, that is just part of the definition of the convolution. They literally just mean that the convolution has the property that as one index increases, the other decreases (see previous answer). The cross-correlation, however, has the property that as one index increases, the other increases, too.}
\end{compactitem}
























\newcommand\var[3][]{\bm{#2}^{(#3)}_{#1}}
\lecture{Modern Practices}{Sequence Modeling (RNNs) (Ch. 10)}{January 15}
% --------------------------------------------
\subsub{Review: The Basics of RNNs}
% --------------------------------------------
\p \blue{Notation/Architecture Used}. 
\begin{compactitem}
	\item \green{U}: input $\rightarrow$ hidden.
	\item \green{W}: hidden $\rightarrow$ hidden.
	\item \green{V}: hidden $\rightarrow$ output.
	\item \green{Activations}: tanh [hidden] and softmax [after output].
	\item \green{Misc. Details}: \begin{scriptsize}
		$\bm x^{(t)}$ is a \textit{vector} of inputs fed at time $t$. Recall that RNNs can be unfolded for any desired number of steps $\tau$. For example, if $\tau = 3$, the general functional representation output of an RNN is $\var{s}{3} = f(\var{s}{2}; ~ \bm \theta )=f(f(\var{s}{1}; \bm \theta); ~ \bm \theta)$. Typical RNNs read information out of the state $\bm h$ to make predictions. 
	\end{scriptsize}
\end{compactitem}

\myspace
\marginnote{Shape of $\var{x}{t}$ fixed, e.g. vocab length.}[-1em]
\marginnote{Black square on recurrent connection $\equiv$ interaction w/delay of a single time step.}[4em]
\myfig[0.4\textwidth]{RNN_Unrolled.PNG}


\myspace
\p \blue{Forward Propagation \& Loss}. Specify initial state $\var{h}{0}$. Then, for each time step from $t=1$ to $t=\tau$, feed input sequence $\var{x}{t}$ and compute the output sequence $\var{o}{t}$. To determine the loss at each time-step, $L^{(t)}$, we compare $\mathrm{softmax}(\var{o}{t})$ with (one-hot) $\var{y}{t}$. 
\begin{alignat}{2}
	\var{h}{t} &= \tanh(\var{a}{t}) \qquad\text{where}\qquad &&\var{a}{t} = b + W\var{h}{t-1} + U\var{x}{t} \tlab{10.9/8}\\
	\var{\hat y}{t} &= \textrm{softmax}(\var{o}{t}) \qquad\text{where}\qquad &&\var{o}{t} = c + V \var{h}{t} \tlab{10.11/10}
\end{alignat}
Note that this is an example of an RNN that maps input seqs to output seqs of the same length\footnote{Where ``same length'' is related to the number of timesteps (i.e. $\tau$ input steps means $\tau$ output steps), not anything about the actual shapes/sizes of each individual input/output.}. We can then compute, e.g., the log-likelihood loss $L = \sum_t L^{(t)}$ over all time steps as:
\marginnote{Convince yourself this is identical to cross-entropy.}[2em]
\begin{align}
L =- \sum_t \log\left( p_{model}\left[ y^{(t)}\mid \{\var{x}{1}, \ldots, \var{x}{t}\} \right]\right) \tlab{10.12/13/14}
\end{align}
where $y^{(t)}$ is the \textbf{ground-truth} (one-hot vector) at time $t$, whose probability of occurring is given by the corresponding element of $\bm{\hat y^{(t)}}$

\myspace
\Needspace{15\baselineskip}
\p \blue{Back-Propagation Through Time}. 
\begin{enumerate}
	\item \textbf{Internal-Node Gradients}. In what follows, when considering what is included in the chain rule(s) for gradients with respect to a node $\bm{N}$, just need to consider paths from it [through its \purple{descendents}] to loss node(s).
	\begin{compactitem}
	
		\item \green{Output nodes}. For any given time $t$, the node $\var{o}{t}$ has only one direct descendant, the loss node $L^{(t)}$. Since no other loss nodes can be reached from $\var{o}{t}$, it is the only one we need consider in the gradient. \marginnote{Ground-truth $y^{(t)}$ here is a \textbf{scalar}, interpreted as the index of the correct label of output vector.}[3em]\marginnote{$
			\bm 1_{i, y^{(t)}} = \begin{cases} 1 & y^{(t)} = i \\ 0 & \mathrm{otherwise} \end{cases}
			$}[20em] 
		\begin{align}
		\begin{split}
		\left( \nabla_{ \var{o}{t} } L \right)_i 
		&=  \pderiv{L}{\var[i]{o}{t}} \\
		&= \pderiv{L}{L^{(t)}} \cdot \pderiv{L^{(t)}}{\var[i]{o}{t}} \\
		&= (1) \cdot \pderiv{L^{(t)}}{\var[i]{o}{t}} \\
		&= \pderiv{}{\var[i]{o}{t}} \left\{
			 - \log\left( \var[y^{(t)}]{\hat y}{t}  \right)\right\} \\
		&= - \pderiv{}{\var[i]{o}{t}} \left\{\log\left( 
			\frac{e^{\var[y^{(t)}]{o}{t}}}{\sum_j e^{\var[j]{o}{t}}} \right)\right\} \\
		&= - \pderiv{}{\var[i]{o}{t}} \left\{ 
			\var[y^{(t)}]{o}{t} - 
			\log\left(  \sum_j e^{\var[j]{o}{t}} \right) \right\} \\
		&= - \left\{ \bm{1}_{i, y^{(t)}} 
				- \pderiv{}{\var[i]{o}{t}}\log\left(  \sum_j e^{\var[j]{o}{t}} \right) \right\} 
		\end{split}
		\end{align}
		\begin{align}
		\begin{split}
		&= - \left\{ \bm{1}_{i, y^{(t)}} 
			- \frac{1}{ \sum_j e^{\var[j]{o}{t}} }  \pderiv{ \sum_j e^{\var[j]{o}{t}} }{\var[i]{o}{t}} \right\}\\
		&= - \left\{ \bm{1}_{i, y^{(t)}} 
		- \frac{e^{\var[i]{o}{t}}}{ \sum_j e^{\var[j]{o}{t}} }   \right\} \\
		&= - \left\{ \bm{1}_{i, y^{(t)}} 
		- \var[i]{\hat y}{t}   \right\} \\
		&=  \var[i]{\hat y}{t}  - \bm{1}_{i, y^{(t)}} 
		\end{split}\tlab{10.18}
		\end{align}
		which leaves all entries of $\var{o}{t}$ unchanged \textit{except} for the entry corresponding to the true label, which will become negative in the gradient. All this means is, since we want to increase the probability of this entry, driving this value up will \textit{decrease} the loss (hence negative) and driving any other entries up will \textit{increase} the loss proportional to its current estimated probability (driving up an [incorrect] entry that is already high is ``worse'' than driving up a small [incorrect entry]).
		
		\item \green{Hidden nodes}. First, consider the simplest hidden node to take the gradient of, the last one, $\var{h}{\tau}$ (simplest because only one descendant [path] reaching any loss node(s)).
		
		\begin{align}
		\begin{split}
		\left( \nabla_{ \var{h}{\tau} } L \right)_i 
		&=  \pderiv{L}{L^{(\tau)}} 
			\sum_{k = 1}^{n_{out}} \pderiv{L^{(\tau)}}{\var[k]{o}{\tau}} \pderiv{\var[k]{o}{\tau}}{\var[i]{h}{\tau}} \\
		&=  \sum_{k = 1}^{n_{out}} \left( \nabla_{\var{o}{\tau}} L \right)_k
			\pderiv{ \var[k]{o}{\tau} }{ \var[i]{h}{\tau} } \\
		&= \sum_{k = 1}^{n_{out}} \left( \nabla_{\var{o}{\tau}} L \right)_k
			\pderiv{}{\var[i]{h}{\tau}} \left\{ c_k + \sum_{j = 1}^{n_{hid}} V_{kj} \var[j]{h}{\tau} \right\}\\
		&= \sum_{k = 1}^{n_{out}} \left( \nabla_{\var{o}{\tau}} L \right)_k V_{ki}  \\
		&= \sum_{k = 1}^{n_{out}} (V^T)_{ik} \left( \nabla_{\var{o}{\tau}} L \right)_k \\
		&= \left( V^T  \nabla_{ \var{o}{t} } L  \right)_i
		\end{split} \tlab{10.19}
		\end{align} 
		Before proceeding, \red{\underline{notice the following useful pattern}}: If two nodes $a$ and $b$, each containing $n_a$ and $n_b$ neurons, are fully connected by parameter matrix $W_{n_b \times n_a}$ and directed like $a \rightarrow b \rightarrow L$, then\footnote{More generally, $$\nabla_a L = (\pderiv{b}{a})^T \nabla_b L$$which is a good example of how vector derivatives map into a matrix. For example, let $\bm a \in \mathbb{R}^{n_a}$ and $\bm{b} \in \mathbb{R}^{n_b}$. Then $$\pderiv{\bm b}{\bm a} \in \mathbb{R}^{n_b \times n_a}$$} $\nabla_a L = W^T \nabla_b L$. Using this result, we can then iterate and take gradients back in time from $t=\tau - 1$ to $t=1$ as follows:\marginnote{\[ \deriv{}{x}\tanh(x) = 1 - \tanh^2(x)\]\[ \left(\mathrm{diag}(\bm{a})\right)_{ii} \triangleq a_i \]}[2em]
		\begin{align}
			\nabla_{ \var{h}{t} } L 
			&= \left( \pderiv{ \var{h}{t+1} }{ \var{h}{t} } \right)^T \left( \nabla_{\var{h}{t+1}} L \right) 
				+ \left( \pderiv{ \var{o}{t} }{ \var{h}{t} } \right)^T \left( \nabla_{\var{o}{t}} L \right)  \tlab{10.20} \\
		\begin{split}
			&= W^T \left( \nabla_{\var{h}{t+1}} L \right) \mathrm{diag}(1 - \tanh^2(\var{a}{t+1}))
			+ V^T \left( \nabla_{\var{o}{t}} L \right) \\
			&= W^T \left( \nabla_{\var{h}{t+1}} L \right) \mathrm{diag}\left(1 - (\var{h}{t+1})^2 \right)
			+ V^T \left( \nabla_{\var{o}{t}} L \right) 
		\end{split} \tlab{10.21}
		\end{align}
	
	\end{compactitem}
	
	\item \textbf{Parameter Gradients}. Now we can compute the gradients for the parameter matrices/vectors, where it is crucial to remember that a given parameter matrix (e.g. $U$) is shared across \textit{all} time steps $t$. We can treat tensor derivatives in the same form as previously done with vectors after a quick abstraction: For any tensor $\mathbf{X}$ of arbitrary rank (e.g. if rank-4 then index like $\mathbf{X}_{ijk\ell}$), use single variable (e.g. $i$) to represent the complete tuple of indices\footnote{More details on tensor derivatives: Consider the chain defined by $\mathbf{Y} = g(\mathbf{X})$, and $z = f(\mathbf{Y})$, where $z$ is some vector. Then $$\nabla_{\mathbf{X}} z = \sum_j (\nabla_{\mathbf{X}} Y_j)\pderiv{z}{Y_j} $$}.
	
	\begin{compactitem}
		\item \green{Bias parameters [vectors]}. These are nothing new, since just vectors.
		\begin{align}
		\begin{split}
			\left( \nabla_{ \bm c } L \right) 
			&= \sum_t \left(  \pderiv{ \var{o}{t} }{ \var{c}{t} }  \right)^T \left( \nabla_{ \var{o}{t} } L \right) \\
			&= \sum_t \left( \nabla_{ \var{o}{t} } L \right) \\
		\end{split}\tlab{10.22}
		\\
		\begin{split}
			\left( \nabla_{ \bm c } L \right) 
			&= \sum_t \left(   \pderiv{ \var{h}{t} }{ \var{b}{t} }  \right)^T \left( \nabla_{ \var{h}{t} } L \right) \\
			&= \sum_t \mathrm{diag}\left(1 - (\var{h}{t})^2 \right) \left( \nabla_{ \var{h}{t} } L \right) \\
		\end{split}\tlab{10.23}
		\end{align}
		
		\item \green{V ($n_{out} \times n_{hid}$)}.
		\begin{subequations}
		\begin{align}
			\matgrad{L}{V} &=  \sum_t^{\tau} \matgrad{L^{(t)}}{V} \\
			&= \sum_t^{\tau} \matgrad{L^{(t)}(\vec[1]{o}^{(t)}, \ldots, \vec[n_{out}]{o}^{(t)})}{V} \\
			&= \sum_t^{\tau}\sum_i^{n_{out}} \left( \nabla_{ \var{o}{t} } L \right)_i \matgrad{\var[i]{o}{t}}{V} \\
			&= \sum_t^{\tau}\sum_i^{n_{out}} \left( \nabla_{ \var{o}{t} } L \right)_i \matgrad{}{V}\left\{ 
				c_i + \sum_{j = 1}^{n_{hid}} V_{ij} \var[j]{h}{t}\right\} \label{c} \\
			&= \sum_t^{\tau}\sum_i^{n_{out}} \left( \nabla_{ \var{o}{t} } L \right)_i 
				\begin{bmatrix}
					0 		& 0 		& \ldots & 0 \\
					\vdots	& \vdots	& \ldots & \vdots \\
					\vec[1]{h}^{(t)} & \vec[2]{h}^{(t)}  & \ldots & \vec[n_{hid}]{h}^{(t)} \\
					\vdots	& \vdots	& \ldots & \vdots \\
					0 		& 0 		& \ldots & 0 \\
				\end{bmatrix} \label{tensorgrad} \\
			&= \sum_t^{\tau} \left( \nabla_{ \var{o}{t} } L \right) (\var{h}{t})^T \label{d}
		\end{align}
		\end{subequations}
		\marginnote{NOTE: In equation ~\ref{tensorgrad}, all rows are 0 except the $i$th row, which is $(\vec{h}^{(t)})^T$. (See footnote for more)}[-9em]where if ~\ref{tensorgrad} confuses you, see the footnote\footnote{
			The general lesson learned here is that, for some matrix $\matr{W} \in \R^{a \times b}$ and vector $\vec{x} \in \R^b$, 
			\begin{align}
			\sum_i \matgrad{}{W}\left[ (\matr{W}{x})_{i} \right] = 
			\begin{bmatrix}
			\vec{x}^{T} \\
			\vec{x}^{T} \\
			\vdots \\
			\vec{x}^{T} 
			\end{bmatrix}
			\end{align}
			where, of course, the output has the same dimensions as $\matr{W}$.}.
		
		
		\item \green{W ($n_{hid} \times n_{hid}$)}. This one is a bit odd, since $\matr{W}$ is, in a sense, even more ``shared'' across time steps than $\matr{V}$\footnote{Specifically, $\vec{h}^{(t)}$ is both
			\begin{enumerate}
			\item An explicit function of the parameter matrix $\matr{W}^{(t)}$ directly feeding into it.
			\item An implicit function of all other $\matr{W}^{t=i}$ that came before.
			\end{enumerate}
		This is different than before, where we had $\vec{o}^{(t)}$ not implicitly depending on earlier $\matr{V}^{(t=i)}$. In other words, $\vec{h}^{(t)}$ is a \underline{descendant} of all earlier (and current) $\matr{W}$. 
		}. The authors here define/choose, when evaluating $\matgrad{h_i^{(t)}}{W}$ to only concern themselves with $\matr{W} := \matr{W}^{(t)}$, i.e. the direct connections to $\vec{h}$ at time $t$. 
			\begin{subequations}
				\begin{align}
				\matgrad{L}{W} &=  \sum_t^{\tau} \matgrad{L^{(t)}}{W} \\
				&= \sum_t^{\tau}\sum_i^{n_{hid}} \left( \nabla_{ \var{h}{t} } L \right)_i \matgrad{\var[i]{h}{t}}{W^{(t)}} \tlab{10.25} \\
				&= \sum_t^{\tau}\sum_i^{n_{hid}} \left( \nabla_{ \var{h}{t} } L \right)_i  
					\left( 
					\diag{1 - (\vec{h}^{(t)})^2}
					\begin{bmatrix}
					0 		& 0 		& \ldots & 0 \\
					\vdots	& \vdots	& \ldots & \vdots \\
					\vec[1]{h}^{(t-1)} & \vec[2]{h}^{(t-1)}  & \ldots & \vec[n_{hid}]{h}^{(t-1)} \\
					\vdots	& \vdots	& \ldots & \vdots \\
					0 		& 0 		& \ldots & 0 \\
					\end{bmatrix}
					\right) \\
				&= \sum_t^{\tau}\diag{1 - (\vec{h}^{(t)})^2} \left( \nabla_{ \var{h}{t} } L \right) (\vec{h}^{(t - 1)})^T \tlab{10.26}
				\end{align}
			\end{subequations}
		
		\item \green{U ($n_{hid} \times n_{in}$)}. Very similar to the previous calculation.
		\begin{subequations}
			\begin{align}
			\matgrad{L}{U} &=  \sum_t^{\tau} \matgrad{L^{(t)}}{U} \\
			&= \sum_t^{\tau}\sum_i^{n_{hid}} \left( \nabla_{ \var{h}{t} } L \right)_i \matgrad{\var[i]{h}{t}}{U^{(t)}} \tlab{10.27} \\
			&= \sum_t^{\tau}\diag{1 - (\vec{h}^{(t)})^2} \left( \nabla_{ \var{h}{t} } L \right) (\vec{x}^{(t)})^T \tlab{10.28}
			\end{align}
		\end{subequations}
	\end{compactitem}

\end{enumerate}



\myspace
% --------------------------------------------
\subsub{RNNs as Directed Graphical Models}
% --------------------------------------------
\myspace

\p The advantage of RNNs is their efficient parameterization of the joint distribution over $\vec{y}^{(i)}$ via parameter sharing. This introduces a built-in assumption that we can model the effect of $y^{(i)}$ in the distant past on the current $y^{(t)}$ \textit{via its effect on $\vec{h}$}. We are also assuming that the conditional probability distribution over the variables at $t + 1$ given the variables at time $t$ is \textbf{stationary}. Next, we want to know how to draw \textit{samples} from such a model. Specifically, how to sample from the conditional distribution ($y^{(t)}$ given $y^{(t - 1)}$) at each time step.


Say we want to model a sequence of scalar random variables $\mathbb{Y} \triangleq \{y^{(1)}, \ldots, y^{(\tau)} \}$ for some sequence length $\tau$. Without making independence assumptions just yet, we can parameterize the joint distribution $P(\mathbb{Y})$ with basic definitions of probability:
\begin{align}
P(\mathbb{Y}) &\triangleq P(\dotseq{y}{\tau}) = \prod_{t = 1}^{\tau} P(y^{(t)} \mid \rdotseq{y}{t - 1})
\end{align}
where I've drawn an example of the \textit{complete graph} for $\tau = 4$ below\marginnote{The complete graph can represent the direct dependencies between any pairs of y values.}[3em].

\begin{center}
	\begin{tikzpicture}[node distance=2cm,->,>=latex,auto,every edge/.append style={thick}]
	\node[state] (1) {$y^{(1)}$};
	\node[state] (2) [right of=1] {$y^{(2)}$};  
	\node[state] (3) [right of=2] {$y^{(3)}$};  
	\node[state] (4) [right of=3] {$y^{(4)}$};  
	
	\path(1) 
	edge[->] node{} (2)
	edge[bend left=70] node{} (3)
	edge[bend left=70] node{} (4);
	
	\path(2) 
	edge[->] node{} (3)
	edge[bend left=40] node{} (4);
	
	\path(3) edge[->] node{} (4);
	
	\end{tikzpicture}
\end{center}

If each value $y$ could take on the same fixed set of $k$ values, we would need to learn $k^4$ parameters to represent the joint distribution $P(\mathbb{Y})$. This clearly inefficient, since the number of parameters needed scales like $\mathcal{O}(k^\tau)$. If we relax the restriction that each $y^{(i)}$ must depend \textit{directly} on all past $y^{(j)}$, we can considerably reduce the number of parameters needed to compute the probability of some particular sequence. \\

\p We could include latent variables $h$ at each timestep that capture the dependencies, reminiscent of a classic RNN:

\begin{center}
	\begin{tikzpicture}[node distance=2cm,->,>=latex,auto,every edge/.append style={thick}]
	\node[state] (1) {$y^{(1)}$};
	\node[state] (2) [right of=1] {$y^{(2)}$};  
	\node[state] (3) [right of=2] {$y^{(3)}$};  
	\node[state] (4) [right of=3] {$y^{(4)}$};  
	
	\node[state] (h1) [above of=1]{$h^{(1)}$};
	\node[state] (h2) [above of=2]{$h^{(2)}$};
	\node[state] (h3) [above of=3]{$h^{(3)}$};
	
	\path(1) 	edge[->] node{} (h1);
	\path(2) 	edge[->] node{} (h2);
	\path(3) 	edge[->] node{} (h3);
	
	\path(h1) 	edge[->] node{} (h2)
	edge[->] node{} (2);
	
	\path(h2) 	edge[->] node{} (h3)
	edge[->] node{} (3);
	
	\path(h3) 	edge[->] node{} (4);
	\end{tikzpicture}
\end{center}
Since in the RNN case all factors $P(h^{(t)} \mid h^{(t-1)})$ are deterministic, we don't need any additional parameters to compute this probability\footnote{Don't forget that, in a neural net, a variable $y^{(t)}$ is represented by a \textit{layer}, which itself is composed of $k$ nodes, each associated with one of the $k$ unique values that $y^{(t)}$ could be.}, other than the single $m^2$ parameters needed to convert any $h^{(t)}$ to the next $h^{(t+1)}$ (which is shared across all transitions). Now, the number of parameters needed as a function of sequence length is constant, and as a function of $k$ is just $\mathcal{O}(k)$. \\

Finally, to view the RNN as a graphical model, we must describe how to sample from it, namely how to sample a sequence $\vec{y}$ from $P(\mathbb{Y})$, if parameterized by our graphical model above. In the general case where we don't know the value of $\tau$ for our sequence $\vec{y}$, one approach is to have a EOS symbol that, if found during sampling, means we should stop there. Also, in the typical case where we actually want to model $P(y \mid x)$ for input sequence $x$, we can reinterpret the parameters $\vec{\theta}$ of our graphical model as a function of $\vec{x}$ the input sequence. In other words, the graphical model interpretation becomes a function of $\vec{x}$, where $\vec{x}$ determines the exact values of the probabilities the graphical model takes on -- an ``instance'' of the graphical model.






\myspace
\p \blue{Bidirectional RNNs}. In many applications, it is desirable to output a prediction of $\vec{y}^{(t)}$ that may depend on \textit{the whole sequence}. For example, in speech recognition, the interpretation of words/sentences can also depend on what is \textit{about} to be said. Below is a typical bidirectional RNN, where the inputs $\vec{x}^(t)$ are fed both to a ``forward'' RNN ($\vec{h}$) and a ``backward'' RNN ($\vec{g}$). \marginnote{Notice how the output units $\vec{o}^{(t)}$ have the nice property of depending on both the past and future while being most sensitive to input values around time $t$.}[5em]

\myfig[0.3\textwidth]{BiRNN.PNG}

\myspace\myspace\Needspace{15\baselineskip}
\p \blue{Encoder-Decoder Seq2Seq Architectures (10.4)} Here we discuss how an RNN can be trained to map an input sequence to output sequence which is not necessarily the same length. (Not really much of a discussion...figure below says everything.)

\myfig{Seq2Seq.PNG}

\myspace\myspace\Needspace{15\baselineskip}
% --------------------------------------------
\subsub{Challenge of Long-Term Deps. (10.7)}
% --------------------------------------------

\myspace
\p Gradients propagated over many stages either vanish (usually) or explode. We saw how this could occur when we took parameter gradients earlier, and for weight matrices $\matr{W}$ further along from the loss node, the expression for $\matgrad{L}{W}$ contained multiplicative Jacobian factors. Consider the (linear activation) repeated function composition of an RNN's hidden state in \ref{10.36}. We can rewrite it as a power method (\ref{10.37}), and if $\matr{W}$ admits an eigendecomposition (remember $\matr{W}$ is necessarily square here), we can further simplify as seen in ~\ref{10.38}.\marginnote{\red{Q}: Explain interp. of mult. $\vec{h}$ by $\matr{Q}$ as opposed to the usual $\matr{Q}^T$ explained in the linear algebra review.}[2em]
\begin{align}
	\vec{h}^{(t)} &= \matr{W}^T \vec{h}^{(t-1)} \tlab{10.36} \\
	&= (\matr{W}^t)^T \vec{h}^{(0)} \tlab{10.37} \\
	&= \matr{Q}^T \matr{\Lambda}^t \matr{Q} \vec{h}^{(0)} \tlab{10.38}
\end{align}
\begin{quote}
	\textbf{Any component of $\vec{h}^{(0)}$ that isn't aligned with the largest eigenvector will eventually be discarded.}\footnote{Make sure to think about this from the right perspective. The largest value of $t = \tau$ in the RNNs we've seen would correspond with either (1) the largest output sequence or (2) the largest input sequence (if fixed-vector output). After we extract the output from a given forward pass, we reset the clock and either back-propagate errors (if training) or get ready to feed another sequence.}
\end{quote}
If, however, we have a non-recurrent network such that the state elements are repeatedly multiplied by different $w^{(t)}$ at each time step, the situation is different. Suppose the different $w^{(t)}$ are i.i.d. with mean 0 and variance $v$. The variance of the product is easily seen to be $\mathcal{O}(v^n)$\footnote{Quick sketch of (my) proof: \begin{align}
	 \Var{w^{(i)}} 
		 &= v 
			 = \E{(w^{(i)})^2} - \cancel{\E{w^{(i)}}^2} \\ 
	\Var{ \prod_t^n  w^{(t)}  } 
		&= \E{
			\left(
			\prod_t^n w^{(t)}
			\right)^2} 
		= \prod_t^n \E{  (    w^{ (t) }   )^2  } = v^n
	\end{align}}. \textit{To obtain some desired variance $v^*$ we may choose the individual weights with variance} $v = \sqrt[n]{v^*}$. 

\myspace\myspace\Needspace{15\baselineskip}
% --------------------------------------------
\subsub{LSTMs and Other Gated RNNs (10.10)}
% --------------------------------------------

\p While leaky units have connection weights that are either manually chosen constants or are trainable parameters, gated RNNs generalize this to connection weights that may change \textit{at each time step}. Furthermore, gated RNNs can learn to both accumulate and \textit{forget}, while leaky units are designed for just accumulation\footnote{Q: Isn't choosing to update with higher relative weight on the present the same as forgetting? A: Sort of. It's like ``soft forgetting'' and will inevitably erase more/less than desired (smeary). In this context, ``forget'' means to set the weight of a specific past cell to zero.}

\myspace
\p \blue{LSTM (10.10.1)}. The idea is we want self-loops to produce paths where the gradient can flow for long durations. The self-loop weights are \textbf{gated}, meaning they are controlled by another hidden unit, interpreted as being conditioned on \textit{context}. Listed below are the main components of the LSTM architecture.
\begin{compactitem}
	\item \textbf{Forget gate $f_i^{(t)} 
		= \sigma\left(b_i^f + \sum_j U_{i,j}^f x_j^{(t)} + \sum_j W_{i,j}^f h_j^{(t-1)}    \right)
		$}. \marginnote{The subscript, $i$, identifies the cell. The superscript, $t$, denotes the time.}
	
	\item \textbf{Internal state} $s_i^{(t)}
		= f_i^{(t)} \odot s_i^{(t-1)} + g_i^{(t)} \odot \sigma\left(b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} h_j^{(t-1)}  \right)
		$. 
	
	\item \textbf{External input gate} $g_i^{(t)}
	= \sigma\left(b_i^g + \sum_j U_{i,j}^g x_j^{(t)} + \sum_j W_{i,j}^g h_j^{(t-1)}    \right)
	$.
	
	\item \textbf{Output gate} $q_i^{(t)}
	= \sigma\left(b_i^o + \sum_j U_{i,j}^o x_j^{(t)} + \sum_j W_{i,j}^o h_j^{(t-1)}    \right)
	$. 
\end{compactitem}
The final hidden state can then be computed via
\begin{align}
	h_i^{(t)} &= \tanh(s_i^{(t)}) \odot q_i^{(t)}
\end{align}










\lecture{Modern Practices}{Applications (Ch. 12)}{February 14}

\myspace

\subsub{Natural Language Processing (12.4)}

\marginnote{Begins on pg. 448}

\myspace
\p \blue{n-grams}. A \textbf{language model} defines a probability distribution over sequences of [discrete] tokens (words/characters/etc). Early models were based on the \textit{n-gram}: a [fixed-length] sequence of $n$ tokens. Such models define the conditional distribution for the $n$th token, given the $(n - 1)$ previous tokens: $$P(x_t \mid x_{t - (n - 1)}, \ldots, x_{t - 1})$$where $x_i$ denotes the token at step/index/position $i$ in the sequence. \\

\p To define distributions over longer sequences, we can just use Bayes rule over the shorter distributions, as usual. For example, say we want to find the [joint] distribution for some $\tau$-gram ($\tau > n$), and we have access to an $n$-gram model and a [perhaps different] model for the initial sequence $P(x_1, \ldots, x_{n - 1})$. We compute the $\tau$ distribution simply as follows:
\begin{align}
	P(x_1, \ldots, x_\tau) &= P(x_1, \ldots, x_{n - 1}) \prod_{t = n}^{\tau} 
		P(x_t \mid x_{t - 1}, \ldots, x_{t - (n - 2)}, x_{t - (n - 1)}) \tlab{12.5}
\end{align}
where it's important to see that each factor in the product is a distribution over a length-$n$ sequence. Since we need that initial factor, it is common to train both an $n$-gram model and an $n-1$-gram model simultaneously. \\

\p Let's do a specific example for a trigram (n = 3). 
\begin{compactitem}
	\item \textbf{Assumptions [for this trigram model example]}:
	\begin{compactitem}
		\item For any $n \ge 3$, $P(x_n \mid x_1, \ldots, x_{n - 1}) = P(x_n \mid x_{n - 2}, x_{n - 1})$. 
		
		\item When we get to computing the full joint distribution over some sequence of arbitrary length, we assume we have access to both $P_3$ and $P_2$, the joint distributions over all subsequences of length 3 and 2, respectively. 
	\end{compactitem}
	
	\item \textbf{Example sequence}: We want to know how to use a trigram model on the sequence ['THE', 'DOG', 'RAN', 'AWAY']. 
	
	\item \textbf{Derivation}: We can use the built-in model assumption to derive the following formula.
	\newcommand\fml[1]{\text{\footnotesize#1}}
	\begin{align}
	\begin{split}
		P(\fml{THE DOG RAN AWAY})
			&= P_3(\fml{AWAY} \mid \fml{THE DOG RAN})  ~ P_3(\fml{THE DOG RAN})\\
			&=  P_3(\fml{AWAY} \mid \fml{DOG RAN})  ~ P_3(\fml{THE DOG RAN})\\
			&= \frac{ P_3(\fml{DOG RAN AWAY}) }{ P_2(\fml{DOG RAN}) }  ~ P_3(\fml{THE DOG RAN})\\
			&= P_3(\fml{THE DOG RAN}) P_3(\fml{DOG RAN AWAY}) / P_2(\fml{DOG RAN})
	\end{split}\tlab{12.7}
	\end{align}
\end{compactitem}
\myspace
\p \blue{Limitations of n-gram}. The last example illustrates some potential problems one may encounter that arise [if using MLE] when the full joint we seek is nonzero, but (a) some $P_n$ factor is zero, or (b) $P_{n - 1}$ is zero\marginnote{Recall that, in MLE, the $P_n$ and $P_{n - 1}$ are usually approximated via counting occurrences in the training set}[-2em]. Some methods of dealing with this are as follows.
\begin{compactitem}
	\item \textbf{Smoothing}: shifting probability mass from the observed tuples to unobserved ones that are similar.
	\item \textbf{Back-off methods}: look up the lower-order (lower values of $n$) $n$-grams if the frequency of the context $x_{t - 1}, \ldots, x_{t - (n - 1)}$ is too small to use the higher-order model.
\end{compactitem}
In addition, $n$-gram models are vulnerable to the curse of dimensionality, since most $n$-grams won't occur in the training set\footnote{For a given vocabulary, which usually has much more than $n$ possible words, consider how many possible sequences of length $n$.}, even for modest $n$.


% =========================================
\subsub{Neural Language Models (12.4.2)}
% =========================================

\p Designed to overcome curse of dimensionality by using a distributed representation of words. Recognize that any model trained on sentences of length $n$ and then told to generalize to new sentences [also of length $n$] must deal with a space\footnote{Ok I tried re-wording that from the book's confusing wording but that was also a bit confusing. Let me break it down. Say you train on a thousand sentences each of length 5. For a given vocabulary of size VOCAB\_SIZE, the number of possible sequences of length 5 is $(\text{VOCAB\_SIZE})^5$, which can be quite a lot more than a thousand (not to mention the possibility of duplicate training examples). To the naive model, all points in this high-dimensional space are basically the same. A neural language model, however, tries to arrange the space of possibilities in a meaningful way, so that an unforeseen sample at test time can be said "similar" as some previously seen training example. It does this by \textit{embedding} words/sentences in a lower-dimensional feature space.} of possible sentences that is exponential in $n$. Such word representations (i.e. viewing words as existing in some high-dimensional space) are often called \textbf{word embeddings}. The idea is to map the words (or sentences) from the raw high-dimensional [vocab sized] space to a smaller feature space, where similar words are closer to one another. Using distributed representations may also be used with graphical models (think Bayes' nets) in the form multiple \textit{latent variables}. 










% ==================================================================================
% DEEP LEARNING RESEARCH
% ==================================================================================
\mysection{Deep Learning Research}\label{Deep Learning Research}

\lecture{Deep Learning Research}{Linear Factor Models (Ch. 13)}{January 12}

\myspace
\p \blue{Overview}. Much research is in building a \textit{probabilistic model}\footnote{Whereas, before, we've been building \textit{functions} of the input (deterministic).} of the input, $p_{model}(x)$. Why? Because then we can perform \textit{inference} to predict stuff about our environment given any of the other variables. We call the other variables \textbf{latent variables}, $h$, with
\begin{align}
	p_{model}(x) &= \sum_h \Pr(h)\Pr(x \mid h) = \mathbb{E}_h\left[ p_{model}(x \mid h)\right]
\end{align}
So what? Well, the latent variables provide another means of \textit{data representation}, which can be useful. \textbf{Linear factor models} (LFM) are some of the simplest probabilistic models with latent variables.
\begin{quote}
	A linear factor model is defined by the use of a stochastic linear decoder function that generates $\vec{x}$ by adding noise to a linear transformation of $\vec{h}$. 
\end{quote}
Note that $\vec{h}$ is a \textit{vector} of arbitrary size, where we assume $p(\vec{h})$ is a \purple{factorial distribution}: $p(\vec{h}) = \prod_i p(h_i)$. This roughly means we assume the elements of $\vec{h}$ are mutually independent\footnote{Note that, technically, this assumption isn't strictly the definition of mutual independence, which requires that every \textit{subset} (i.e. not just the full set) of $\{h_i \in \vec{h}\}$ follow this factorial property.}. The LFM describes the data-generation process as follows:
\begin{compactenum}
	\item Sample the explanatory factors: $\vec{h} \sim p(\vec{h})$.
	\item Sample the real-valued observable variables given the factors:
	
	\graybox{
		\vec{x} = \matr{W} \vec{h} + \vec{b} + \textrm{noise}
		}
\end{compactenum}

\myspace
\p \blue{Probabilistic PCA and Factor Analysis}.
\begin{compactitem}
	\item \textbf{Factor analysis}: 
	\begin{align}
		\vec{h} &\sim \mathcal{N}(\vec{h}; \vec{0}, \matr{I}) \\
		\text{noise} &\sim \mathcal{N}(\vec{0}, \vec{\psi}\equiv\text{diag}(\vec{\sigma^2})) \\
		\vec{x} &\sim \mathcal{N}(\vec{x};~ \vec{b}, \matr{W}\matr{W}^T + \vec{\psi})
	\end{align}
	where the last relation can be shown by recalling that a linear combination of Gaussian variables is itself Gaussian, and showing that $\mathbb{E}_h\left[\vec{x}\right] = \vec{b}$, and $\text{Cov}(\vec{x}) = \matr{W}\matr{W}^T + \vec{\psi}$. \\
	
	\p It is worth emphasizing the interpretation of $\vec{\psi}$ as the matrix of \green{conditional variances} $\sigma_i^2$. \textit{Huh?} Let's take a step back. The fact that we were able to separate the distributions in the above relations for $\vec{h}$ and $\text{noise}$ is from a built-in \underline{assumption} that $\Pr(x_i | \vec{h}, x_{j \ne i}) = \Pr(x_i | \vec{h})$\footnote{Due to $<$\textsc{math}$>$, this introduces a constraint that knowing the value of some element $x_j$ doesn't alter the probability $\Pr(x_i = W_i \cdot h + b_i + \text{noise})$. Given how we've defined the variable $\vec{h}$, this means that knowing $\text{noise}_j$ provides no clues about $\text{noise}_i$. Mathematically, the noise must have a diagonal covariance matrix.}. \\
	
	\redbox[The Big Idea]{
		The latent variable $\vec{h}$ is a big deal because it \textbf{captures the dependencies} between the elements of $\vec{x}$. \textit{How do I know?} Because of our assumption that the $x_i$ are conditionally independent given $\vec{h}$. If, once we specify $\vec{h}$, all the elements of $\vec{x}$ become independent, then any information about their interrelationship is hiding somewhere in $\vec{h}$.
	}
	\vspace{0.3em}
	Detailed walkthrough of Factor Analysis (a.k.a me slowly reviewing, months after taking this note):
	{\small
	\begin{compactitem}
		\item \textbf{Goal}. Analyze and understand the motivations behind how Factor Analysis defines the data-generation process under the framework of LFMs (defined in steps 1 and 2 earlier). Assume $\vec{h}$ has dimension $n$. 
		\item \textbf{Prior}. Defines $p(\vec{h}) := \mathcal{N}(\vec{h}; \vec{0}, \matr{I})$, the unit-variance Gaussian. Explicitly,
			$$
				p(\vec{h}) := \frac{1}{(2\pi)^{n/2}} e^{-\frac{1}{2} \sum_i^n h_i^2}
			$$
		\item \textbf{Noise}. Assumed to be drawn from a Gaussian with diagonal covariance matrix $\matr{\psi} := \diag{\vec{\sigma}^2}$. Explicitly,
		$$
			p(\text{noise} = \vec{a}) := \frac{1}{ (2\pi)^{n / 2} \prod_i^n \sigma_i } e^{-\frac{1}{2} \sum_i^n a_i^2 / \sigma_i^2}
		$$
		
		\item \textbf{Deriving distribution of $\vec{x}$}. We use the fact that any linear combination of Gaussians is itself Gaussian. Thus, deriving $p(\vec{x})$ is reduced to computing it's mean and covariance matrix.
		\begin{align}
			\vec[x]{\mu} &= \E[\vec{h}]{\matr{W} \vec{h} + \vec{b}} \\
			&= \int p(\vec{h}) (\matr{W} \vec{h} + \vec{b}) \mathrm{d}\vec{h} \\
			&= \vec{b} + \int \frac{1}{(2\pi)^{n/2}} e^{-\frac{1}{2} \sum_i^n h_i^2} \matr{W} \vec{h} \mathrm{d}\vec{h} \\
			&= \vec{b} \\
			\text{Cov}(\vec{x}) &= \E{(\vec{x} - \E{\vec{x}}) (\vec{x} - \E{\vec{x}})^T} \\
			&= \E{(\matr{W}\vec{h} + \text{noise})(\vec{h}^T \matr{W}^T + \text{noise}^T)} \\
			&= \E{(\matr{W}\vec{h}\vec{h}^T\matr{W}^T)} + \matr{\psi} \\
			&= \matr{W}\matr{W}^T + \matr{\psi}
		\end{align}
		where we compute the expectation of $\vec{x}$ over $\vec{h}$ because $\vec{x}$ is defined as a function of $\vec{h}$, and noise is always expectation zero.
		\item \textbf{Thoughts}. Not really seeing why this is useful/noteworthy. Feels very contrived (many assumptions) and restrictive -- it only applies if the dependencies between each $x_i$ can be modeled with a random variable $\vec{h}$ sampled from a unit variance Gaussian.
	\end{compactitem}}
	
	
	
	\vspace{0.3em}
	\item \textbf{Probabilistic PCA}: Just factor analysis with $\vec{\psi} = \sigma^2 \matr{I}$. So zero-mean spherical Gaussian noise. It becomes regular PCA as $\sigma \rightarrow 0$. Here we can use an iterative EM algorithm for estimating the parameters $\matr{W}$. 
\end{compactitem}

%
% AUTOENCODERS
% 
\lecture{Deep Learning Research}{Autoencoders (Ch. 14)}{May 07}

\p \blue{Introduction}. An autoencoder learns to copy its input to its output, via an encoder function $\vec{h} = f(\vec{x})$ and a decoder function $\vec{r} = g(\vec{h})$\marginnote{$\vec{r}$ for ``reconstruction''}. Modern autoencoders generalize this to allow for stochastic mappings $p_{encoder}(\vec{h} \mid \vec{x})$ and $p_{decoder}(\vec{x} \mid \vec{h})$.

\myspace
\p \blue{Undercomplete Autoencoders}. Constrain dimension of $\vec{h}$ to be smaller than that of $\vec{x}$. The learning process minimizes some $L(\vec{x}, g(f(\vec{x})))$, where the loss function could be e.g. mean squared error. Be careful not to have too many learnable parameters in the functions $g$ and $f$ (thus increasing model capacity), since that defeats the purpose of using an undercomplete autoencoder in the first place.

\myspace
\p \blue{Regularized Autoencoders}. We can remove the undercomplete constraint/necessity by modifying our loss function. For example, a \green{sparse autoencoder} one that adds a penalty $\Omega(\vec{h})$ to the loss function that encourages the \textit{activations on} (not connections to/from) the hidden layer to be sparse. One way to achieve \textit{actual zeros} in $\vec{h}$ is to use rectified linear units for the activations.


%
% AUTOENCODERS
% 
\lecture{Deep Learning Research}{Representation Learning (Ch. 15)}{May 07}

\myspace
\p \blue{Greedy Layer-Wise Unsupervised Pretraining}. Given:
\begin{compactitem}[-]
	\item Unsupervised learning algorithm $\mathcal{L}$ which accepts as input a training set of examples $\matr{X}$, and outputs an encoder/feature function $f$. 
	\item $f^{(i)}(\matr{\tilde{X}})$ denotes the output of the $i$th layer of $f$, given as \textit{immediate input} the (possibly transformed) set of examples $\matr{\tilde{X}}$.
	\item Let $m$ denote the number of layers (``stages'') in the encoder function (note that each layer/stage here \textit{must} use a representation learning algorithm for its $\mathcal{L}$ e.g. an RBM, autoencoder, sparse coding model, etc.)
\end{compactitem}
The procedure is as follows:
\begin{compactenum}
	\item Initialize.
	\begin{align}
		f(\cdot) &\leftarrow I(\cdot) \\
		\tilde{\matr{X}} &= \matr{X}
	\end{align}
	
	\item For each layer (stage) $i$ in $\text{range}(m)$, do:
	\begin{align}
		f^{(k)} &= \mathcal{L}(\tilde{\matr{X}}) \\
		f(\cdot) &\leftarrow  f^{(k)}\left(f\left(\cdot\right)\right) \\
		\tilde{\matr{X}} &\leftarrow f^{(k)}(\tilde{\matr{X}})
	\end{align}
	In English: just apply the regular learning/training process for each layer/stage \textbf{sequentially and individually}\footnote{In other words, you proceed one layer at a time \textit{in order}. You don't touch layer $i$ until the weights in layer $i - 1$ have been learned.}.
\end{compactenum}
When this is complete, we can run \green{fine-tuning}: train all layers together (including any later layers that could not be pretrained) with a supervised learning algorithm. Note that we do indeed allow the pretrained encoding stages to be optimized here (i.e. not fixed).

% 
% PROBABILISTIC MODELS
% 
\lecture{Deep Learning Research}{Structured Probabilistic Models for DL (Ch. 16)}{October 01, 2017}

\p \blue{Motivation}. In addition to classification, we can ask probabilistic models to perform other tasks such as density estimation ($\vec{x} \rightarrow p(\vec{x})$), denoising, missing value imputation, or sampling. What these [other] tasks have in common is they require a \textit{complete understanding of the input}. Let's start with the most naive approach of modeling $p(\vec{x})$, where $\vec{x}$ contains $n$ elements, each of which can take on $k$ distinct values: we store a lookup table of all possible $\vec{x}$ and the corresponding probability value $p(\vec{x})$. This requires $k^n$ parameters\footnote{Consider the common NLP case where our vector $\vec{x}$ contains $n$ word tokens, each of which can take on any symbol in our vocabulary of size $v$. If we assign $n=100$ and $v=100,000$, which are relatively common values for this case, this amounts to $(1e5)^{1e2} = 10^{500}$ parameters.}. Instead, we use graphs to describe model structure (direct/indirect interactions) to drastically reduce the number of parameters.

\myspace
\p \blue{Directed Models}. Also called \green{belief networks} or \green{Bayesian networks}. Formally, a directed graphical model defined on a set of variables $\{\vec{x}\}$ is defined by a DAG, $\mathcal{G}$, whose vertices are the random variables in the model, and a set of \textbf{local conditional probability distributions}, $p(x_i \mid Pa_{\mathcal{G}}(x_i))$, where $Pa_{\mathcal{G}}(x_i)$ gives the parents of $x_i$ in $\mathcal{G}$. The probability distribution over $\vec{x}$ is given by
\graybox{
	p(\vec{x}) = \prod_i p(x_i \mid Pa_{\mathcal{G}}(x_i))
	}
	
\myspace
\p \blue{Undirected Graphical Models}. Also called \green{Markov Random Fields (MRFs)} or \green{Markov Networks}. Appropriate for situations where interactions do not have a well-defined direction. Each \green{clique} $\mathcal{C}$ (any set of nodes that are all [maximally] connected) in $\mathcal{G}$ is associated with a factor $\phi(\mathcal{C})$. The factor $\phi(\mathcal{C})$, also called a \green{clique potential}, is just a function (not necessarily a probability) that outputs a number when given a possible set of values over the nodes in $\mathcal{C}$. The output number measures the affinity of the variables in that clique for being in the states specified by the inputs\marginnote{Clique potentials are constrained to be nonnegative.}[-2em]. The set of all factors in $\mathcal{G}$ defines an \textbf{unnormalized probability distribution}:
\graybox{
	\widetilde{p}(\vec{x}) = \prod_{\mathcal{C} \in \mathcal{G}} \phi(\mathcal{C})
	}

\myspace
\p \blue{The Partition Function}. To obtain a valid probability distribution, we must normalize the probability distribution:
\begin{align}
	p(\vec{x}) &= \frac{1}{Z} \widetilde{p}(\vec{x}) \\
	Z &= \int \widetilde{p}(\vec{x}) \mathrm{d}\vec{x}
\end{align}
where the normalizing function $Z = Z(\{\phi\})$ is known as the \green{partition function} (physicz sh0ut0uT). It is typically intractable to compute, so we resort to approximations. Note that $Z$ isn't even guaranteed to exist -- it's only for those definitions of the clique potentials that cause the integral over $\widetilde{p}(\vec{x})$ to converge/be defined.

\myspace
\p \blue{Energy-Based Models} (EBMs). A convenient way to enforce $\forall \vec{x}, ~ \widetilde{p}(\vec{x}) > 0$ is to use EBMs, where
\begin{align}
	\widetilde{p}(\vec{x}) \triangleq \exp\left( -E(\vec{x}) \right)
\end{align}
and $E(\vec{x})$ is known as the \green{energy function}\footnote{Physics throwback: this mirrors the Boltzmann factor, $\exp(-\varepsilon/\tau)$, which is proportional to the probability of the system being in quantum energy state $\varepsilon$.}. Many algorithms need to compute not $p_{model}(\vec{x})$ but only $\log\widetilde{p}_{model}(\vec{x})$ (unnormalized log probabilities - logits!). For EBMs with latent variables $\vec{h}$, such algorithms are phrased in terms of the \green{free energy}:
\begin{align}
	\mathcal{F}(\vec{x} = x) &= -\log \sum_{h} \exp\left( - E(\vec{x}=x, ~ \vec{h} = h ) \right)
\end{align}
where we sum over all possible assignments of the latent variables.


\myspace
\p \blue{Separation and D-Separation}. We want to know which subsets of variables are conditionally independent from each other, given the values of other subsets of variables.  A set of variables $\mathbb{A}$ is \green{separated} (if undirected model)/\green{d-separated} (if directed model) from another set of variables $\mathbb{B}$ given a third set of variables $\mathbb{S}$ if the graph structure implies that $\mathbb{A}$ is independent from $\mathbb{B}$ given $\mathbb{S}$. 

\begin{compactitem}
	\item \green{Separation}. For \textit{undirected} models. If variables $a$ and $b$ are connected by a path involving only \underline{unobserved} variables (an \green{active} path), then $a$ and $b$ are \textit{not} separated. Otherwise, they are separated. Any paths containing at least one observed variable are called \green{inactive}.

	\item \green{D-Separation}\footnote{The D stands for dependence.}. For \textit{directed} models. Although there are rules that help determine whether a path between $a$ and $b$ is d-separated, it is simplest to just determine whether $a$ is independent from $b$ given any observed variables along the path.
\end{compactitem}


% ------------------------------------------------
\myspace\Needspace{18\baselineskip}
\subsub{Sampling from Graphical Models}
\myspace
% ------------------------------------------------

\p For directed graphical models, we can do \green{ancestral sampling} to produce a sample $\vec{x}$ from the joint distribution represented by the model. Just sort the variables $x_i$ into a topological ordering such that $\forall i,j: j > i \iff x_i \in Pa_{\mathcal{G}}(x_j)$. To produce the sample, just sequentially sample from the beginning, $x_1 \sim P(x_1)$, $x_2 \sim P(x_2 \mid Pa_{\mathcal{G}}(x_1) )$, etc.\\

\p For undirected graphical models, one simple approach is \green{Gibbs sampling}. Essentially, this involves drawing a conditioned sample from $x_i \sim p(x_i \mid \text{neighbors}(x_i))$ for each $x_i$. This process is repeated many times, where each subsequent pass uses the previously sampled values in $\text{neighbors}(x_i)$ to obtain an asymptotically converging [to the correct distribution] estimate for a sample from $p(\vec{x})$.

% ------------------------------------------------
\myspace\Needspace{18\baselineskip}
\subsub{Inference and Approximate Inference}
\myspace
% ------------------------------------------------

\p One of the main tasks with graphical models is predicting the values of some subset of variables given another subset: inference. Although the graph structures we've discussed allow us to represent complicated, high-dimensional distributions with a reasonable number of parameters, the graphs used for deep learning are usually not restrictive enough to allow efficient inference. \green{Approximate inference} for deep learning usually refers to variational inference, in which we approximate the distribution $p(\vec{h} \mid \vec{v})$ by seeking an approximate distribution $q(\vec{h} \mid \vec{v})$ that is as close to the true one as possible. 

\myspace
\p \blue{Example: Restricted Boltzmann Machine}. The quintessential example of how graphical models are used for deep learning. The canonical RBM is an energy-based model with \textbf{binary} visible and hidden units. Its energy function is
\graybox{
	E(\vec{v}, \vec{h}) &= -\vec{b}^T\vec{v} - \vec{c}^T\vec{h} - \vec{v}^T\matr{W}\vec{h}
	}
where $\vec{b}$, $\vec{c}$, and $\matr{W}$ are unconstrained, real-valued, learnable parameters. One could interpret the values of the bias parameters as the affinities for the associated variable being its given value, and the value $\matr[i,j]{W}$ as the affinity of $v_i$ being its value and $h_j$ being its value at the same time\footnote{More concretely, remember that $\vec{v}$ is a one-hot vector representing some state that can assume len($\vec{v}$) unique values, and similarly for $\vec{h}$. Then $\matr[i,j]{W}$ gives the affinity for the state associated with $v$ being its $i$th value \textit{and} the state associated with $h$ being its $j$th value.}. \\

The restrictions on the RBM structure, namely the fact that there are no intra-layer connections, yields nice properties. Since $\widetilde p(\vec{h},\vec{v})$ can be factored into clique potentials, we can say that:
\begin{align}
	p(\vec{h} \mid \vec{v}) &= \prod_i p(h_i \mid \vec{v}) \\
	p(\vec{v} \mid \vec{h}) &= \prod_i p(v_i \mid \vec{h})
\end{align}
Also, due to the restriction of binary variables, each of the conditionals is easy to compute, and can be quickly derived as
\begin{align}
	p(h_i = 1 \mid \vec{v}) &= \sigma\left( c_i + \vec{v}^T\matr[:,i]{W} \right)
\end{align}
allowing for efficient block Gibbs sampling. 



%
% MONTE CARLO METHODS
% 
\lecture{Deep Learning Research}{Monte Carlo Methods (Ch. 17)}{May 09}

\p \blue{Monte Carlo Sampling (Basics)}. We can approximate the value of a (usually prohibitively large) sum/integral by viewing it as an \textit{expectation} under some distribution. We can then approximate its value by taking samples from the corresponding probability distribution and taking an empirical average. Mathematically, the basic idea is show below:
\graybox{
	s = \int p(\vec{x}) f(\vec{x}) \mathrm{d}\vec{x} = \E[p]{f(\rvec{x})}
	\quad\rightarrow\quad
	\hat{s}_n = \frac{1}{n} \sum_{i = 1,~\rvec{x}^{(i)} \sim p}^{n} f(\vec{x}^{(i)})
}
As we've seen before, the empirical average is an unbiased\footnote{
	Recall that expectations on such an average are still taken over the underlying (assumed) probability distribution:
	\begin{align}
		\E[p]{\hat{s}_n} &= \frac{1}{n} \sum_{i = 1}^{n} \E[p]{f(\vec{x}^{(i)})} \\
		&= \frac{1}{n} \sum_{i = 1}^{n} s \\
		&= s
	\end{align}
	You should think of the expectation $\E[p]{f(\vec{x}^{(i)})}$ as the expected value of the \textit{random sample} from the underlying distribution, which of course is $s$, because we defined it that way. 
	} estimator. Furthermore, the central limit theorem tells us that the distribution of $\hat{s}_n$ converges to a normal distribution with mean $s$ and variance $\Var{f(\vec{x})}/n$. 
	
\myspace
\p \blue{Importance Sampling}. What if it's not feasible for us to sample from $p$? We can approach this a couple ways, both of which will exploit the following identity:
\begin{align}
	p(\vec{x}) f(\vec{x}) = q(\vec{x}) \frac{p(\vec{x}) f(\vec{x})}{q(\vec{x})}
\end{align}
\begin{itemize}
	\item \textbf{Optimal importance sampling}. We can use the aforementioned identity/decomposition to find the \green{optimal $q*$} -- optimal in terms of number of samples required to achieve a given level of accuracy. First, we rewrite our estimator $\hat{s}_p$ (they now use subscript to denote the sampling distribution) as $\hat{s}_q$:
	\begin{align}
		\hat{s}_q &=  \frac{1}{n} \sum_{i = 1,~\rvec{x}^{(i)} \sim q}^{n} \frac{p(\vec{x}^{(i)}) f(\vec{x}^{(i)})}{q(\vec{x}^{(i)})}
	\end{align}
	At first glance, it feels a little wonky, but recognize that we are \textit{sampling from q instead of p} (i.e. if this were an integral, it would be over $q(\vec{x})\mathrm{d}\vec{x}$). The catch is that, now, the variance can be greatly sensitive to the choice of $q$:
	\begin{align}
		\Var{\hat{s}_q} &= \Var{\frac{p(\vec{x}) f(\vec{x})}{q(\vec{x})}} / n
	\end{align}
	with the optimal (minimum) value of $q$ at:
	\graybox{
		q* &= \frac{p(\vec{x})\mid f(\vec{x}) \mid}{Z}
		}
		
	\item \textbf{Biased importance sampling}. Computing the optimal value of $q$ can be as challenging/infeasible as sampling from $p$. Biased sampling does not require us to find a normalization constant for $p$ or $q$. Instead, we compute:
	\begin{align}
		\hat{s}_{BIS} &= \dfrac{
			\sum_{i = 1}^{n} \frac{\tilde p(\vec{x}^{(i)})}{\tilde q(\vec{x}^{(i)})}  f(\vec{x}^{(i)})
			}{
			\sum_{i = 1}^{n} \frac{\tilde p(\vec{x}^{(i)})}{\tilde q(\vec{x}^{(i)})}  
			}
	\end{align}
	where $\tilde p$ and $\tilde q$ are the unnormalized forms of $p$ and $q$, and the $\vec{x}^{(i)}$ samples are still drawn from [the original/unknown] $q$. $\E{\hat s_{BIS}} \ne s$ except asymptotically when $n \rightarrow \infty$. 
\end{itemize}







%
% Confronting the Partition Function
% 
\lecture{Deep Learning Research}{Confronting the Partition Function (Ch. 18)}{August 30, 2018}



\p \blue{Noise Contrastive Estimation} (NCE) (18.6). We now estimate
\begin{align}
	\log p_{model}(\vec x)
		&= \log \tilde{p}_{model}(\vec x; \vec\theta) + c
\end{align}
and explicitly learn an approximation, $c$, for $-\log Z(\vec\theta)$. Obviously MLE would just try jacking up $c$ to maximize this, so we adopt a surrogate supervised training problem: binary classification that a given sample $\vec x$ belongs to the (true) data distribution $p_{data}$ or to the noise distribution $p_{noise}$. We introduce binary variable $y$ to indicate whether the sample is in the true data distribution ($y{=}1$) or the noise distribution ($y{=}0$). Our surrogate model is thus defined by
\begin{align}
	p_{joint}(y{=}1) &= \onehalf \\
	p_{joint}(\vec x \mid y{=}1) &= p_{model}(\vec x) \\
	p_{joint}(\vec x \mid y{=}0) &= p_{noise}(\vec x) 
\end{align}
We can now use MLE on the optimization problem,
\graybox{
	\vec\theta , c 
		&= \argmax_{\vec\theta , c} \E[\vec x, y \sim p_{train}]{\log p_{joint}(y \mid \vec x)} \\
	p_{joint}(y{=}1 \mid \vec x)
		&= \frac{  p_{model}(\vec x) }{  p_{model}(\vec x) + p_{noise}(\vec x) } \\
		&= \inv{  1 + p_{noise}(\vec x) / p_{model}(\vec x)  } \\
		&= \sigma\left(  
			\log p_{model}\left( \vec x \right) -
			\log p_{noise}\left( \vec x \right)
		\right)
}

































%
% APPROXIMATE INFERENCE
% 
\lecture{Deep Learning Research}{Approximate Inference (Ch. 19)}{Nov 15, 2017}

% log(p(v; theta))
\newcommand\lpv{\log p(\vec{v}; \vec{\theta})}
\renewcommand\elbo{\mathcal{L}(\vec v, \vec{\theta}, q)}
\newcommand\Dkl[2]{D_{KL}\left( #1 ~||~ #2 \right)}

\p \blue{Overview}. Most graphical models with multiple layers of hidden variables have intractable posterior distributions. This is typically because the partition function scales exponentially with the number of units and/or due to marginalizing out latent variables. Many approximate inference approaches make use of the observation that exact inference can be described as an optimization problem. \\ 

Assume we have a probabilistic model consisting of observed variables $\vec{v}$ and latent variables $\vec{h}$. We want to compute $\log p(\vec v;\vec{\theta})$, but it's too costly to marginalize out $\vec{h}$. Instead, we compute a lower bound $\mathcal{L}(\vec v, \vec{\theta}, q)$ -- often called the \green{evidence lower bound} (ELBO) or negative \green{variational free energy} -- on $\log p(\vec v; \vec{\theta})$\footnote{Recall that $D_{KL}(P || Q) = \E[x \sim P(x)]{\log \frac{P(x)}{Q(x)} }$}:\marginnote{$q$ is an arbitrary probability distribution over $\vec{h}$. Note that the book will write $q$ when they really mean $q(\vec h \mid \vec v)$.}[2em]
\graybox{
	\elbo &= \lpv - \Dkl{ q(\vec{h} \mid \vec{v}) }{p(\vec{h} \mid \vec{v}; \vec{\theta})} \\
	&= - \E[\vec{h} \sim q(\vec h \mid \vec v)]{\log p(\vec h, \vec v)} + H(q(\vec h \mid \vec v))
}
where the second form is the more canonical definition\footnote{This can be derived easily from the first form. Hint: $$
	\log \frac{q(\vec h \mid \vec v)}{ p(\vec h \mid \vec v)} = 
	\log \frac{q(\vec h \mid \vec v)}{ p(\vec h, \vec v; \vec{\theta}) / p(\vec v; \vec{\theta})} 
$$}. Note that $\elbo$ is a lower-bound on $\lpv$ by definition, since 
$$\lpv - \elbo = D_{KL}\left(q(\vec{h} \mid \vec{v}) ||
p(\vec{h} \mid \vec{v}; \vec{\theta}) \right) \ge 0
$$
With equality (to zero) iff $q$ is the same distribution as $p(\vec h \mid \vec v)$. In other words, $\mathcal L$ can be viewed as a function parameterized by $q$ that's maximized when $q$ is $p(\vec h \mid \vec v)$, and with maximal value $\log p(\vec v)$. Therefore, we can cast the \textit{inference} problem of computing the (log) probability of the observed data  $\log p(\vec v)$ into an \textit{optimization} problem of maximizing $\mathcal L$. Exact inference can be done by searching over a family of functions that contains $p(\vec h \mid \vec v)$. 

\myspace
\p \blue{Expectation Maximization} (19.2). Technically not an approach to approximate inference, but rather an approach to learning with an approximate posterior. Popular for training models with latent variables. The EM algorithm consists of alternating between the followi[p]ng 2 steps until convergence:
\begin{compactenum}
	\item \textbf{E-step}. For each training example $\vec{v}^{(i)}$ (in current batch or full set), set
	\begin{align}
		q(\vec{h} \mid \vec{v}^{(i)}  )
			&= p(\vec{h} \mid \vec{v}^{(i)}; \vec{\theta}^{(0)})
	\end{align}
	where $\vec{\theta}^{(0)}$ denotes the current parameter values of the model at the beginning of the E-step. This can also be interpreted as maximizing $\mathcal L$ w.r.t. $q$. 
	
	\item \textbf{M-step}. Update the parameters $\vec{\theta}$ by completely or partially finding
	\begin{align}
		\argmax_{\vec{\theta}} \sum_i \mathcal{L}\left(\vec{v}^{(i)}, \vec{\theta}, q(\vec{h} \mid \vec{v}^{(i)}; \vec{\theta}^{(0)} ) \right)
	\end{align} 
\end{compactenum}













%
% APPROXIMATE INFERENCE
% 
\lecture{Deep Learning Research}{Deep Generative Models (Ch. 20)}{July 28, 2018}


\p \blue{Boltzmann Machines} (20.1). An energy-based model over a $d$-dimensional binary random vector $\rvec{x} \in \{0, 1\}^d$. The energy function is simply $E(\vec{x}) = -\vec{x}^T \matr U \vec x - \vec{b}^T \vec x$, i.e. parameters between all pairs of $x_i, x_j$, and bias parameters for each $x_i$\footnote{Authors are being lazy because it's assumed the reader is familiar (which is fair, I guess). i.e. they aren't mentioning that this formula implies that $\matr U$ is either lower or upper triangular, and the diagonal is zero.}. In settings where our data consists of samples of fully observed $\vec x$, this is clearly limited to very simple cases, since e.g. the probability of some $x_i$ being on is given by logistic regression from the values of the other units. 

\begin{example}[Proof: prob of \(x_i\) being on is logistic regression on other units]
	\tiny
	It's important to be as specific as possible here, since the task stated as-is is ambiguous. We want to prove that the probability of some fully observed state $\vec x$ that has its $i$th element clamped to $1$, which I'll denote as $p_{i=on}(\vec x)$, is logistic regression over the other units.\\
	
	To prove this, it's easier to use the conventional definition where $\matr U$ is symmetric with zero diagonal, and we write $E(\vec x)$ as
	\begin{align}
		E(\vec x)
			&= - \sum_{i=1}^{d} \sum_{j=i+1}^{d} x_i U_{i,j} x_j - \sum_{i=1}^{d} b_i x_i
	\end{align}
	where the difference is that we explicitly only sum over the upper triangle of $\matr U$. \\
	
	Intuitively, since $p(\{ \vec x \}_{j \ne i}) = p_{i=on}(\vec x) + p_{i=off}(\vec x)$, our final formula for $p_{i=on}$ should only contain terms involving the parameters that interact with $x_i$, and only for those cases where $x_i=1$. This motivates exploring the formula for $\Delta E_i (\vec x) \triangleq E_{i=off} - E_{i=on}$ where I've dropped the explicit notation on $\vec x$ for simplicity/space. Before jumping in to deriving this, step back and realize that $\Delta E_i$ will only contain summation terms where either the row or column index of $\matr U$ is $i$, and only for terms with bias element $b_i$. Since our summation is over the upper triangle of $\matr U$, this means terms along the slices $U_{i, i+1:d}$ and $U_{1:i-1, i}$. Now there is no derivation needed and we can simply write
	\begin{align}
		\Delta E_i 
			&= \sum_{k = i + 1}^{d} U_{i, k} x_k + \sum_{k = 1}^{i-1} x_k U_{k, i} + b_i 
	\end{align}
	
	The goal is to use this to get a logistic-regression-like formula for $p_{i=on}$, so we should now think about the relationship between any given $p(\vec x)$ and the associated $E(\vec x)$. The critical observation is that $E(\vec x) = -\ln p(\vec x) - \ln Z$, which therefore means
	\begin{align}
		\Delta E_i
			&= \ln p_{i=on}(\vec x) - \ln p_{i=off}(\vec x)
			= - \ln\left( \frac{ 1 - p_{i=on}(\vec x)  }{p_{i=on}(\vec x)}  \right) \\
		\exp(- \Delta E_i)
			&=  \frac{ 1 - p_{i=on}(\vec x)  }{p_{i=on}(\vec x)} =  \frac{ 1 }{p_{i=on}(\vec x)} - 1 \\
		\therefore p_{i=on}(\vec x)
			&= \frac{ 1 }{ 1 +  \exp(- \Delta E_i) }
	\end{align}
	Since $\Delta E_i$ is a linear function of all other units, we have proven that $p_{i=on}(\vec x)$ for some state $\vec x$ reduces to logistic regression over the other units. 
\end{example}

\myspace
\p \blue{Restricted Boltzmann Machines} (20.2). A BM with variables partitioned into two sets: hidden and observed. The graphical model is bipartite over the hidden and observed nodes, as I've drawn in the example below. 
\begin{center}
	\begin{tikzpicture}[font=\sffamily,node distance=1.cm,->,>=latex,auto,line width=0.4mm]
	
	\tikzset{node st/.style={state, draw=none,
			fill=gray!30!white,
			text=blue!60!white}}
	\tikzset{node obs/.style={state, draw=none,
			fill=gray!60!black,
			text=blue!20!white}}
	
	\node[node obs] (X1) {X1};
	\node[node obs, right=of X1] (X2) {X2};
	\node[node obs, right=of X2] (X3) {X3};
	\node[node obs, right=of X3] (X4) {X4};
	
	\node[node st, below=of X1, xshift=0.75cm] (Y1) {H1};
	\node[node st, right=of Y1] (Y2) {H2};
	\node[node st, right=of Y2] (Y3) {H3};
	
	\draw[every loop,
	auto=right,
	draw=blue!30!gray]
	(X1) edge[-]				node{} (Y1)
	(X1) edge[-]				node{} (Y2)
	(X1) edge[-]				node{} (Y3)
	(X2) edge[-]				node{} (Y1)
	(X2) edge[-]				node{} (Y2)
	(X2) edge[-]				node{} (Y3)
	(X3) edge[-]				node{} (Y1)
	(X3) edge[-]				node{} (Y2)
	(X3) edge[-]				node{} (Y3)
	(X4) edge[-]				node{} (Y1)
	(X4) edge[-]				node{} (Y2)
	(X4) edge[-]				node{} (Y3);
	\end{tikzpicture}
\end{center}
Although the joint distribution $p(\vec x, \vec h)$ has a potentially intractable partition function, the conditional distributions can be computed efficiently by exploiting independencies:
\begin{align}
	p(\vec h \mid \vec x)
		&= \prod_{j = 1}^{n_h} \sigma \left( [2 \vec h - 1] \odot [\vec c + \matr{W}^T \vec x]    \right)_j \\
	p(\vec x \mid \vec h) 
		&= \prod_{i = 1}^{n_x} \sigma \left( [2 \vec x - 1] \odot [\vec b + \matr W \vec h]  \right)_i
\end{align}
where $\vec b$ and $\vec c$ are the observed and hidden bias parameters, respectively.

\myspace
\p \blue{Deep Belief Networks} (20.3). Several layers of (usually binary) latent variables and a single observed layer. The "deepest" (away from the observed) layer connections are undirected, and all other layers are directed and pointing toward the data. I've drawn an example below.

\begin{center}
	\scalebox{0.8}{
	\begin{tikzpicture}[font=\sffamily,node distance=1.cm,->,>=latex,auto,line width=0.4mm]
	
	\tikzset{node st/.style={state, draw=none,
			fill=gray!30!white,
			text=blue!60!white}}
	\tikzset{node obs/.style={state, draw=none,
			fill=gray!60!black,
			text=blue!20!white}}
	
	\node[node obs] (X1) {H1$^{(2)}$};
	\node[node obs, right=of X1] (X2) {H2$^{(2)}$};
	\node[node obs, right=of X2] (X3) {H3$^{(2)}$};
	
	\node[node obs, below=of X1, xshift=-0.75cm] (X11) {H1$^{(1)}$};
	\node[node obs, right=of X11] (X21) {H2$^{(1)}$};
	\node[node obs, right=of X21] (X31) {H3$^{(1)}$};
	\node[node obs, right=of X31] (X41) {H4$^{(1)}$};	
	
	\node[node st, below=of X11, xshift=1.5cm] (Y1) {X1};
	\node[node st, right=of Y1] (Y2) {X2};
	\node[node st, right=of Y2] (Y3) {X3};

	
	\foreach \o in {X11, X21, X31, X41} {
		\foreach \i in {X1, X2, X3} {
			\draw[every loop, auto=right, blue!30!gray] (\i) edge[-] (\o) ; 
		};
	} ;
	
	\foreach \o in {Y1, Y2, Y3} {
		\foreach \i in {X11, X21, X31, X41} {
			\draw[every loop, auto=right, blue!30!gray] (\i) edge[->] (\o);
		};	
	};
	
	\end{tikzpicture}}
\end{center}
We can sample from a DBN via first Gibbs sampling on the undirected layer, then ancestral sampling through the rest of the (directed) model to eventually obtain a sample from the visible units.

\myspace 
\p \blue{Deep Boltzmann Machines} (20.4). Same as DBNs, but now all layers are undirected. Note that this is very close to the standard RBM, since we have a set of hidden and observed variables, except now we interpret certain subgroups of hidden units as being in a ``layer'', thus allowing for connections between hidden units in adjacent layers. What's interesting is that this still defines a bipartite graph, with odd-numbered layers on one side and even on the other\footnote{Recall that this immediately implies that units in all odd layers are conditionally independent given the even layers (and vice-versa for even to odd).}. \\

\myspace
\p \blue{Differentiable Generator Networks} (20.10.2). Use a differentiable function $g(\vec z; \vec{\theta}^{(g)})$ to transform samples of latent variables $\rvec z$ to either (a) samples $\rvec x$, or (b) distributions over samples $\rvec x$. For an example of case (a), the standard procedure for sampling from $\mathcal N(\vec\mu, \matr\Sigma)$ is to first sample from $\mathcal{N}(\vec 0, \matr I)$ into a generator network consisting of a single affine layer:\marginnote{\textbf{Case a}: interpret $g(\vec z)$ as emitting $\vec x$ directly.}[-1em]
$$
	\vec x \leftarrow g(\vec z) = \vec{\mu} + \matr L \vec z
$$
where $\matr L$ is the \textit{Cholesky decomposition}\footnote{The [unique] Cholesky decomposition of a (real-symmetric) p.d. matrix $\matr A$ is a decomposition of the form $\matr A = \matr L \matr L^T$, where $\matr L$ is lower triangular.} of $\matr\Sigma$. In general, we think of the generator function $g$ as providing a change of variables that transforms the distribution over $\rvec z$ into the desired distribution $\rvec x$. Of course, there \textit{is} an exact formula for doing this,
\begin{align}
	p_x(\vec x) 
		&= \frac{ p_z(g^{-1}(\vec x))  }{  \left|  \det \pderiv{g}{\vec z}  \right| }
\end{align}
but it's usually far easier to use indirect means of learning $g$, rather than trying to maximize/evaluation $p_x(\vec x)$ directly. \\

For case (b), the common approach is to train the generator net to emit conditional probabilities\marginnote{\textbf{Case b}: interpret $g(\vec z)$ as emitting $p(\vec x \mid \vec z)$.}[0em]
\begin{align}
	p(x_i \mid \vec z) = g(\vec z)_i \qquad\qquad p(\vec x) = \E[\vec z]{p(\vec x \mid \vec z)}
\end{align}
which can also support generating discrete data (case a cannot). The challenge in training generator networks is that we often have a set of examples $\vec x$, but the value of $\vec z$ for each $\vec x$ is not fixed and known ahead of time. We'll now look at some ways of training generator nets given only training samples for $\vec x$. Note that such a setting is very unlike unsupervised learning, where we typically interpret $\vec x$ as inputs that we don't have labels for, while here we interpret $\vec x$ as \textit{outputs} that we don't know the associated inputs for. 

\myspace 
\p \blue{Variational Autoencoders} (20.10.3). VAEs are directed models that use learned approximate inference and can be trained purely with gradient-based methods. To generate a sample $\vec x$, the VAE first samples $\vec z$ from the \textit{code distribution} $p_{model}(\vec z)$. This sample is then fed through the a differentiable generator network $g(\vec z)$. Finally, $\vec x$ is sampled from $p_{model}(\vec x; g(\vec z)) = p_{model}(\vec x \mid \vec z)$.  



% ========================= SECTIONSWITCH ========================= 

% ==================================================================================
% PAPERS AND TUTORIALS
% ==================================================================================
\mysection{Papers and Tutorials}\label{Papers and Tutorials}


% ============================================================================================
\lecture{Papers and Tutorials}{WaveNet}{January 15, 2017}

\myspace
\p \blue{Introduction}. 
\begin{compactitem}
	\item Inspired by recent advances in \green{neural autoregressive generative models}, and based on the PixelCNN architecture.  
	
	\item Long-range dependencies dealt with via ``dilated causal convolutions, which exhibit very large receptive fields.''
\end{compactitem}

\myspace
\p \blue{WaveNet}. The joint probability of a waveform $x = \{x_1, \ldots, x_T\}$ is factorised as a product of conditional probabilities, 
\begin{align}
p(x) = \prod_{t = 1}^{T} p(x_t \mid x_1, \ldots, x_{t - 1})
\end{align}
which are modeled by a stack of convolutional layers (no pooling). Main ingredient of WaveNet is \textit{dilated} causal convolutions, illustrated below. Note the absence of recurrent connections, which makes them faster to train than RNNs, but at the cost of requiring many layers,  or large filters to increase the receptive field\footnote{Loose interpretation of receptive fields here is that large fields can take into account more info (back in time) as opposed to smaller fields, which can be said to be ``short-sighted''}. 

\myfig[0.5\textwidth]{CausalConv.PNG}

\begin{definition}[-1em][Dilated Convolution]
	A dilated convolution (a convolution with holes) is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step. It is equivalent to a convolution with a larger filter derived from the original filter by dilating it with zeros, but is significantly more efficient. A dilated convolution effectively allows the network to operate on
	a coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but
	here the output has the same size as the input. As a special case, dilated convolution with dilation
	1 yields the standard convolution.
\end{definition}

\myspace
\p \blue{Softmax distributions}. To deal with the fact that there are $2^{16}$ possible values, first apply a \green{$\mu$-law companding transformation}\footnote{In telecommunication and signal processing \textbf{companding} (occasionally called compansion) is a method of mitigating the detrimental effects of a channel with limited dynamic range.} to data, and then quantize it to 256 possible values:\marginnote{$-1 < x_t  < 1$ and $\mu = 255$}[2em]
\graybox{
	f(x_t) = \mathrm{sign}(x_t)\frac{\ln(1+\mu|x_t|)}{\ln(1 + \mu)}
	}
which (after plotting in Wolfram) looks identical to the sigmoid function.

\myspace
\p \blue{Gated Activation and Residual/Skip Connections}. Use the same gated activation unit as PixelCNN:
\begin{align}
z = \tanh\left( W_{f,k} * x \right) ~ \odot ~ \sigma\left( W_{g,k} * x \right)
\end{align}
where $*$ denotes conv operator, $\odot$ denotes elem-wise mult., $k$ is layer index, $f,g$ denote filter/gate, and $W$ is learnable conv filter. This is illustrated below, along with the residual/skip connections used to speed up convergence/enable training deeper models.

\myfig[0.6\textwidth]{WaveNetRes.PNG}

The 1x1 blocks are 1x1 convolutions (i.e. position-wise dense layers). 

\Needspace{15\baselineskip}
\bluesec{Conditional Wavenets}. Can also model conditional distribution of $x$ given some additional $h$ (e.g. speaker identity).
\begin{align}
p(\vec x \mid \vec h) = \prod_{t = 1}^{T} p(x_t \mid x_1, \ldots, x_{t - 1}, h)
\end{align}
\begin{compactitem}[$\rightarrow$]
	\item \textbf{Global conditioning}. Single $h$ that influences output dist. accross all times. Activation becomes:
	\begin{align}
	z = \tanh\left( W_{f,k} * \vec x +  V_{f,k}^T \vec h\right) 
		~ \odot ~ 
		\sigma\left( W_{g,k} * \vec x + V_{g,k}^T \vec h\right)
	\end{align}
	
	\item \textbf{Local conditioning}. Have a second time-series $h_t$. They first transform this $h_t$ using a \green{transposed CNN (learned upsampling)} that maps it to a new time-series $\vec y = f(\vec h)$ w/same resolution as $\vec x$. 
		\begin{align}
	z = \tanh\left( W_{f,k} * \vec x +  V_{f,k} * \vec y \right) 
	~ \odot ~ 
	\sigma\left( W_{g,k} * \vec x + V_{g,k}^T \vec y\right)
	\end{align}
	
	
\end{compactitem}




\myspace
\p \blue{Experiments}. 
\begin{itemize}
	\item \textbf{Multi-Speaker Speech Generation}. Dataset: multi-speaker corpus of 44 hours of data from 109 different speakers\footnote{Speakers encoded as ID in form of a one-hot vector}. Receptive field of 300 milliseconds.
	
	
	\item \textbf{Text-to-Speech}. Single-speaker datasets of 24.6 hours (English) and 34.8 hours (Chinese) speech. Locally conditioned on \textit{linguistic features}. Receptive field of 240 milliseconds. Outperformed both LSTM-RNN and HMM.
	
	
	\item \textbf{Music}. Trained the WaveNets to model two music datasets: (1) 200 hours of annotated music audio, and (2) 60 hours of solo piano music from youtube. Larger receptive fields sounded more musical.
	
	\item \textbf{Speech Recognition}. ``With WaveNets we have shown that layers of dilated convolutions allow the receptive field to grow longer in a much cheaper way than using LSTM units.''
\end{itemize}

\myspace
\p \blue{Conclusion} (verbatim): ``This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned
on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition.''




% ============================================================================================
\lecture{Papers and Tutorials}{Neural Style}{January 22}

\p \blue{Notation}. 
\begin{compactitem}
	\item \textbf{Content image}: $\vec{p}$
	\item \textbf{Filter responses}: the matrix $P^l \in \mathcal{R}^{N_l \times M_l}$ contains the activations of the filters in layer $l$, where $P_{ij}^l$ would give the activation of the $i$th filter at position $j$ in layer $l$. $N_l$ is number of feature maps, each of size $M_l$ (height $\times$ width of the feature map)\footnote{If not clear, $M_l$ is a scalar, for any given value of $l$.}.
	\item \textbf{Reconstructed image}: $\vec{x}$ (initially random noise). Denote its corresponding filter response matrix at layer $l$ as $P^l$. 
\end{compactitem}

\myspace
\p \blue{Content Reconstruction}. 
\begin{enumerate}
	\item Feed in \green{content image} $~\vec{p}~$ into pre-trained network, saving any desired filter responses during the forward pass. These are interpreted as the various ``encodings'' of the image done by the network. Think of them analogously to ``ground-truth'' labels.
	
	\item Define $\vec x$ as the \green{generated image}, which we first initialize to random noise. We will be changing the pixels of $\vec x$ via gradient descent updates. 
	
	\item Define the \green{loss function}. After each forward pass, evaluate with squared-error loss between the two representations at the layer of interest:
	\graybox{
		\mathcal{L}_{content}(\vec p, \vec x, l) &= \onehalf \sum_{i, j} (F_{ij}^l - P_{ij}^l)^2 \tlab{1} \\
		\pderiv{\mathcal L_{content}}{F_{ij}^l} &= 
			\begin{cases}
			(F^l - P^l)_{ij} & F_{ij}^l > 0 \\
			0 & F_{ij}^l < 0
		\end{cases}\tlab{2}
		}
	where it appears we are assuming ReLU activations (\red{?}).
	
	\item Compute iterative updates to $\vec x$ via \green{gradient descent} until it generates the same response in a certain layer of the CNN as the original image $\vec p$. 
\end{enumerate}

\myspace
\p \blue{Style Representation}. On top of the CNN responses in each layer, the authors built a style representation that computes the correlations between the different [aforementioned] filter responses. The correlation matrix for layer $l$ is denoted in the standard way with a Gram matrix $G^l \in \mathcal{R}^{N_l \times N_l}$, with entries
\begin{align}
G_{ij}^l &= \langle F^l_i, F_j^l \rangle =  \sum_k F_{ik}^l F_{jk}^l \tlab{3}
\end{align}
To generate a texture that matches the style of a given image, do the following.
\begin{enumerate}
	\item Let $\vec a$ denote the original [style] image, with corresponding $A^l$. Let $\vec x$, initialized to random noise, denote the generated [style] image, with corresponding $G^l$.
	
	\item The contribution to the loss of layer $l$, denoted $E_l$, to the total loss, denoted $\mathcal L_{style}$, is given by
	\graybox{
		E_l &= \frac{1}{4 N_l^2 M_l^2} \sum_{ij} (G_{ij}^l - A_{ij}^l)^2 \tlab{4} \\
		\mathcal L_{style}(\vec a, \vec x) &= \sum_{l = 0}^L w_l E_l \tlab{5} \\
		\pderiv{ E_{l}}{F_{ij}^l} &= 
		\begin{cases}
			\frac{1}{N_l^2 M_l^2}	\left( (F^l)^T (G^l - A^l) \right)_{ji} & F_{ij}^l > 0 \\
			0 & F_{ij}^l < 0
		\end{cases}\tlab{6}
	}
	where $w_l$ are [as of yet unspecified] weighting factors of the contribution of layer $l$ to the total loss. 
\end{enumerate}

\myspace
\p \blue{Mixing content with style}. Essentially a joint minimization that combines the previous two main ideas. 
\begin{enumerate}
	\item Begin with the following images: white noise $\vec x$, content image $\vec p$, and style image $\vec a$. 
	
	\item The loss function to minimize is a linear combination of ~\ref{1} and ~\ref{5}:
	\graybox{
		\mathcal L_{total}(\vec p, \vec a, \vec x, l) 
			&= \alpha \mathcal L_{content}(\vec p, \vec x, l) 
			+ \beta \mathcal L_{style}(\vec a, \vec x) \tlab{7}
		}
	Note that we can choose which layers $L_{style}$ uses by tweaking the layer weights $w_l$. For example, the authors chose to set $w_l = 1/5$ for 'conv[1, 2, 4, 5]\_1' and 0 otherwise. For the ratio $\alpha/\beta$, they explored $1 \times 10^{-3}$ and $1 \times 10^{-4}$.
\end{enumerate}





% ============================================================================================
\lecture{Papers and Tutorials}{Neural Conversation Model}{February 8}
\vspace{-1em}
[{\scriptsize Reminder: \red{red text} means I need to come back and explain what is meant, once I understand it.}]

\p \blue{Abstract}. This paper presents a simple approach for conversational modeling which uses the sequence to sequence framework. It can be trained end-to-end, meaning fewer hand-crafted rules. The \red{lack of consistency} is a common failure of our model. 

\myspace
\p \blue{Introduction}. Major advantage of the seq2seq model is it requires little feature engineering and domain specificity. Here, the model is tested on chat sessions from an IT helpdesk dataset of conversations, as well as movie subtitles.

\myspace
\p \blue{Related Work}. The authors' approach is based on the following (linked and saved) papers on seq2seq:
\begin{compactitem}
	\item \href{https://arxiv.org/pdf/1306.3584.pdf}{Kalchbrenner \& Blunsom, 2013.}
	\item \href{https://arxiv.org/pdf/1409.3215.pdf}{Sutskever et al., 2014.} (Describes Seq2Seq model)
	\item \href{https://arxiv.org/pdf/1409.0473.pdf}{Bahdanau et al., 2014.}
\end{compactitem}

\myspace
\p \blue{Model}. Succinctly described by the authors:\vspace{-1em}
\begin{quote}
	The model reads the input sequence one token at a time, and predicts the output sequence, also one token at a time. During training, the true output sequence is given to the model, so learning can be done by backpropagation. The model is trained to maximize the cross entropy of the correct sequence given
	its context. During inference, in which the true output sequence is not observed, we simply feed the predicted output token as input to predict the next output. This is a greedy inference approach. 
\end{quote}
\marginnote{Example of less greedy approach: \red{beam search}.}[-6em]
\myfig[0.8\textwidth]{Seq2SeqModel.PNG}

\myspace
\p The \green{thought vector} is the hidden state of the model when it receives [as input] the end of sequence symbol $\langle eos \rangle$, because it stores the info of the sentence, or \textit{thought}, ``ABC''. The authors acknowledge that this model will \textit{not} be able to ``solve'' the problem of modeling dialogue due to the objective function not capturing the actual objective achieved through human communication, which is typically longer term and based on exchange of information [rather than next step prediction]\marginnote{Ponder: what \textit{would} be a reasonable objective function \& model for conversation?}[-2em]\footnote{I'd imagine that, in order to model human conversation, one obvious element needed would be a \textit{memory}. Reminds me of DeepMind's DNC. There would need to be some online filtering \& output process to capture the crucial aspects/info to store in memory for later, and also some method of retrieving them when needed later. The method for retrieval would likely be some inference process where, given a sequence of inputs, the probability of them being related to some portion of memory could be trained. This would allow for conversations that stretch arbitrarily back in the past. Also, when storing the memories, I'd imagine a reasonable architecture would be some encoder-decoder for a sparse distributed representation of memory.}.

\myspace
\p \blue{IT Data \& Experiment}.\marginnote{Reminder: Check out \href{https://github.com/farizrahman4u/seq2seq}{this git repo}}
\begin{compactitem}
	\item \textbf{Data Description}: Customers talking to IT support, where typical interactions are 400 words long and turn-taking is clearly signaled. 
	
	\item \textbf{Training Data}: 30M tokens, 3M of which are used as validation. They built a vocabulary of the most common 20K words, and introduced special tokens indicating turn-taking and actor.
	
	\item \textbf{Model}: A single-layer LSTM with 1024 memory cells. 
	
	\item \textbf{Optimization}: SGD with gradient clipping.
	
	\item \textbf{Perplexity}: At convergence, achieved \red{perplexity} of 8, whereas an n-gram model achieved 18.
\end{compactitem}

% ============================================================================================
\lecture{Papers and Tutorials}{NMT By Jointly Learning to Align \& Translate}{February 27}

\href{https://arxiv.org/abs/1409.0473}{[Bahdanau et. al, 2014]}. The primary motivation for me writing this is to better understand the \green{attention mechanism} in my sequence to sequence chatbot implementation.

\myspace
\p \blue{Abstract}. The authors claim that using a fixed-length vector [in the vanilla encoder-decoder for NMT] is a bottleneck. They propose allowing a model to (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.

\myspace
\p \blue{Learning to Align\footnote{By ``align'' the authors are referring to aligning the source-search to the relevant parts for prediction.} and translate}. 
\begin{itemize}
	\item \textbf{Decoder}. Their encoder defines the conditional output distribution as
	\begin{align}
		p(y_i \mid y_1, \ldots, y_{i - 1}, \vec{x}) &= g(y_{i - 1}, s_i, c_i) \\
		s_i &= f(s_{i - 1}, y_{i - 1}, c_i)
	\end{align}
	where $s_i$ is the RNN [decoder] hidden state at time $i$. 
	\begin{compactitem}
		\item NOTE: $c_i$ is \textit{not} the $i$th element of the standard context vector; rather, it is \textit{itself} a distinct context vector that depends on a sequence of \green{annotations} $(h_1, \ldots, h_{T_x})$. It seems that each annotation $h_i$ is a hidden (encoder) state ``that contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence.''
		
		\item The context vector $c_i$ is computed as follows:
		\begin{align}
			c_i &= \sum_{j = 1}^{T_x} \alpha_{ij} h_j \\
			\alpha_{ij} &= \frac{\exp(e_{ij})}{\sum_{k = 1}^{T_x} \exp(e_{ik})} \\
			e_{ij} &= a(s_{i - 1}, h_j) 
		\end{align}
		where the function $e_{ij}$ is given by an \green{alignment model} which scores how well the inputs around position $j$ and the output at position $i$ match. 
	\end{compactitem}
	
	
	\item \textbf{Encoder}. It's just a bidirectional RNN. What they call ``annotation $h_j$'' is literally just a concatenated vector of $h_j^{forward}$ and $h_j^{backward}$
\end{itemize}


\myspace
\subsub{Detailed Model Architecture}

(Appendix A). Explained with the TensorFlow user in mind.\\

\myspace
\p \blue{Decoder Internals}. It's just a GRU. However, it will be helpful to detail how we format the inputs (given we now have attention). Wherever we'd usually pass the previous decoder state $s_{i - 1}$, we now pass a \textit{concatenated} state, $[s_{i - 1},~ c_i]$, that also contains the $i$th context vector. Below I go over the flow of information from GRU input to output:
\begin{compactenum}
	\item \textbf{Notation}: $y_t$ is the loop-embedded \underline{output} of the decoder (prediction) at time $t$, $s_t$ is the internal hidden state of the decoder at time $t$, and $c_t$ is the context vector at time $t$. $\tilde s_t$ is the proposed/proposal state at time $t$.
	\item \textbf{Gates}:
	\begin{align}
		z_t &= \sigma\left( W_z y_{t - 1} + U_z [s_{t - 1},~ c_t] \right)
		\qquad \mgreen{\text{[update gate]}} \\
		r_t &= \sigma\left( W_r y_{t - 1} + U_r [s_{t - 1},~ c_t] \right)
		\qquad \mgreen{\text{[reset gate]}} \\
	\end{align}
		
	\item \textbf{Proposal state}:
	\begin{align}
		\tilde s_t &= \tanh\left( W y_{t - 1} + U [r_t \circ s_{t - 1}, ~ c_t]  \right)
	\end{align}
	
	\item \textbf{Hidden state}:
	\begin{align}
		s_t &= (1 - z_t) \circ s_{t - 1} + z_t \circ \tilde s_t
	\end{align}
	
\end{compactenum}

\myspace 
\p \blue{Alignment Model}. All equations enumerated below are for some timestep $t$ during the decoding process.
\begin{compactenum}
	\item \textbf{Score}: For all $j \in [0, L_{enc}-1]$ where $L_{enc}$ is the number of words in the encoder sequence, compute:
	\begin{align}
		a_j = a(s_{t - 1}, h_j) &= v_a^T \tanh\left( W_a s_{t - 1} + U_a h_j \right)
	\end{align}
	
	\item \textbf{Alignments}: Feed the unnormalized alignments (scores) through a softmax so they represent a valid probability distribution.
	\begin{align}
		a_j \leftarrow \frac{e^{a_j}}{\sum_{k = 0}^{L_{enc}-1} e^{a_k}}
	\end{align}
	
	\item \textbf{Context}: The context vector input for our decoder at this timestep:
	\begin{align}
		c = \sum_{j = 1}^{L_{enc}} a_j h_j
	\end{align}
\end{compactenum}

\myspace
\p \blue{Decoder Outputs}. All below is for some timestep $t$ during the decoding process. To find the probability of some (one-hot) word $y$ [at timestep $t$]:
\begin{align}
	\Pr\left(y \mid s, c \right) &\propto e^{y^T W_o u} \\
	u &= \left[\max\{ \tilde u_{2j - 1}, \tilde u_{2j} \}\right]_{j = 1,\ldots, \ell}^T \\
	\tilde u &= U_o [s_{t - 1}, ~ c] + V_o y_{t - 1}
\end{align}
\red{N.B.}: From reading other (and more recent) papers, these last few equations do not appear to be the way it is usually done (thank the lord). See Luong's work for a much better approach.

% ==============================================================================
% ==============================================================================
\lecture{Papers and Tutorials}{Effective Approaches to Attention-Based NMT}{May 11}
% ==============================================================================
% ==============================================================================

\href{https://arxiv.org/abs/1508.04025}{[Luong et. al, 2015]}

\p \blue{Attention-Based Models}. For attention especially, the devil is in the details, so I'm going to go into somewhat excruciating detail here to ensure no ambiguities remain. For both global and local attention, the following information holds true:
\begin{compactitem}
	\item ``At each time step t in the decoding phase, both approaches first take as input the hidden state $\vec[t]{h}$ at the top layer of a stacking LSTM.'' 
	\item Then, they derive [with different methods] a context vector $\vec[t]{c}$ to capture source-side info.
	\item Given $\vec[t]{h}$ and $\vec[t]{c}$, they both compute the \green{attentional hidden state} as:
	\graybox{
		\vec[t]{\tilde h} &= \tanh\left(  \matr[c]{W} [\vec[t]{c};~ \vec[t]{h}]   \right)
		}
		
	\item Finally, the predictive distribution (decoder outputs) is given by feeding this through a softmax:
	\begin{align}
		p(y_t \mid y_{<t}, x) &= \text{softmax}\left(  \matr[s]{W} \vec[t]{\tilde h}   \right)
	\end{align}
\end{compactitem}

\myspace
\p \blue{Global Attention}. Now I'll describe in detail the processes involved in $\vec[t]{h} \rightarrow  \vec[t]{a} \rightarrow \vec[t]{c} \rightarrow   \vec[t]{\tilde h}$.
\begin{compactenum}
	\item $\vec[t]{h}$: Compute the hidden state $\vec[t]{h}$ in the normal way (not obvious if you've read e.g. Bahdanau's work...)
	
	\item $\vec[t]{a}$: 
	\begin{compactenum}
		\item Compute the \green{scores} between $\vec[t]{h}$ and each source $\vec[s]{\bar h}$, where our options are:
		\graybox{
			\text{score}(\vec[t]{h}, \vec[s]{\bar h}) 
			&= \begin{cases}
				\vec[t]{h}^T \vec[s]{\bar h} & \mgreen{dot} \\
				\vec[t]{h}^T \matr[a]{W} \vec[s]{\bar h} & \mgreen{general} \\
				\vec[a]{v}^T \tanh\left(\matr[a]{W} [\vec[t]{h}; ~ \vec[s]{\bar h}] \right) & \mgreen{concat} 
			\end{cases}
			}
			
		\item Compute the \green{alignment vector} $\vec[t]{a}$ of length $L_{enc}$ (number of words in the encoder sequence):
		\begin{align}
			\vec[t]{a}(s) &= \text{align}(\vec[t]{h}, \vec[s]{\bar h}) \\
			&= \frac{\exp(\text{score}(\vec[t]{h}, \vec[s]{\bar h}) )}{
				\sum_{s'} \exp(\text{score}(\vec[t]{h}, \vec[s']{\bar h}) )
				}
		\end{align}
	\end{compactenum}
	
	\item $\vec[t]{c}$: The weighted average over all source (encoder) hidden states\footnote{NOTE: Right after mentioning the context vector, the authors have the following cryptic footnote that may be useful to ponder: \textit{For short sentences, we only use the top part of
		$a_t$ and for long sentences, we ignore words near the end}.}:
	\begin{align}
		\vec[t]{c} &= \sum_{i = 1}^{L_{enc}} \vec[t]{a}(i) \vec[i]{\bar h}
	\end{align}
	
	\item $\vec[t]{\tilde h}$: For convenience, I'll copy the equation for $\vec[t]{\tilde h}$ again here:
	\begin{align}
		\vec[t]{\tilde h} &= \tanh\left(  \matr[c]{W} [\vec[t]{c};~ \vec[t]{h}]   \right)
	\end{align}
\end{compactenum}

\myspace
\p \blue{Input-Feeding Approach}. A copy of each output $\vec[t]{\tilde h}$ is sent forward and concatenated with the inputs for the next timestep, i.e. the inputs go from $\vec[t+1]{x}$ to $[\vec[t]{\tilde h}; \vec[t+1]{x}]$. 


% ==============================================================================
% ==============================================================================
 \lecture{Papers and Tutorials}{Using Large Vocabularies for NMT}{March 11}
% ==============================================================================
% ==============================================================================

\p Paper information:
\begin{compactitem}[-]
	\item Full title: On Using Very Large Target Vocabulary for Neural Machine Translation. 
	\item Authors: Jean, Cho, Memisevic, Bengio.
	\item Date: 18 Mar 2015. 
	\item \href{https://arxiv.org/abs/1412.2007}{[arXiv link]}
\end{compactitem}


\myspace
\p \blue{NMT Overview}. Typical implementation is encoder-decoder network. Notation for inputs \& encoder:
\begin{align}
x &= (x_1, \ldots, x_T)
\qquad \mgreen{\text{[source sentence]}} \\
h &= (h_1, \ldots, h_T)
\qquad \mgreen{\text{[encoder state seq]}} \\
h_t &= f(x_t, h_{t - 1}) \label{h}
\end{align}
where $f$ is the function defined by the \textit{cell state} (e.g. GRU/LSTM/etc.). Then the decoder generates the output sequence $y$, and with probability given below:\marginnote{The functions $q$, $g$, and $r$ are just placeholders -- ``some function of [inputs].''}[2em]
\begin{align}
y &= (y_1, \ldots, y_T') \qquad \mgreen{[y_i \in \mathbb{Z}]} \\
\Pr[y_t \mid y_{<t}, ~ x] &\propto e^{q(y_{t-1},~ z_t, ~ c_t)}  \label{y}\\
z_t &= g(y_{t - 1}, z_{t - 1}, c_t) 
\qquad \mred{\text{[decoder hidden?]}} \label{z} \\
c_t &= r(z_{t - 1}, h_1, \ldots, h_T) 
\qquad \mred{\text{[decoder inp?]}} \label{c}
\end{align}
As usual, model is jointly trained to maximize the conditional log-likelihood of correct translation. For $N$ training sample pairs $(x^n, y^n)$, and denoting the length of the $n$-th target sentence as $T_n$, this can be written as,
\begin{align}
\theta^* &= \argmax_\theta \sum_{n = 1}^{N} \sum_{t = 1}^{T_n}
\log\left(\Pr[y_t^n \mid y_{<t}^n, ~ x^n]  \right)
\end{align}

\myspace
\p \blue{Model Details}. Above is the general structure. Here I'll summarize the specific model chosen by the authors. 
\begin{compactitem}
	\item \textbf{Encoder}. Bi-directional, which just means $h_t = \left[h_t^{backward};~h_t^{forward} \right]$. The chosen cell state (the function $f$) is GRU.
	
	\item \textbf{Decoder}. At each timestep, computes the following:
	\begin{compactitem}[$\rightarrow$]
		\item \green{Context vector $c_t$}. \marginnote{$a$ is a standard single-hidden-layer NN.}[7em]
		\begin{align}
		c_t &= \sum_{i = 1}^{T} \alpha_i h_i \\
		\alpha_t &= \dfrac{e^{a(h_t, z_{t - 1})}}{\sum_k e^{a(h_k, z_{t - 1}})} 
		\end{align}
		\item \green{Decoder hidden state $z_t$}. Also a GRU cell. Computed based on the previous hidden state $z_{t - 1}$, the previously generated symbol $y_{t - 1}$, and also the computed context vector $c_t$. 
	\end{compactitem}
	
	\item \textbf{Next-word probability}. They model equation ~\ref{y} as\footnote{Note: The formula for $Z$ is correct. Notice that the only part of the RHS of Pr($y_t$) with a $t$ is as the subscript of $w$. To be clear, $w_k$ is a full word vector and the sum is over all words in the output \textit{vocabulary}, the index $k$ has absolutely nothing to do with timestep. They use the word target but make sure not to misinterpret that as somehow meaning target words in the sentence or something.} ,\marginnote{Reminder: $y_i$ is an integer token, while $\rvec[i]{w}$ is the target vector of length vocab size}
	\begin{align}
	\Pr[y_t \mid y_{<t}, ~ x]  &= \frac{1}{Z} e^{\rvec[t]{w}^T \phi(y_{t - 1}, z_t, c_t) + b_t} \\
	Z &= \sum_{k:~y_k\in V}    e^{\rvec[k]{w}^T \phi(y_{t - 1}, z_t, c_t) + b_k}
	\end{align}
	where $\phi$ is affine transformation followed by a nonlinear activation, $\rvec[t]{w}$ and $b_t$ are the \green{target word vector} and bias. $V$ is the set of all target \textit{vocabulary}. 
\end{compactitem}

\myspace
\p \blue{Approximate Learning Approach}. Main idea:
\vspace{-1em}
\begin{center}
	\begin{quote}
		{\footnotesize ``In order to avoid the growing complexity of computing the normalization constant, we propose here to use only a small subset $V'$ of the target vocabulary at each update.''}
	\end{quote}
\end{center}
Consider the gradient of the log-likelihood\footnote{\red{NOTE TO SELF}: After long and careful consideration, I'm concluding that the authors made a typo when defining $\mathcal{E}(y_j)$, which they choose to subscript all parts of the RHS with $j$, but that is in direct contradiction with a step-by-step derivation, which is why I have written it the way it is. I'm pretty sure my version is right, but I know you'll have to re-derive it yourself next time you see this. And you'll somehow prove me wrong. Actually, after reading on further, I doubt you'll prove me wrong. Challenge accepted, me. Have fun!}, written in terms of the energy $\mathcal{E}$. 
\begin{align}
\nabla \log\left(\Pr[y_t \mid y_{<t}, ~ x]  \right)
&= \nabla \mathcal{E}(y_t) 
- \sum_{k:~y_k\in V}\Pr[y_k \mid y_{<t}, ~ x]\nabla\mathcal{E}(y_k)\\
\mathcal{E}(y_j) &= \rvec[j]{w}^T \phi(y_{t - 1}, z_t, c_t) + b_j
\end{align}


\myspace
\p The crux of the approach is interpreting the second term as $\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right]$, where $P$ denotes $Pr(y \mid y_{<t}, x)$. They approximate this expectation by taking it over a \underline{subset} $V'$ of the predefined proposal \underline{distribution} $Q$. So $Q$ is a p.d.f. over the possible $y_i$, and we sample \textit{from} $Q$ to generate the elements of the subset $V'$. 
\graybox{
	\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right] 
	&\approx \sum_{k:~y_k\in V'} \dfrac{\omega_k}{
		\sum_{k':y_{k'}\in V'} \omega_{k'} } \nabla\mathcal{E}(y_k)\\
	\omega_k &= e^{\mathcal{E}(y_k) - \log Q(y_k)}
}

Here is some math I did that was illuminating to me; I'm not sure why the authors didn't point out these relationships. 
\begin{align}
&\omega_k = \frac{e^{\mathcal{E}(y_k)}}{Q(y_k)} 
\quad\text{thus}\quad
p(y_k \mid y_{<t}, x) = \omega_k \frac{Q(y_k)}{Z} \\
\rightarrow~~  &e^{\mathcal{E}(y_k)} = Z \cdot p(y_k \mid y_{<t}, x) 
= Q(y_k) \cdot \omega_k
\end{align}

\redbox[Now \textit{check this out}]{
	Below are the exact and approximate formulas for $\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right]$ written in a \sout{seductive} suggestive manner. Pay careful attention to subscripts and primes.
	\begin{align}
	\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right] 
	&=  \sum_{k:~y_k\in V} \dfrac{ \omega_k \cdot Q(y_k) }{ 
		\sum_{k':y_{k'}\in V} \omega_{k'} \cdot Q(y_{k'}) }  
	\nabla\mathcal{E}(y_k)\\
	\mathbb{E}_P\left[\nabla \mathcal{E}(y) \right] 
	&=  \sum_{k:~y_k\in V'} \dfrac{ \omega_k }{  \sum_{k':y_{k'}\in V'} \omega_{k'} }  \nabla\mathcal{E}(y_k)
	\end{align}
	They're almost the same! It's much easier to see why when written this way. I interpret the difference as follows: in the exact case, we explicitly attach the probabilities $Q(y_k)$ and sum over all values in $V$. In the second case, by sampling a subset $V'$ from $Q$, we have encoded these probabilities implicitly as the relative frequency of elements $y_k$ in $V'$
}

\myspace
\p \blue{How to do in practice (very important)}. 
\begin{center}
	\begin{quote}
		``In practice, we partition the training corpus and define a subset $V'$ of the target vocabulary for each partition prior to training. Before training begins, we sequentially examine each target sentence in the training corpus and accumulate
		unique target words until the number of unique target words reaches the predefined threshold $\tau$. The accumulated vocabulary will be used for this partition of the corpus during training. We repeat this until the end of the training set is reached. Let us refer to the subset of target words used for the i-th partition by $V'_i$.
	\end{quote}
\end{center}




% ============================================================================================
% ============================================================================================
\lecture{Papers and Tutorials}{Candidate Sampling -- TensorFlow}{March 19}
% ============================================================================================
% ============================================================================================

{\scriptsize \href{https://www.tensorflow.org/extras/candidate_sampling.pdf}{[Link to article]}}

\myspace
\blue{What is Candidate Sampling} The goal is to learn a compatibility function $F(x, y)$ which says something about the compatibility of a class $y$ with a context $x$. Candidate sampling: for each training example $(x_i, y_i)$, only need to evaluate $F(x, y)$ for a small set of classes $\{C_i\} \subset \{L\}$, where $\{L\}$ is the set of all possible classes (vocab size number of elements). We represent $F(x, y)$ as a \textit{layer that is trained by back-prop from/within the loss function}. 

\myspace
\p \blue{C.S. for Sampled Softmax}. I'll further narrow this down to my use case of having exactly 1 target class (word) at a given time. Any other classes are referred to as \green{negative} classes (for that example). 

\myspace
\p \blue{Sampling algorithm}. For each training example $(x_i, y_i)$, do:
\begin{compactitem}
	\item Sample the subset $S_i \subset L$. How? By sampling from $Q(y|x)$ which gives the probability of any particular $y$ being included in $S_i$. 
	
	\item Create the set of \green{candidates}, which is just $C_i := S_i \cup y_i$. 
\end{compactitem}

\myspace
\p \blue{Training task}. We are given this set $C_i$ and want to find out which element of $C_i$ is the target class $y_i$. In other words, we want the posterior probability that any of the $y$ in $C_i$ are the target class, given what we know about $C_i$ and $x_i$. We can evaluate and rearrange as usual with Bayes' rule to get:
\graybox{
	\Pr\left(y^{true}_i = y \mid C_i, ~ x_i \right) 
	&= 
		\dfrac{\Pr\left(y^{true}_i = y \mid x_i \right) 
			\cdot \Pr\left( C_i \mid y^{true}_i = y, ~ x_i \right)
			}{
			\Pr\left(C_i \mid x_i\right) 
		} \\
	%
	&= 
		\frac{\Pr\left(y\mid x_i\right)}{Q\left(y\mid x_i\right)} 
		\cdot 
		\frac{1}{K(x_i, C_i)}  \label{candform}
}
where they've just defined 
\begin{align}
	K(x_i, C_i) \triangleq \frac{ \Pr\left(C_i \mid x_i \right) }{
		\prod_{y'\in C_i} Q(y' \mid x_i)
		\prod_{y'\in(L - C_i)} \left( 1 - Q(y' \mid x_i)\right)
		}
\end{align}

\myspace
\p \blue{Clarifications}. 
\begin{itemize}
	\item The learning function $F(x, y)$ is the \textit{input} to our softmax. \underline{It is our neural network}, excluding the softmax function. 
	
	\item After training our network, it should have learned the general form
	\begin{align}
		F(x, y) = \log( \Pr( y \mid x) )  + 
		K(x) \label{Fxy}
	\end{align}
	which is the general form because
	\begin{align}
		\text{Softmax}(\log(\Pr(y \mid x)) + K(x)) 
		&= 
		\frac{ e^{\log(\Pr(y \mid x)) + K(x)}   }{
			\sum_{y'} e^{\log(\Pr(y' \mid x) + K(x) }} \\
		&= \Pr(y \mid x)	
	\end{align}
	Note that I've been a little sloppy here, since $\Pr(y \mid x)$ up until the last line actually represented the (possibly) unnormalized/\textit{relative} probabilities. 
	
	\item {\footnotesize \red{[MAIN TAKEAWAY]}}. Time to bring it all together. Notice that we've only trained $F(x, y)$ to include \textit{part} of what's needed to compute the probability of any $y$ being the target given $x_i$ \underline{and $C_i$} \textellipsis equation ~\ref{Fxy} doesn't take into account $C_i$ at all! Luckily we know the form of the full equation because it just the log of equation ~\ref{candform}. We can easily satisfy that by subtracting $\log(Q(y\mid x))$ from $F(x, y)$ right before feeding into the softmax. \\
	
	\p\textbf{TL;DR}. Train network to learn $F(x, y)$ before softmax, but instead of feeding $F(x, y)$ to softmax directly, feed 
	\graybox{
		\text{Softmax Input: } F(x, y) - \log(Q(y\mid x))
		}
	instead. That's it.
\end{itemize}




% ==========================================================================
\lecture{Papers and Tutorials}{Attention Terminology}{April 04}
%===========================================================================

Generally useful info. Seems like there are a few notations floating around, and here I'll attempt to set the record straight. The order of notes here will loosely correspond with the order that they're encountered going from encoder output to decoder output.\\

\myspace
\p \blue{Jargon}. The people in the attention business \textit{love} obscure names for things that don't need names at all. Terminology:
\begin{compactitem}
	\item \textbf{Attentions keys/values}: Encoder output sequence.
	\item \textbf{Query}: Decoder [cell] state. Typically the most recent one.
	\item \textbf{Scores}: Values of $e_{ij}$. For the Bahdanau version, in code this would be computed via
	\begin{align}
		e_i &= v^T \tanh(\text{FC}(s_{i - 1}) + \text{FC}(h))
	\end{align}
	where we'd have FC be tf.layers.fully\_connected with num\_outputs equal to our attention size (up to us). Note that $v$ is a vector.
	\item \textbf{Alignments}: output of the softmax layer on the attention scores. 
	\item \textbf{Memory}: The $\alpha$ matrix in the equation $c_i = \sum_{j = 1}^{T_x} \alpha_{ij} h_j$.
\end{compactitem}

\myspace
\p When someone lazily calls some layer output the ``attention'', they are usually referring to the layer \textit{just after} the linear combination/map of encoder hidden states. You'll often see this as some vague function of the previous decoder state, context vector, and possibly even decoder output (after project), like $f(s_{i - 1}, y_{i - 1}, c_i)$. In 99.9\% of cases, this function is just a fully connected layer (if even needed) to map back to the state size for decoder input. That is it.


\p \blue{From encoder to decoder}. The path of information flow from encoder outputs to decoder inputs, a non-trivial process that isn't given the \textit{attention} (heh) it deserves\footnote{For some reason, the literature favors explaining the path ``backwards'', starting with the highly abstracted ``decoder inputs as a weighted sum of encoder states'' and then breaking down what the weights are. Unfortunately, the weights are computed via a multi-stage process so that becomes very confusing very quick.}
\begin{enumerate}
	\item \textbf{Encoder outputs}. Tensor of shape \purple{[batch size, sequence length, state size]}. The state is typically some RNNCell state.
	\begin{compactitem}
		\item Note: TensorFlow's AttenntionMechanism classes will actually convert this to \purple{[batch size, $L_{enc}$, attention size]}, and refer to it as the ``memory''. It is also what is returned when calling myAttentionMech.values. 
	\end{compactitem}
	
	
	\item \textbf{Compute the scores}. The attention scores are the computation described by Luong/Bahdanau techniques. They both take an inner product of sorts on \textit{copies} of the encoder outputs and decoder previous state (query). The main choices are:
	\begin{align}
				\text{score}(\vec[t]{h}, \vec[s]{\bar h}) 
				&= \begin{cases}
				\vec[t]{h}^T \vec[s]{\bar h} & \mgreen{dot} \\
				\vec[t]{h}^T \matr[a]{W} \vec[s]{\bar h} & \mgreen{general} \\
				\vec[a]{v}^T \tanh\left(\matr[a]{W} [\vec[t]{h}; ~ \vec[s]{\bar h}] \right) & \mgreen{concat} 
				\end{cases}
	\end{align}
	where the shapes are as follows (for single timestep during decoding process):\marginnote{\green{Synonyms:\\ - scores \\
			 - unnormalized alignments}}[-5em]
	\begin{compactitem}
		\item $\vec[s]{\bar h}$: \purple{[batch size, 1, state size]}
		\item $\vec[t]{h}$: \purple{[batch size, 1, state size]}
		\item $\matr[a]{W}$: \purple{[batch size, state size, state size]}
		\item $\text{score}(\vec[t]{h}, \vec[s]{\bar h})$: \purple{[batch size]}
	\end{compactitem}
		
	\item \textbf{Softmax the scores}. In the vast majority of cases, the attention scores are next fed through a softmax to convert them into a valid probability distribution. Most papers will call this some vague probability function, when in reality they are using softmaxonly.\marginnote{ \green{Synonyms:\\
				- softmax outputs \\
				- attention dist. \\
				- alignments}}[-3.5em]
	\begin{align}
	\vec[t]{a}(s) &= \text{align}(\vec[t]{h}, \vec[s]{\bar h}) \\
	&= \frac{\exp(\text{score}(\vec[t]{h}, \vec[s]{\bar h}) )}{
		\sum_{s'} \exp(\text{score}(\vec[t]{h}, \vec[s']{\bar h}) )
	}
	\end{align}
	where the alignment vector $\vec[t]{a}$ has shape \purple{[batch size, $L_{enc}$]}
		
	\item \textbf{Compute the context vector}. The inner product of the softmax outputs and the raw encoder outputs.\marginnote{\green{Synonyms:\\ 
			- context vector\\
			- attention}}[-1em]
	This will have shape \purple{[batch size, attention size]} in TensorFlow, where attention size is from the constructor for your AttentionMechanism.
	
	\item \textbf{Combine context vector and decoder output}: Typically with a concat. \textit{The result is what people mean when they say ``attention''}. Luong et al. denotes this as $\vec[t]{\tilde h}$, the decoder output at timestep $t$. This is what TensorFlow means by ``Luong-style mechanisms output the attention.'' And yes, these are used (at least for Luong) to compute the prediction:
	\graybox{
		\vec[t]{\tilde h} &= \tanh\left( \matr[c]{W}\left[
			\vec[t]{c}, ~ \vec[t]{h}
		 \right]   \right) \\
		 p( y_t \mid y_{<t}, x  ) &= \text{softmax}(
			 \matr[s]{W}  \vec[t]{\tilde h}
		 )
		}
\end{enumerate}










% ============================================================================================
% ============================================================================================
\lecture{Papers and Tutorials}{TextRank}{May 03}
% ============================================================================================
% ============================================================================================

\myspace
\p \blue{Introduction}. A graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.\marginnote{\textbf{Semantic graph}: one whose structure encodes meaning between the nodes (semantic elements).}[-2em] \green{TextRank} is a graph-based ranking model for graphs extracted from natural language texts. The authors investigate/evaluate TextRank on unsupervised keyword and sentence extraction.

\myspace
\p \blue{The \sout{TextRank} PageRank Model}. In general [graph-based ranking], a vertex can be ranked based on certain properties such as the number of vertices pointing to it (in-degree), how highly-ranked \textit{those} vertices are, etc. Formally, the authors [of \textit{PageRank}] define the score of a vertex $V_i$ as follows:\marginnote{The factor $d$ is usually set to 0.85.}[2em]

\graybox{
	S(V_i) &= (1 - d) + d * \sum_{V_j \in \textrm{In}(V_i)} \frac{1}{\mid \textrm{Out}(V_i) \mid} S(V_j) 
	\quad \text{where} \quad 
	d \in \Re[0, 1]} \label{textrank-formula}
and the damping factor $d$ is interpreted as the probability of jumping from a given vertex\footnote{Note that $d$ is a single parameter for the graph, i.e. it is the same for all vertices.} to another \underline{random} vertex in the graph. In practice, the algorithm is implemented through the following steps:
\begin{compactitem}
	\item[(1)] Initialize all vertices with arbitrary values.\footnote{The authors do not specify what they mean by arbitrary. What range? What sampling distribution? Arbitrary as in uniformly random? \red{EDIT}: The authors claim that the vertex values upon completion are not affected by the choice of initial value. Investigate!}
	
	\item[(2)] Iterate over vertices, computing equation~\ref{textrank-formula} until convergence [of the error rate] below a predefined threshold. The error-rate, defined as the difference between the "true score" and the score computed at iteration $k$, $S^k(V_i)$, is \textit{approximated} as:
	\begin{align}
		\textrm{Error}^k(V_i) \approx S^{k}(V_i) - S^{k - 1}(V_i)
	\end{align}
\end{compactitem}

\myspace 
\p \blue{Weighted Graphs}. In contrast with the PageRank model, here we are concerned with natural language texts, which may include multiple or partial links between the units (vertices). The authors hypothesize that modifying equation~\ref{textrank-formula} to incorporate \textit{weighted} connections may be useful for NLP applications.\marginnote{$w_{ij}$ denotes the connection between vertices $V_i$ and $V_j$.}[2em]

\graybox{
	\textcolor{OliveGreen}{W}S(V_i) &= (1 - d) + d * \sum_{j \in \textrm{In}(V_i)}
		\textcolor{OliveGreen}{
			 \frac{w_{ji}}{ \sum{V_k \in \textrm{Out}(V_j)} w_{jk} }
			W }
	S(V_j) \label{textrank-formula-2}
}
where I've shown the modified part in green. The authors mention they set all weights to random values in the interval 0-10 (no explanation). 

\myspace
\p \blue{Text as a Graph}. In general, the application of graph-based ranking algorithms to natural language texts consists of the following main steps:
\begin{compactitem}
	\item[(1)] Identify text units that best define the task at hand, and add them as vertices in the graph.
	
	\item[(2)] Identify relations that connect such text unit in order to draw edges between vertices in the graph. Edges can be directed or undirected, weighted or unweighted.
	
	\item[(3)] Iterate the algorithm until convergence.\vspace{0.3em}
	
	\item[(4)] Sort [in reversed order] vertices based on final score. Use the values attached to each vertex for ranking/selection decisions.
\end{compactitem}

\myspace\Needspace{30\baselineskip}
\subsub{Keyword Extraction}
\myspace

\p \blue{Graph}. The authors apply TextRank to extract words/phrases that are representative for a given document. The individual graph components are defined as follows:
\begin{compactitem}[-]
	\item \green{Vertex}: sequence of one or more lexical units from the text.
	\begin{compactitem}
		\item In addition, we can restrict which vertices are added to the graph with \underline{syntactic filters}.
		\item Best filter [for the authors]: \textit{nouns and adjectives only}.
	\end{compactitem}
	\item \green{Edge}: two vertices are connected if their corresponding lexical units co-occur within a window of $N$ words\marginnote{Typically $N \in \Z[2, 10]$}[-1em]\footnote{That is \textellipsis  simpler than expected. \textit{Can we do better?}}.
\end{compactitem}

\myspace\Needspace{15\baselineskip}
\p \blue{Procedure}:
\begin{compactitem}
	\item[(1)] \textbf{Pre-Processing}: Tokenize and annotate [with POS] the text.\vspace{0.3em}
	
	\item[(2)] \textbf{Build the Graph}: Add all [single] words to the graph that pass the syntactic filter, and connect [undirected/unweighted] edges as defined earlier (co-occurrence).
	
	\item[(3)] \textbf{Run algorithm}: Initialize all scores to 1. For a convergence threshold of 0.0001, usually takes about 20-30 iterations.
	
	\item[(4)] \textbf{Post-Processing}: 
	\begin{compactitem}
		\item[(i)] Keep the top $T$ vertices (by score), where the authors chose $T = |V|/3$.\footnote{Another approach is to have $T$ be a fixed value, where typically $5 < T < 20$.} Remember that vertices are still individual words.
		
		\item[(ii)] From the new subset of $T$ keywords, collapse any that were adjacent in the original text in a single lexical unit.
	\end{compactitem}
\end{compactitem}

\myspace
\p \blue{Evaluation}. The data set used is a collection of 500 abstracts, each with a set of keywords. Results are evaluated using \green{precision}, \green{recall}, and \green{F-measure}\footnote{
	Brief terminology review:
	\begin{compactitem}
		\item \green{Precision}: fraction of keywords extracted that are in the "true" set of keywords.
		\item \green{Recall}: fraction of "true" keywords that are in the extracted set of keywords.\marginnote{A PR Curve plots precision as a function of recall.}[-2em]
		\item \green{F-score}: combining precision and recall to get a single number for evaluation:
		$$
		F = \frac{2pr}{p + r}
		$$
	\end{compactitem}
	}. % end footnote
The best results were obtained with a co-occurrence window of 2 [on an undirected graph], which yielded:
$$
\text{Precision: } 31.2\% \quad \text{Recall: } 43.1\% \quad \text{F-measure: } 36.2
$$
The authors found that larger window size corresponded with lower precision, and that directed graphs performed worse than undirected graphs.

\myspace
\subsub{Sentence Extraction} 
\myspace 

\p \blue{Graph}. Now we move to ``sentence extraction for automatic summarization.''
\begin{compactitem}
	\item \green{Vertex}: a vertex is added to the graph for each sentence in the text.
	\item \green{Edge}: each weighted edge represents the similarity between two sentences. The authors use the following similarity measure between two sentences $S_i$ and $S_j$:
	\graybox{
		\textrm{Similarity}(S_i, S_j) &= 
			\frac{\mid S_i \cap S_j \mid}{\log(\mid S_i \mid) + \log(\mid S_j \mid)}
		}
		where the numerator is the number of words that occur in both $S_i$ and $S_j$. 
\end{compactitem}
The \blue{procedure} is identical to the algorithm described for keyword extraction, except we run it on full sentences.

\myspace
\p \blue{Evaluation}. The data set used is 567 news articles. For each article, TextRank generates a 100-word summary (i.e. they set $T = 100$). They evaluate with the \href{http://www.isi.edu/licensed-sw/see/rouge/}{\textsc{Rouge}} evaluation toolkit (Ngram statistics). 


\begin{comment}
\myspace\myspace\Needspace{25\baselineskip}
\p \blue{Thoughts and Email Questions}.
\begin{itemize}
	\item How would you evaluate how ``good'' an implementation is (e.g. TextRank project)?
	\begin{compactitem}
		\item TextRank is pretty simple and mechanical -- write the algorithm and you are done.
		\item With that said, finding good values for the parameters like co-occurrence window N and damping factor $d$ for the data you're interested in.
		\item If you branch off a bit from the authors, e.g. the github project uses Levenshtein Distance instead of co-occurrence, which ``feels'' like it should have more generalize power but of course needs testing/comparisons.
		\item Ultimately, a project is good if it serves your purposes well -- a master generalization model could be just as good as logistic regression depending on the setting.
	\end{compactitem}
	
	\item What's appropriate for Agora to implement in the short-term?
	\begin{compactitem}
		\item From what I understand, Agora aims to 
		\begin{enumerate}
			\item Synthesize what communities have tried to solve certain issues.
			\item Have some metric for the efficacy of these attempts.
			\item Provide a service where users can easily get guidance on their approaches/community issues.
		\end{enumerate}
		
		\item Regardless of architecture/model choices, a neural network would only be appropriate here if:
		\begin{enumerate}
			\item Data. You need a substantial amount of data, preferably as close to your domain as possible, before it becomes ready for consumers. 
			\item \textit{Clean} data. How are you extracting what has been said? What is the format of your database? SQL/NoSQL? These will determine the most straightforward options.
			\item I tried out the slack app briefly: the agora commands are great for persona-based conversation models. Here, we can use what are known as distributed embeddings to ``grab'' a personality, e.g. agora-help would run the model on the ``help'' embeddings (this would be so fun!!)
		\end{enumerate}
		
		\item Short-term, it would be helpful to know, in the sense of curation paper, what makes a good conversation model on your platform? What qualities are you trying to optimize for? A model as simple as multiclass logistic regression or decision trees may do just fine for a starter model. 
		
		\item Collect data for everything you possibly can! As fine-grained as how long a user spends with the bot, the distribution of commands from user to bot, etc. If a neural net is used, save checkpoints so that transfer learning is possible.
		
		\item \green{Question}: What happens when a user issues an Agora command?
	\end{compactitem}
\end{itemize}
\end{comment}


% ============================================================================================
\lecture{Papers and Tutorials}{Simple Baseline for Sentence Embeddings}{June 12, 2017}
% ============================================================================================


\myspace
\p \blue{Overview}. It turns out that simply taking a weighted average of word vectors and doing some PCA/SVD is a competitive way of getting unsupervised word embeddings. Apparently it \textit{beats} supervised learning with LSTMs (?!). The authors claim the theoretical explanation for this method lies in a latent variable generative model for sentences (of course).\marginnote{Discussion based on paper by Arora et al., (2017).}[-2em]

\myspace
\p \blue{Algorithm}.
\begin{compactenum}
	\item Compute the weighted average of the word vectors in the sentence:\marginnote{The authors call their weighted average the \green{Smooth Inverse Frequency (SIF)}.}[2em]
	\begin{align}
	\frac{1}{N} \sum_{i}^{N} \frac{a}{a + p(\vec[i]{w})} \vec[i]{w}
	\end{align}
	where $\vec[i]{w}$ is the word vector for the $i$th word in the sentence, $a$ is a parameter, and $p(\vec[i]{w})$ is the (estimated) word frequency [over the entire corpus]. 
	
	\item Remove the projections of the average vectors on their first principal component (``common component removal'') (y tho?).
\end{compactenum}

\myfig[0.8\textwidth]{AroraEmbeddingAlg.png}

\myspace
\p \blue{Theory}. Latent variable generative model. The model treats corpus generation as a dynamic process, where the $t$-th word is produced at time step $t$, driven by the random walk of a \green{discourse vector} $c_t \in \Re^d$ ($d$ is size of the embedding dimension). The discourse vector is \textit{not} pointing to a specific word; rather, it describes what is being talked about. We can tell how related (correlation) the discourse is to any word $w$ and corresponding vector $v_w$ by taking the inner product $c_t \cdot v_w$. Similarly, we model the probability of observing word $w$ at time $t$, $w_t$, as:
\graybox{ \Prob{w_t \mid c_t} \propto e^{c_t \cdot v_w} \label{naive-embed} }
\begin{compactitem}
	\item \textbf{The Random Walk}. If we assume that $c_t$ doesn't change much over the words in a \underline{single sentence}, we can assume it stays at some $c_s$. The authors claim that in their previous paper they showed that the MAP\footnote{Review of MAP: $$ \theta_{MAP} = \argmax_\theta \sum_i \log\left(p_X(x \mid \theta) p(\theta) \right)$$ } estimate of $c_s$ is -- up to multiplication by a scalar -- the average of the embeddings of the words in the sentence.
	
	\item \textbf{Improvements/Modifications to~\ref{naive-embed}}. 
	\begin{compactenum}
		\item Additive term $\alpha p(w)$ where $\alpha$ is a scalar. Allows words to occur even if $c_t \cdot v_w$ is very small.
		\item Common discourse vector $c_0 \in \Re^d$. Correction term for the most frequent discourse that is often related to syntax.
	\end{compactenum}
	
	\item \textbf{Model}. Given the discourse vector $c_s$ for a sentence $s$, the probability that $w$ is in the sentence (at all (?)):
	\graybox{
		\Prob{w \mid c_s} &= \alpha p(w) + (1 - \alpha) \frac{ e^{\tilde c_s \cdot v_w} }{ Z_{\tilde c_s} } \\
		\tilde c_s &= \beta c_0 + (1 - \beta) c_s
	}
	with $c_0 \perp c_s$ and $Z_{\tilde c_s}$ is a normalization constant, taken over all $w \in V$. 
	
\end{compactitem}


% ============================================================================================
\lecture{Papers and Tutorials}{Survey of Text Clustering Algorithms}{July 03, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Aggarwal et al., ``A Survey of Text Clustering Algorithms,'' (2012).}

\p \blue{Introduction}. The unique characteristics for clustering \textit{text}, as opposed to more traditional (numeric) clustering, are (1) large dimensionality but highly sparse data, (2) words are typically highly correlated, meaning the number of principal components is much smaller than the feature space, and (3) the number of words per document can vary, so we must normalize appropriately.\\

\p Common types of clustering algorithms include agglomerative clustering algorithms, partitioning algorithms, and standard parametric modeling based methods such as the EM-algorithm.

\p \blue{Feature Selection}. 
\begin{compactitem}
	\item \green{Document Frequency-Based}. Using document frequency to filter \textit{out} irrelevant features. Dealing with certain words, like ``the'', should probably be taken a step further and simply removed (stop words). 
	
	\item \green{Term Strength}. A more aggressive technique for stop-word removal. It's used to measure how informative a word/term $t$ is for identifying two related documents, $x$ and $y$. Denoted $s(t)$, it is defined as:\marginnote{See ref 94 of the paper for more.}
	\begin{align}
		s(t) &= \Prob{t \in y \mid t \in x}
	\end{align}
	So, how do we know $x$ and $y$ are related to begin with? One way is a user-defined cosine similarity threshold. Say we gather a set of such \textit{pairs} and randomly identify one of the pair as the ``first'' document of the pair, then we can approximate $s(t)$ as
	\begin{align}
		s(t) &= \frac{ \text{Num pairs in which t occurs in both}   }{ \text{Num pairs in which t occurs in the first of the pair}  }
	\end{align}
	
	\begin{quote}
		\textit{{\small In order to prune features, the term strength may be compared to the expected strength of a term which is randomly distributed in the training documents with the same frequency. If the term strength of t is not at least two standard deviations greater than that of the random word, then it is removed from the collection.}}
	\end{quote}
	
	\item \green{Entropy-Based Ranking}. The quality of a term is measured by the entropy reduction when it is removed [from the collection]. The entropy $E(t)$ of term $t$ in a collection of $n$ documents is:
	\begin{align}
		E(t) &= - \insum \jnsum \left( S_{ij} \cdot \log(S_{ij}) 
		+ (1 - S_{ij}) \cdot \log(1 - S_{ij})\right)\\
		S_{ij} &= 2^{-d_{ij} / \bar d }
	\end{align}
	where
	\begin{compactitem}
		\item $S_{ij} \in (0, 1)$ is the similarity between doc $i$ and $j$.
		\item $d_{ij}$ is the distance between $i$ and $j$ after the term $t$ is removed
		\item $\bar d$ is the average distance between the documents after the term $t$ is removed.
	\end{compactitem} 
\end{compactitem}

\myspace 
\p \blue{LSI-based Methods}. Latent Semantic Indexing\marginnote{See ref 28. of paper for more on LSI.} is based on dimensionality reduction where the new (transformed) features are a linear combination of the originals. This helps magnify the semantic effects in the underlying data. LSI is quite similar to PCA\footnote{The difference between LSI and PCA is that PCA subtracts out the means, which destroys the sparseness of the design matrix.}, except that we use an approximation of the covariance matrix $C$ which is appropriate for the sparse and high-dimensional nature of text data.\\

\p Let $\matr{A} \in \R^{n \times d}$ be term-document matrix, where $\matr[i, j]{A}$ is the (normalized) frequency for term $j$ in document $i$. Then $\matr{A}^T \matr{A} = n \cdot \Sigma$ is the (scaled) approximation to covariance matrix\footnote{Approximation because it is based on our training data, not on true expectations over the underlying data-distribution.}, assuming the data is mean-centered. Quick check/reminder:
\begin{align}
	(\matr{A}^T \matr{A})_{ij} &= \matr[:, i]{A}^T \matr[:, j]{A} \triangleq \vec[i]{a}^T \vec[j]{a} \\
	&\approx n \cdot \E{\rvec[i]{a} \rvec[j]{a}}
\end{align}
where the expectation is technically over the underlying data distribution, which gives e.g. $P(a_i = x)$, the probability the $i$th word in our vocabulary having frequency $x$. Apparently, since the data is sparse, we don't have to worry much about it actually being mean-centered (\red{why?}). \\

\p As usual, we using the eigenvectors of $\matr{A}^T \matr{A}$ with the largest variance in order to represent the text\footnote{In typical collections, only about 300 to 400 eigenvectors are required for the representation.}. In addition:
\begin{quote}
	\textit{{\small One excellent characteristic of LSI is that the truncation of the dimensions
			removes the noise effects of synonymy and polysemy, and the similarity computations are more closely affected by the semantic concepts in the data. }}
\end{quote}

\myspace
\p \blue{Non-negative Matrix Factorization}. Another latent-space method (like LSI), but particularly suitable for clustering. The main characteristics of the NMF scheme:
\begin{compactitem}
	\item In LSI, the new basis system consists of a set of orthonormal vectors. This is \textit{not} the case for NMF.
	
	\item In NMF, the vectors in the basis system \textbf{\underline{directly correspond to cluster topics.}} Therefore, the cluster membership for a document may be determined by examining the largest component of the document along any of the [basis] vectors.
\end{compactitem}
\p Assume we want to create $k$ clusters, using our $n$ documents and vocabulary size $d$. The goal of NMF is to find matrices $\matr{U} \in \R^{n \times k}$ and $\matr{V} \in \R^{d \times k}$ that minimize:\marginnote{$$u_{ij} \ge 0$$ $$v_{ij} \ge 0$$}[2em]
\graybox{
	J &= \frac{1}{2} || \matr{A} - \matr{U} \matr{V}^T ||_F^2 \\
	&= \frac{1}{2} \left(
		tr(\matr{A} \matr{A}^T)
		- 2 tr(\matr{A} \matr{V} \matr{U}^T)
		+ tr(\matr{U} \matr{V}^T \matr{V} \matr{U}^T)
	\right)
	}
Note that the columns of $\matr{V}$ provide the $k$ basis vectors which correspond to the $k$ different clusters. We can interpret this as trying to factorize $\matr{A} \approx \matr{U} \matr{V}^T$. For each row, $\vec{a}$, of $\matr{A}$ (document vector), this is 
\begin{align}
	\vec{a} &\approx \vec{u} \cdot \matr{V}^T \\
	&= \sum_{i = 1}^{k} \vec[i]{u} \matr[i]{V}^T
\end{align}
Therefore, the document vector $\vec{a}$ can be rewritten as an approximate linear (non-negative) combination of the basis vector which corresponds to the $k$ columns of $\matr{V}^T$. \\

\p Langrange-multiplier stuff: Our optimization problem can be solved using the Lagrange method.
\begin{compactitem}
	\item Variables to optimize: All elements of both $\matr{U} = [u_{ij}]$ and $\matr{V} = [v_{ij}]$
	\item Constraint: non-negativity, i.e. $\forall i,j$, $u_{ij} \ge 0$ and $v_{ij} \ge 0$. 
	\item Multipliers: Denote as \underline{matrices} $\alpha$ and $\beta$, with same dimensions as $\matr{U}$ and $\matr{V}$, respectively. 
	\item Lagrangian: I'll just show it here first, and then explain in this footnote\footnote{
		 Recall that in Lagrangian minimization, $\mathcal{L}$ takes the form of [the-function-to-be-minimized] + $\lambda$ ([constraint-function] - [expected-value-of-constraint-at-optimum]). So the second term is expected to tend toward zero (i.e. critical point) at the optimal values. In our case, since our optimal value is sort-of (\red{?}) at 0 for any value of $u_{ij}$ and/or $v_{ij}$, we just have a sum over [langrange-mult] $\times$ [variable]. 
		}:\marginnote{Any matrix multiplication with a $\cdot$ is just a reminder to think of the matrices as column vectors.}[1em]
	\begin{align}
		\mathcal{L} &= J + tr(\alpha \cdot \matr{U}^T) + tr(\beta \cdot \matr{V}^T) \\
		\text{where} \quad tr(\alpha \cdot \matr{U}^T) 
			&= \insum \alpha_i \cdot \vec[i]{u}
			= \insum \jnsum \alpha_{ij} u_{ij}
	\end{align}
	You should think of $\alpha$ as a column vector of length $n$, and $\matr{U}^T$ as a row vector of length $n$. The reason we prefer $\mathcal{L}$ over just $J$ is because now we have an \textit{unconstrained} optimization problem.
	
	\item Optimization: Set the partials of $\mathcal{L}$ w.r.t both $U$ and $V$ (separately) to zero\footnote{Recall that the Lagrangian consists entirely of traces (re: scalars). Derivatives of traces with respect to matrices output the same dimension as that matrix, and derivatives are taken element-wise as always.}:
	\begin{align}
		\pderiv{\mathcal{L}}{U} &= - \matr{A} \cdot \matr{V} 
			+ \matr{U} \cdot \matr{V}^T \cdot \matr{V}
			+ \matr{\alpha} = 0 \\
		\pderiv{\mathcal{L}}{V} &= - \matr{A}^T \cdot \matr{U} 
		+ \matr{V} \cdot \matr{U}^T \cdot \matr{U}
		+ \matr{\beta} = 0
	\end{align} 
	Since, ultimately, these just say [some matrix] = 0, we can multiply both sides (element-wise) by a constant ($x \times 0 = 0$). \textit{Using}\footnote{i.e. the equations that follow are \textit{not} the KT conditions, they just \textit{use}/exploit them\textellipsis} the \green{Kuhn-Tucker conditions} $\alpha_{ij} \cdot u_{ij} = 0$ and
	$\beta_{ij} \cdot v_{ij} = 0$, we get:
	\begin{align}
		(\matr{A} \cdot \matr{V})_{ij} \cdot u_{ij}
			- (\matr{U} \cdot \matr{V}^T \cdot \matr{V})_{ij} \cdot u_{ij}
			 &= 0 \\
		(\matr{A}^T \cdot \matr{U} )_{ij} \cdot v_{ij}
			- (\matr{V} \cdot \matr{U}^T \cdot \matr{U})_{ij} \cdot v_{ij}
		&= 0
	\end{align}
	
	\item Update rules:
	\graybox{
		u_{ij} &= \frac{  (\matr{A} \cdot \matr{V})_{ij} \cdot u_{ij}  }{  
			(\matr{U} \cdot \matr{V}^T \cdot \matr{V})_{ij}} \\
		v_{ij} &= \frac{ (\matr{A}^T \cdot \matr{U} )_{ij} \cdot v_{ij}  }{ (\matr{V} \cdot \matr{U}^T \cdot \matr{U})_{ij}   }
		}
\end{compactitem}

\myspace\Needspace{15\baselineskip}
\subsub{Distance-based Clustering Algorithms}
\myspace

\p One challenge in clustering short segments of text (e.g., tweets) is that exact keyword matching may not work well. One general strategy for solving this problem is to expand text representation by exploiting related text documents, which is related to smoothing of a document language model in information retrieval\marginnote{See ref. 66 in the paper for computing similarities of short text segments.}[-2em].

\myspace
\p \blue{Agglomerative and Hierarchical Clustering}. ``Agglomerative'' refers to the process of bottom-up clustering to build a tree -- at the bottom are leaves (documents) and internal nodes correspond to the merged groups of clusters. The different methods for merging groups of documents for the different agglomerative methods are as follows:
\begin{compactitem}
	\item \green{Single Linkage Clustering}. Defines similarity between two groups (clusters) of documents as the largest similarity between any pair of documents from these two groups. First, (1) compute all similarity pairs [between documents; ignore cluster labels], then (2) sort in decreasing order, and (3) walk through the list in that order, merging clusters if the pair belong to different clusters. One drawback is \textit{chaining}: the resulting clusters assume transitivity of similarity\footnote{Here, transitivity of similarity means if $A$ is similar to $B$, and $B$ is similar to $C$, then $A$ is similar to $C$. This is not guaranteed by any means for textual similarity, and so we can end up with $A$ and $Z$ in the same cluster, even though they aren't similar at all. }.
	
	\item \green{Group-Average Linkage Clustering}. Similarity between two clusters is the \textit{average} similarity over all unique pairwise combinations of documents from one cluster to the other. One way to speed up this computation with an approximation is to just compute the similarity between the mean vector of either cluster. 
	
	\item \green{Complete Linkage Clustering}. Similarity between two clusters is the \textit{worst-case} similarity between any pair of documents. 
\end{compactitem}

\myspace
\p \blue{Distance-Based Partitioning Algorithms}. 
\begin{compactitem}
	\item \green{K-Medoid Clustering}. Use a set of points from training data as anchors (medoids) around which the clusters are built. Key idea is we are using an optimal set of representative documents \textit{from the original corpus}. The set of $k$ reps is successively improved via randomized inter-changes. In each iteration, we replace a randomly picked rep in the current set of medoids with a randomly picked rep from the collection, if it improves the clustering objective function. This approach is applied until convergence is achieved.\marginnote{K-Medoid isn't great for clustering text, especially short texts.}[-2em]
	
	\item \green{K-Means Clustering}. Successively (1) assigning points to the nearest cluster centroid and then (2) re-computing the centroid of each cluster. Repeat until convergence. Requires typically few iterations (about 5 for many large data sets). Disadvantage: sensitive to initial set of seeds (initial cluster centroids). One method for improving the initial set of seeds is to use some supervision - e.g. initialize with $k$ pre-defined topic vectors (see ref. 4 in paper for more). 
\end{compactitem}

\myspace
\p \blue{Hybrid Approach: Scatter-Gather Method}. Use a hierarchical clustering
algorithm on a sample of the corpus in order to find a robust initial set of seeds\marginnote{Scatter-Gather is discussed in detail in ref. 25 of the paper}. This robust set of seeds is used in conjunction with a standard k-means clustering algorithm in order to determine good clusters.\red{TODO:} resume note-taking; page 19/52 of PDF.

\myspace
% ====================================
\subsub{Probabilistic Document Clustering and Topic Models}
% ====================================
\myspace

\p \blue{Overview}. Primary assumptions in any topic modeling approach:\marginnote{From pg. 31/52 of paper.}
\begin{compactitem}
	\item The $n$ documents in the corpus are assumed to each have a probability of belonging to one of $k$ topics. Denote the probability of document $D_i$ belonging to topic $T_j$ as $\Prob{T_j \mid D_i}$. This allows for \textit{soft cluster membership} in terms of probabilities.
	
	\item Each topic is associated with a probability vector, which quantifies the probability of the different terms in the lexicon for that topic. For example, consider a document that belongs completely to topic $T_j$. We denote the probability of term $t_l$ occurring in that document as $\Prob{t_l \mid T_j}$. 
\end{compactitem}
The two main methods for topic modeling are \green{Probabilistic Latent Semantic Indexing} (PLSA) and \green{Latent Dirichlet Allocation} (LDA).

\myspace
\p \blue{PLSA}. We note that the two aforementioned probabilities, $\Prob{T_j \mid D_i}$ and $\Prob{t_l \mid T_j}$ allow us to calculate $\Prob{t_l \mid D_i}$: the probability that term $t_l$ occurs in some document $D_i$:
\begin{align}
	\Prob{t_l \mid D_i} &= \sum_{j = 1}^{k} \Prob{t_l \mid T_j} \cdot \Prob{T_j \mid D_i} \label{wacka-flocka}
\end{align}
which should be interpreted as a weighted average\footnote{This is actually pretty bad notation, and borderline incorrect. $\Prob{T_j \mid D_i}$ is NOT a conditional probability! It is our prior! It is literally $\Prob{\text{ClusterOf}(D_i) = T_j}$.}. From here, we can generate a $n \times d$ matrix of probabilities. \\


\p Recall that we also have our $n \times d$ term-document matrix $\matr{X}$, where $\matr[i, l]{X}$ gives the number of times term $l$ occurred in document $D_i$. This allows us to do maximum likelihood! Our negative log-likelihood, $J$ can be derived as follows:\marginnote{Interpret $\Prob{\matr{X}}$ as the joint probability of observing the words in our data and with their assoc. frequencies.}[1em]
\begin{align}
	J &= - \log\left(  \Prob{\matr{X}} \right) \\
	&= - \log\left( \prod_{i, l} \Prob{ t_l \mid D_i   }^{ \matr[i, l]{X} } \right) \\
	&= - \sum_{i, l} \matr[i, l]{X} \cdot \log\left(  \Prob{ t_l \mid D_i   } \right) 
\end{align}
and we can plug-in eqn~\ref{wacka-flocka} to for evaluating $\Prob{ t_l \mid D_i}$. We want to optimize the value of $J$, subject to the constraints:
\begin{align}
	(\forall T_j): ~ \sum_l \Prob{t_l \mid T_j} = 1 
	\qquad\quad 
	(\forall D_i): ~ \sum_j \Prob{T_j \mid D_i} = 1
\end{align}
This can be solved with a Lagrangian method, similar to the process for NMF described earlier. See page 33/52 of the paper for details.

\myspace 
\p \blue{Latent Dirichlet Allocation} (LDA). The term-topic probabilities and topic-document probabilities are modeled with a \textit{Dirichlet distribution} as a prior\footnote{LDA is the Bayesian version of PLSI}. Typically preferred over PLSI because \underline{PLSI more prone to overfitting}.

\myspace\myspace\Needspace{25\baselineskip}
% ====================================
\subsub{Online Clustering with Text Streams}
% ====================================
\begin{center}
\linespread{0.5}\tiny Reference List: \myref{3}: A Framework for Clustering Massive Text and Categorical Data Streams; \purple{[112]}: Efficient Streaming Text Clustering; \myref{48}: Bursty feature representation
for clustering text streams; \myref{61}: Clustering Text Data Streams (Liu et al.)
\end{center}
\myspace

\p \blue{Overview}. Maintaining text clusters in real time.\marginnote{See ref. 112 for more on OSKM} One method is the \green{Online Spherical K-Means Algorithm} (OSKM)\footnote{Authors only provide a very brief description, which I'll just copy here:
\vspace{-1em}
\begin{quote}
	{\tiny \textit{This technique
			divides up the incoming stream into small segments, each of which can
			be processed effectively in main memory. A set of k-means iterations
			are applied to each such data segment in order to cluster them. The
			advantage of using a segment-wise approach for clustering is that since
			each segment can be held in main memory, we can process each data
			point multiple times as long as it is held in main memory. In addition,
			the centroids from the previous segment are used in the next iteration
			for clustering purposes. A decay factor is introduced in order to age-
			out the old documents, so that the new documents are considered more
			important from a clustering perspective.} }
\end{quote}}.

\myspace
\p \blue{Condensed Droplets Algorithm}. I'm calling it that because they don't call it anything -- it is the algorithm in \myref{3}. 
\begin{compactitem}
	\item \textbf{Fading function}: $f(t) = 2^{-\lambda \cdot t}$. A time-dependent weight for each data point (text stream). Non-monotonic decreasing; decays uniformly with time. 
	
	\item \textbf{Decay rate}: $\lambda = 1/t_0$. Inverse of the half-life of the data stream.
\end{compactitem}
When a cluster is created by a new point, it is allowed to remain as a trend-setting outlier for at least one half-life. During that period, if at least one more data point arrives, then the cluster becomes an active and mature cluster. If not, the trend-setting outlier is recognized as a true anomaly and is removed from the list of current clusters (\textit{cluster death}). Specifically, this happens when the (weighted) number of points in the [single-point] cluster is 0.5. The same criterion is used to define the death of
mature clusters. The statistics of the data points are referred to as \green{condensed droplets}, which represent the word distributions within a cluster, and can be used in order to compute the similarity of an incoming data point to the cluster. Main idea of algorithm is as follows:
\begin{compactenum}
	\item Initialize empty set of clusters $\mathcal{C} = \{\}$. As new data points arrive, unit clusters
	containing individual data points are created. Once a maximum number k of such clusters have been created, we can begin the process of online cluster maintenance.
	
	\item For a new data point $X$, compute its similarity to each cluster $C_j$, denoted as $S(X, C_j)$.
	\begin{compactitem}[-]
		\item If $S(X, C_{best}) > \text{thresh}_{outlier}$, or if there are no inactive clusters left\footnote{We specify some max allowed number of clusters $k$.},  insert $X$ to the cluster with maximum similarity.
		\item Otherwise, a new cluster is created\footnote{The new cluster replaces the least recently updated inactive cluster.} containing the solitary data point $X$.
	\end{compactitem} 
\end{compactenum}

\myspace
\p \blue{Misc.}
\begin{compactitem}
	\item Mandatory read: reference \myref{61}. Details phrase extraction/\green{topic signatures}. The use of using phrases instead of individual words is referred to as \green{semantic smoothing}. 
	\item For \textit{dynamic} (and more recent) topic modeling, see reference \myref{107} of the paper, titled ``A probabilistic model for online document clustering with application to novelty detection.''
\end{compactitem}


\Needspace{15\baselineskip}
\myspace
\p \blue{Semi-Supervised Clustering}. Useful when we have any prior knowledge about the kinds of clusters available in the underlying data. Some approaches:
\begin{compactitem}
	\item Incorporate this knowledge when seeding the cluster centroids for $k$-means clustering. 
	
	\item Iterative EM approach: unlabeled documents are assigned labels using a naive Bayes approach on the currently labeled documents. These newly labeled documents are then again used for re-training a Bayes
	classifier. Iterate to convergence.
	
	\item Graph-based approach: graph nodes are documents and the edges are similarities between the connected documents (nodes). We can incorporate prior knowledge by adding certain edges between nodes that we know are similar. A \textit{normalized cut algorithm} is then applied to this graph in order to create the final clustering.
\end{compactitem}
We can also use partially supervised methods in conjunction with pre-existing categorical \textit{hierarchies}. 





% ============================================================================================
\lecture{Papers and Tutorials}{Deep Sentence Embedding Using LSTMs}{July 10, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Palangi et al., ``Deep Sentence Embeddings Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval,'' (2016).}

\p \blue{Abstract}. Sentence embeddings using LSTM cells, which automatically attenuate unimportant words and detect salient keywords. Main emphasis on applications for document retrieval (matching a query to a document\footnote{Note that this similar to topic extraction.}).

\myspace
\p \blue{Introduction}. Sentence embeddings are learned using a loss function defined on \textit{sentence pairs}. For example, the well-known Paragraph Vector\footnote{Q. V. Le and T. Mikolov, Distributed representations of sentences and documents.} is learned in an unsupervised manner as a distributed representation of sentences and documents, which are then used for sentiment analysis. \\

\p The authors appear to use a dataset of their own containing examples of (search-query, clicked-title) for a search engine. Their training objective is to maximize the similarity between the two vectors mapped by the LSTM-RNN from the query and the clicked document, respectively. One very interesting claim to pay close attention to:
\vspace{-1em}
\begin{quote}
	{\small \textit{We further show that different cells in the learned model indeed correspond to \purple{different topics}, and the keywords associated with a similar topic activate the same cell unit in the model.} }
\end{quote}

\myspace
\p \blue{Related Work}. (Identified by reference number)
\begin{compactitem}
	\item ~ [2] Good for sentiment, but doesn't capture fine-grained sentence structure.
	
	\item ~ [6] Unsupervised embedding method trained on the BookCorpus [7]. Not good for document retrieval task.
	
	\item ~ [9] Semi-supervised Recursive Autoencoder (RAE) for sentiment prediction.
	
	\item ~ [3] DSSM (uses bag-of-words) and [10] CLSM (uses bag of n-grams) models for IR and also sentence embeddings.
	
	\item ~ [12] Dynamic CNN for sentence embeddings. Good for sentiment prediction and question type classification. In [13], a CNN is proposed for \green{sentence matching}\footnote{Might want to look into this.}
\end{compactitem}

\p \blue{Basic RNN}. The information flow (sequence of operations) is enumerated below.
\begin{compactenum}
	\item Encode $t$th word [of the given sentence] in one-hot vector $\vec{x}(t)$. 
	
	\item Convert $\vec{x}(t)$ to a \underline{letter} tri-gram vector $\vec{l}(t)$ using fixed hashing operator\footnote{Details aside, the hashing operator serves to lower the dimensionality of the inputs a bit. In particular we use it to convert one-hot word vectors into their letter tri-grams. For example, the word ``good'' gets surrounded by hashes, `\#good\#`, and then hashed from the one-hot vector to vectorized tri-grams, ``\#go'', ``goo'', ``ood'', ``od\#''.} $\matr{H}$:
	\begin{align}
		\vec{l}(t) &= \matr{H} \vec{x}(t)
	\end{align}
	
	\item Compute the hidden state $\vec{h}(t)$, which is the sentence embedding for $t = T$, the length of the sentence.
	\begin{align}
		\vec{h}(t) &= \tanh \left(\matr{U} \vec{l}(t) + \matr{W} \vec{h}(t - 1) + \vec{b} \right)
	\end{align}
	where $\matr{U}$ and $\matr{W}$ are the usual parameter matrices for the input/recurrent paths, respectively.
\end{compactenum}

\myspace
\p \blue{LSTM}. With peephole connections that expose the internal cell state $s$ to the sigmoid computations. I'll rewrite the standard LSTM equations from my textbook notes, but with the modifications for peephole connections:
\begin{align}
	f_i^{(t)} 
		&= \sigma\left(b_i^f + \sum_j U_{i,j}^f x_j^{(t)} + \sum_j W_{i,j}^f h_j^{(t-1)}
		+  \sum_j P_{i,j}^f s_j^{(t - 1)} \right) \\
	s_i^{(t)} 
		&= f_i^{(t)} \odot s_i^{(t-1)} + g_i^{(t)} \odot \sigma\left(b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} h_j^{(t-1)} \right) \\
	g_i^{(t)}
		&= \sigma\left(b_i^g + \sum_j U_{i,j}^g x_j^{(t)} + \sum_j W_{i,j}^g h_j^{(t-1)} 
		+ \sum_j P_{i,j}^g s_j^{(t - 1)} \right) \\
	q_i^{(t)}
		&= \sigma\left(b_i^o + \sum_j U_{i,j}^o x_j^{(t)} + \sum_j W_{i,j}^o h_j^{(t-1)} 
		+ \sum_j P_{i,j}^o s_j^{(t)}\right) 
\end{align}
The final hidden state can then be computed via
\begin{align}
h_i^{(t)} &= \tanh(s_i^{(t)}) \odot q_i^{(t)}
\end{align}


\myspace
\p \blue{Learning method}. We want to maximize the likelihood of the clicked document given query, which can be formulated as the following optimization problem:
\graybox{
	L(\matr{\Lambda}) 
	&= \min_{\matr{\Lambda}} \left\lbrace
		-\log \prod_{r = 1}^{N} \Prob{D_r^+ \mid Q_r} \right\rbrace
	=  \min_{\matr{\Lambda}} \sum_{r = 1}^{N} l_r(\matr{\Lambda}) \\
	l_r(\matr{\Lambda}) 
	&= \log\left(  1 + \jnsum e^{-\gamma \cdot \Delta_{r,j}} \right)
}
where 
\begin{compactitem}
	\item $N$ is the number of (query, clicked-doc) pairs in the corpus, while $n$ is the number of negative samples used during training.
	\item $D_r^+$ is the clicked document for $r$th query.
	\item $\Delta_{r,j} = R(Q_r, D_r^+) - R(Q_r, D_{r,j}^-)$ (R is just cosine similarity)\footnote{
		Note that $\Delta_{r,j} \in [-2, 2]$. We use $\gamma$ as a scaling factor so as to expand this range.
		}.
	\item $\matr{\Lambda}$ is all the parameter matrices (and biases) in the LSTM. 
\end{compactitem}
The authors then describe standard BPTT updates with momentum, which need not be detailed here. See the ``Algorithm 1'' figure in the paper for extremely detailed pseudo-code of the training procedure.




% ============================================================================================
\lecture{Papers and Tutorials}{Clustering Massive Text Streams}{July 10, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Aggarwal et al., ``A Framework for Clustering Massive Text and Categorical Data
	Streams,'' (2006).}

\p \blue{Overview}. Authors present an online approach for clustering massive text and categorical data streams with the use of a statistical summarization methodology. First, we will go over the process of storing and maintaining the data structures necessary for the clustering algorithm. Then, we will discuss the differences which arise from using different kinds of data, and the empirical results.

\myspace
\p \blue{Maintaining Cluster Statistics}. The data stream consists of $d$-dimensional records, where each dimension corresponds to the numeric frequency of a given word in the vector space representation. Each data point is weighted by the \green{fading function} $f(t)$, a non-monotonic decreasing function which decays uniformly with time $t$. The authors define the \green{half-life} of a data point (e.g. a tweet) as:
\begin{align}
	t_0 ~~ \text{s.t.} ~~ f(t_0) = \frac{1}{2} f(0)
\end{align}
and, similarly, the \green{decay-rate} as its inverse, $\lambda = 1/t_0$. Thus we have $f(t) = 2^{-\lambda \cdot t}$. \\

\p To achieve greater accuracy in the clustering process, we require a high level of granularity in the underlying data structures. To do this, we will use a process in which condensed clusters of data points are maintained, referred to as \green{cluster droplets}. We define them differently for the case of text and categorical data, beginning with categorical:
\begin{compactitem}
	\item \textbf{Categorical}. A cluster droplet $\mathcal{D}(t, \mathcal{C})$ for a set of
	categorical data points $\mathcal{C}$ at time $t$ is defined as the tuple:
	\begin{align}
		\mathcal{D}(t, \mathcal{C}) \triangleq (\bar{DF2},  \bar{DF1}, n, w(t), l)
	\end{align}
	where
	\begin{compactitem}
		\item Entry $k$ of the vector $\bar{DF2}$ is the (weighted) number of points in cluster $\mathcal{C}$ where the $i$th dimension had value $x$ and the $j$ dimension had value $y$. In other words, all pairwise combinations of values in the categorical vector\footnote{This is intentionally written hand-wavy because I'm really concerned with \textit{text} streams and don't want to give this much space.}. $\sum_{i = 1}^{d} \sum_{j \ne i}^{d} v_i v_j$ entries total\footnote{$v_i$ is the number of values the $i$th categorical dimension can take on.}.
		
		\item Similarly, $\bar{DF1}$ consists of the (weighted) counts that some dimension $i$ took on the value $x$. $\sum_{i = 1}^{d}v_i $ entries total.
		
		\item $w(t)$ is the sum of the weights of the data points at time $t$. 
		
		\item $l$ is the time stamp of the last time a data point was added to the cluster.
	\end{compactitem}
	
	
	\item \textbf{Text}. Can be viewed as an example of a \underline{sparse} numeric data set.  A cluster droplet $\mathcal{D}(t, \mathcal{C})$ for a set of text data points $\mathcal{C}$ at time $t$ is defined as the tuple:
	\begin{align}
	\mathcal{D}(t, \mathcal{C}) \triangleq (\bar{DF2},  \bar{DF1}, n, w(t), l)
	\end{align}
	where
	\begin{compactitem}
		\item $\bar{DF2}$ contains $3 \cdot wb \cdot (wb - 1) / 2$ entries, where $wb$ is the number of \underline{distinct words} \underline{in the \textit{cluster} $\mathcal{C}$}.
		
		\item $\bar{DF1}$ contains $2 \cdot wb$ entries.
		
		\item $n$ is the number of data points in the cluster $\mathcal{C}$. 
	\end{compactitem}
\end{compactitem}

\myspace
\p \blue{Cluster Droplet Maintenance}. 
\begin{compactenum}
	\item We first start of with $k$ trivial clusters (the first $k$ data points that arrived). 
	
	\item When a new point $\bar{X}$ arrives, the cosine similarity to each cluster's $\bar{DF1}$ is computed. 
	
	\item $\bar{X}$ is inserted into the cluster for which this is a maximum, so long as the associated $S(\bar{X}, \bar{DF1}) > \mathrm{thresh}$, a predefined threshold. If not above the threshold \textit{and} some \green{inactive cluster} exists, a new cluster is created containing the solitary point $\bar{X}$, which replaces the inactive cluster. If not above threshold but no inactive clusters, then we just insert it into the max similarity cluster anyway.
	
	\item If $\bar{X}$ was inserted (i.e. didn't replace an inactive cluster), then we need to:
	\begin{compactenum}
		\item Update the statistics to reflect the decay of the data points at the current moment in time\footnote{In other words, the statistics for a cluster do not decay, until a new point is added to it.}. This is done by multiplying entries in the droplet vectors by $2^{-\lambda \cdot (t - l)}$.
		
		\item Add the statistics for each newly arriving data point to the cluster statistics.
	\end{compactenum}
\end{compactenum}




% ============================================================================================
\lecture{Papers and Tutorials}{Supervised Universal Sentence Representations (InferSent)}{July 12, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Conneau et al., ``Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,'' Facebook AI Research (2017).}

\p \blue{Overview}. Authors claim universal sentence representations trained using the supervised data of the Stanford Natural Language Inference (SNLI) dataset can consistently outperform unsupervised methods like SkipThought on a wide range of transfer tasks. They emphasize that training on NLI tasks in particular results in embeddings that perform well in transfer tasks. Their best encoder is a Bi-LSTM architecture with max pooling, which they claim is SOTA when trained on the SNLI data.

\myspace
\p \blue{The Natural Language Inference Task}. Also known as \textit{Recognizing Textual Entailment} (RTE). The SNLI data consists of sentence pairs labeled as one of entailment, contradiction, or neutral. Below is a typical architecture for training on SNLI.

\myfig[0.4\textwidth]{SNLI_diagram.png}

Note that the same sentence encoder is used for both $u$ and $v$. To obtain a sentence vector from a BiLSTM encoder, they experiment with (1) the average $h_t$ over all $t$ (\green{mean pooling}), and (2) selecting the max value over each dimension of the hidden units [over all timesteps] (\green{max pooling})\footnote{Since the authors have already mentioned that BiLSTM did the best, I won't go over the other architectures they tried: self-attentive networks, hierarchical convnet, vanilla LSTM/GRU.}.



% ============================================================================================
\lecture{Papers and Tutorials}{Dist. Rep. of Sentences from Unlabeled Data (FastSent)}{July 13, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Hill et al., ``Learning Distributed Representations of Sentences from Unlabelled Data,'' (2016).}

\p \blue{Overview}. A systematic comparison of models that learn distributed representations of phrases/sentences from unlabeled data. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics.\\

\p Authors propose two new phrase/sentence representation learning objectives:
\begin{compactenum}
	\item \green{Sequential Denoising Autoencoders} (SDAEs)
	
	\item \green{FastSent}: a sentence-level log-linear BOW model.
\end{compactenum}

\myspace
\p \blue{Distributed Sentence Representations}. Existing models trained on text:
\begin{compactitem}
	\item \green{SkipThought Vectors} (Kiros et al., 2015). Predict target sentences $S_{i \pm 1}$ given source sentence $S_i$. Sequence-to-sequence model.
	 
	\item \green{ParagraphVector} (Le and Mikolov, 2014). Defines 2 log-linear models:
	\begin{compactenum}
		\item \textbf{DBOW}: learns a vector $\vec{s}$ for every sentence $S$ in the training corpus which, together with word embeddings $v_w$, define a softmax distribution to predict words $w \in S$ given $S$. 
		
		\item \textbf{DM}: select k-grams of consecutive words $\{w_i ~ \cdots ~ w_{i + k} \in S \}$ and the sentence vector $\vec{s}$ to predict $w_{i + k + 1}$.\marginnote{The authors use \purple{gensim} to implement ParagraphVector.}[-3em]
	\end{compactenum}
	
	\item \green{Bottom-Up Methods}. Train \textbf{CBOW} and \textbf{Skip-Gram} word embeddings on the Books corpus.
\end{compactitem}
\vspace{1em}

\p Models trained on \textit{structured} (and freely-available) resources:
\begin{compactitem}
	\item \green{DictRep} (Hill et al., 2015a). Map dictionary definitions to pre-trained word embeddings, using either BOW or RNN-LSTM encoding.
	
	\item \green{NMT}. Consider sentence representations learned by sequence-to-sequence NMT models.
\end{compactitem}

\myspace
\p \blue{Novel Text-Based Methods}. 
\begin{compactitem}
	\item \green{Sequential (Denoising) Autoencoders}. To avoid needing coherent inter-sentence narrative, try this representation-learning objective based on DAEs. For a given sentence $S$ and \textbf{noise function} $N(S \mid p_o, p_x)$ (where $p_0, p_x \in [0, 1]$),the approach is as follows:
	\begin{compactenum}
		\item For each $w \in S$, $N$ deletes $w$ with probability $p_o$.
		\item For each non-overlapping bigram $w_iw_{i + 1} \in S$, $N$ swaps $w_i$ and $w_{i + 1}$ with probability $p_x$.\marginnote{Authors recommend $p_o = p_x = 0.1$}
	\end{compactenum}
	\begin{quote}
		{\footnotesize \textit{
				We then train the same LSTM-based encoder-decoder architecture as NMT, but with the denoising objective to predict (as target) the original source sentence $S$ given a corrupted version $N(S \mid p_o, p_x)$ (as source).
				}}
	\end{quote}
	
	\item \green{FastSent}. Designed to be a more efficient/quicker to train version of SkipThought.
\end{compactitem}





% ============================================================================================
\lecture{Papers and Tutorials}{Latent Dirichlet Allocation}{July 22, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Blei et al., ``Latent Dirichlet Allocation,'' (2003).}

\myspace
\p \blue{Introduction}. At minimum, one should be familiar with generative probabilistic models, mixture models, and the notion of latent variables before continuing. The ``Dirichlet'' in LDA of course refers to the \green{Dirichlet distribution}, which is a generalization of the beta distribution, $B$. It's PDF is defined as\footnote{
	Recall that for positive integers $n$, $\Gamma(n) = (n - 1)!$.
	}\footnote{The Dirichlet distribution is \green{conjugate} to the multinomial distribution. TODO: Review how to interpret this.}:\marginnote{$$\sum_{i=1}^{K} \vec[i]{x} = 1$$  $$(\forall i \in [1,K]):\vec[i]{x} \ge 0$$}[1em]
	\begin{align}
		\text{Dir}(\vec{x}; \vec{\alpha}) = \frac{1}{B(\vec{\alpha})} \prod_{i=1}^{K} \vec[i]{x}^{\vec[i]{\alpha} - 1} 
		\quad \text{where} \quad
		B(\vec{\alpha}) = \frac{ \prod_{i=1}^{K} \Gamma\left( \vec[i]{\alpha}\right) }{  \Gamma(\sum_{i=1}^{K} \vec[i]{\alpha}) }
		\label{dirichlet}
	\end{align}
Main things to remember about LDA:
\begin{compactitem}[-]
	\item Generative probabilistic model for collections of \underline{discrete} data such as text corpora.
	\item Three-level \green{hierarchical Bayesian model}. Each document is a mixture of topics, each topic is an infinite mixture over a set of topic probabilities.
\end{compactitem}
Condensed comparisons/history of related models leading up to LDA:
\begin{compactitem}[-]
	\item \textbf{TF-IDF}. Design matrix $\matr{X} \in \R^{V \times M}$, where $M$ is the number of docs, and $\matr[i,j]{X}$ gives the TF-IDF value for $i$th word in vocabulary and corresp. to document $j$. 
	
	\item \textbf{LSI}:\footnote{Recall that LSI is basically PCA but without subtracting off the means} Performs SVD on the TF-IDF design matrix $\matr{X}$ to identify a linear subspace in the space of tf-idf features that captures most of the variance in the collection.
	
	\item \textbf{pLSI}: \red{TODO}
\end{compactitem}
\vspace{-1em}
\begin{quote}
	{\small \textit{pLSI is incomplete
			in that it provides no probabilistic model at the level of documents. In pLSI, each document is
			represented as a list of numbers (the mixing proportions for topics), and there is no generative
			probabilistic model for these numbers.}}
\end{quote}

 


\myspace
\p \blue{Model}. LDA assumes the following generative process for each document (word sequence) $\vec{w}$:
\begin{compactenum}
	\item \blue{$N \sim \text{Poisson}(\lambda)$}: Sample $N$, the number of words (length of $\vec{w}$), from $\text{Poisson}(\lambda) = e^{-\lambda} \frac{\lambda^n}{n!}$. The parameter $\lambda$ \textit{should} represent the average number of words per document.
	
	\item \red{$\theta \sim \text{Dir}(\alpha)$}: Sample $k$-dimensional vector $\theta$ from the Dirichlet distribution (eq.~\ref{dirichlet}), $\text{Dir}(\alpha)$. $k$ is the number of topics (pre-defined by us). Recall that this means $\theta$ lies in the (k-1) simplex. The Dirichlet distribution thus tells us the probability density of $\theta$ over this simplex -- it defines the probability of $\theta$ being at a given position on the simplex.
	
	\item Do the following $N$ times to \underline{generate the words} for this document.
	\begin{compactenum}
		\item \green{$z_n \sim \text{Multinomial}(\theta)$}. Sample a topic $z_n$.
		
		\item \purple{$w_n \sim \Prob{w_n \mid z_n, \beta} $}: Sample a word $w_n$ from $\Prob{w_n \mid z_n, \beta}$, a ``multinomial probability conditioned on topic $z_n$.''\footnote{\red{TODO}: interpret meaning of the multinomial distributions here. Seems a bit different than standard interp...} The parameter $\beta$ gives the distribution of words given a topic: 	
		\begin{align}
		\beta_{ij} &= \Prob{w_j \mid z_i} 
		\end{align}
		In other words, we really sample $w_n \sim \beta_{i,:}$
	\end{compactenum}
\end{compactenum}
The defining equations for LDA are thus:
\graybox{
	&\Prob{\theta, \vec{z}, \vec{w} \mid \alpha, \beta}
		= \mred{\Prob{\theta \mid \alpha}} \prod_{n=1}^{N} 
			\mgreen{ \Prob{z_n \mid \theta} } 
			\mpurple{ \Prob{w_n \mid z_n, \beta} } \\
	&\Prob{\vec{w} \mid \alpha, \beta}
		= \int  \mred{\Prob{\theta' \mid \alpha}} \left( \prod_{n=1}^{N}\sum_{z'_n}
			\mgreen{\Prob{z'_n \mid \theta'}}\mpurple{\Prob{w_n \mid z'_n, \beta} }
			\right) \mathrm{d}\theta' \\
	&\Prob{\mathcal{D} = \{ \vec{w}^{(1)}, \ldots, \vec{w}^{(M)} \} \mid \alpha, \beta}
		= \prod_{d=1}^{M} \Prob{\vec{w}^{(d)} \mid \alpha, \beta}
	}


\p Below is the plate notation for LDA, followed by an interpretation:

\myfig[0.4\textwidth]{LDA_plate_notation.png}

\begin{compactitem}
	\item \textbf{Outermost Variables}: $\alpha$ and $\beta$. Both represent a (Dirichlet) prior distribution: $\alpha$ parameterizes the probability of a given \textit{topic}, while $\beta$ a given \textit{word}. 
	
	\item \text{Document Plate}. $M$ is the number of documents, $\vec[m]{\theta}$ gives the true distribution of topics for document $m$\footnote{In other words, the meaning of $\vec[m,i]{\theta} = x$ is ``x percent of document $m$ is about topic $i$.''}.
	
	\item \text{Topic/Word Place}. $\vec[mn]{z}$ is the topic for word $n$ in doc $m$, and $\vec[mn]{w}$ is the word. It is shaded gray to indicate it is the only \textbf{observed variable}, while all others are \textbf{latent variables}. 
\end{compactitem}



\myspace
\p \blue{Theory}. I'll quickly summarize and interpret the main theoretical points. Without having read all the details, this won't be of much use (i.e. it is for someone who has read the paper already). 
\begin{compactitem}
	\item \textbf{LDA and Exchangeability}. We assume that each document is a bag of words (order doesn't matter; frequency sitll does) \textit{and} a bag of topics. In other words, a document of $N$ words \textit{is} an unordered list of words and topics. De Finetti's theorem tells us that we can model the joint probability of the words and topics as if a random parameter $\theta$ were drawn from some distribution and then the variables within $\vec{w},\vec{z}$ were \textbf{conditionally independent given $\theta$}. LDA posits that a good distribution to sample $\theta$ from is a Dirichlet distribution.
	
	\item \textbf{Geometric Interpretation}: \red{TODO}
\end{compactitem}

\myspace
\p \blue{Inference and Parameter Estimation}. As usual, we need to find a way to compute the posterior distribution of the hidden variables given a document $\vec{w}$:
\begin{align}
	\Prob{\vec{\theta}, \vec{z} \mid \vec{w}, \vec{\alpha}, \vec{\beta}} 
		&= \frac{ 	\Prob{\vec{\theta}, \vec{z}, \vec{w} \mid \vec{\alpha}, \vec{\beta}}  }{ \Prob{ \vec{w} \mid \vec{\alpha}, \vec{\beta}}     }
\end{align}
Computing the denominator exactly is intractable. Common approximate inference algorithms for LDA include Laplace approximation, variational approximation, and Markov Chain Monte Carlo.








% ============================================================================================
\lecture{Papers and Tutorials}{Conditional Random Fields}{July 30, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Lafferty et al., ``Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,'' (2001).}

\p \blue{Introduction}. CRFs offer improvements to HMMs, MEMMs, and other discriminative Markov models. MEMMs and other non-generative models share a weakness called the \green{label bias problem}: the transitions leaving a given
state compete only against each other, rather than against all other transitions in the model. The key difference between CRFs and MEMMs is that a CRF has a single exponential model for the joint probability of the \underline{entire sequence of labels} given the observation sequence.

\myspace
\p \blue{The Label Bias Problem}. Recall that MEMMs are run left-to-right. One way of interpreting such a model is to consider how the probabilities (of state sequences) are distributed as we continue through the sequence of observations. The issue with MEMMs is that there's nothing we can do if, somewhere along the way, we observe something that makes one of these state paths extremely likely/unlikely; we can't redistribute the probability mass amongst the various allowed paths. The CRF solution:
\vspace{-1em}
\begin{quote}
	{\small \textit{Account for whole state sequences at once by letting some transitions ``vote''
			more strongly than others depending on the corresponding observations. This implies that score mass will not be conserved, but instead individual transitions can ``amplify'' or
			``dampen'' the mass they receive.}}
\end{quote}

\myspace
\p \blue{Conditional Random Fields}. Here we formalize the model and notation. Let $\rvec{X}$ be a random variable over data sequences to be labeled (e.g. over all words/sentences), and let $\rvec{Y}$ the random variable over corresponding label sequences\footnote{We assume all components $\rvec[i]{Y}$ can only take on values in some finite label set $\mathcal{Y}$.}. Formal definition: 
\vspace{-1em}
\begin{quote}
	{\small \textit{Let $G = (V, E)$ be a graph such that $Y = (Y_v)_{v \in V}$, so that Y is indexed by the vertices of G. Then $(X, Y)$ is a CRF if, when conditioned on $X$, the random variables $Y_v$ obey the Markov property with respect to the graph:
	\begin{align}
	\Prob{Y_v | X, Y_w , w \ne v} =  \Prob{Y_v | X, Y_w , w \sim v}
	\end{align}
	where $w \sim v$ means that $w$ and $v$ are neighbors in G.}}
\end{quote}
All this means is a CRF is a random field (discrete set of random-valued points in a space) where all points (i.e. globally) are conditioned on $\rvec{X}$. If the graph $G = (V, E)$ of $\rvec{Y}$ is a tree, its cliques\footnote{A clique is a subset of vertices in an undirected graph such that every two distinct vertices in the clique are adjacent} are the edges and vertices. Take note that $\rvec{X}$ is not a member of the vertices in $G$. $G$ only contains vertices corresponding to elements of $\rvec{Y}$. Accordingly, when the authors refer to cases where $G$ is a ``chain'', remember that they just mean the $\rvec{Y}$ vertex sequence.  \\

\p By the fundamental theorem of random fields:
\graybox{
	p_{\theta}(\rvec{y} \mid \rvec{x})
	&\propto \exp\left(
		\sum_{e \in E, k} \lambda_k f_k(e, \rvec{y}|_e, \rvec{x}) +
		\sum_{v \in V, k} \mu_k g_k(v, \rvec{y}|_v, \rvec{x})
	\right)
	}
where $\rvec{y}|_S$ is the set of components of $\rvec{y}$ associated with the vertices in subgraph $S$. We assume the $K$ feature [functions] $f_k$ and $g_k$ are given and fixed. Note that $f_k$ are the feature functions over \textit{transitions} $y_{t-1}$ to $y_t$, and $g_k$ are the feature functions over \textit{states} $y_t$ and $x_t$. Our estimation problem is thus to determine parameters $\theta = (\lambda_1, \lambda_2, \ldots; \mu_1, \mu_2, \ldots)$ from the labeled training data.

\myspace
\p \blue{Linear-Chain CRF}. Let $|\mathcal Y|$ denote the number of possible labels. At each position $t$ in the observation sequence $\vec x$, we define the $|\mathcal Y| \times |\mathcal Y|$ matrix random variable $\matr[t]{M}(\vec x)$
\begin{align}
	\matr[t]{M}(y', y, \mid \vec x) &= \exp(\matr[t]{\Lambda}(y', y \mid x)) \\
	\matr[t]{\Lambda}(y', y \mid x) &=
		\sum_k \lambda_k f_k(y', y, \vec x) + 
		\sum_k \mu_k g_k(y, \vec x)
\end{align}
where $y_{t-1} := y'$ and $y_t := y$. We can see that the individual elements correspond to specific values of $e$ and $v$ in the double-summations of $p_{\theta}(\vec y \mid \vec x)$ above. Then the normalization (partition function) $Z_{\theta}(\vec x)$ is the $(y_0, y_{T+1})$ entry (the fixed boundary states) of the product:
\begin{align}
	Z_{\theta}(\vec x) &= \left[ \prod_{t = 1}^{T + 1} \matr[t]{M}(\vec x)  \right]_{y_0, y_{T + 1}}
\end{align} 
which includes all possible sequences $\vec y$ that start with the fixed $y_0$ and end with the fixed $y_{T + 1}$. Now we can write the conditional probability as a function of just these matrices:
\graybox{
	p_{\theta}(\vec y \mid \vec x)
	&= \dfrac{
		\prod_{t = 1}^{T + 1} \matr[t]{M}(y_{t - 1}, y_t \mid \vec x)	
	}{
		\left[ \prod_{t = 1}^{T + 1} \matr[t]{M}(\vec x)  \right]_{y_0, y_{T + 1}}
	}	
}

\myspace
\p \blue{Parameter Estimation} (for linear-chain CRFs). For each $t$ in $[0, T + 1]$, define the \green{forward vectors} $\alpha_t(\vec x)$ with base case $\alpha_0(y \mid \vec x) = 1$ if $y = y_0$, else 0. Similarly, define the \green{backward vectors} $\beta_t(\vec x)$ with base case $\beta_{T + 1}(y \mid \vec x) = 1$ if $y = y_{T+1}$ else 0\footnote{Remember that $y_0$ and $y_{T + 1}$ are their own fixed symbolic constants representing a fixed start/stop state.}. Their recurrence relations are
\begin{align}
	\alpha_t(\vec x) &=  \alpha_{t - 1}(\vec x) \matr[t]{M}(\vec x) \\
	\beta_t(\vec x)^T &= \matr[t+1]{M}(\vec x) \beta_{t + 1}(\vec x)
\end{align}

% ============================================================================================
\lecture{Papers and Tutorials}{Attention Is All You Need}{September 04, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Vaswani et al., ``Attention Is All You Need,'' (2017)}

\p \blue{Overview}. Authors refer to sequence \textit{transduction} models a lot -- just a fancy way of referring to models that transform input sequences into output sequences. Authors propose new architecture, the \green{Transformer}, based solely on attention mechanisms (no recurrence!). 

\myspace
\p \blue{Model Architecture}. 
\begin{compactitem}
	\item \textbf{Encoder}. $N \eq 6$ identical layers, each with 2 sublayers: (1) a \green{multi-head} \textbf{self-attention} mechanism and (2) a position-wise FC feed-forward network. They apply a residual connection and layer norm such that each sublayer, instead of outputting Sublayer(x), instead outputs LayerNorm(x + Sublayer(x)).
	
	\item \textbf{Decoder}. $N \eq 6$ with 3 sublayers each. In addition to the two sublayers described for the encoder, the decoder has a third sublayer, which performs \green{multi-head} attention over the output of the encoder stack. Same residual connections and layer norm. 
\end{compactitem}\marginnote{Figure shows encoder-decoder template layers. The actual model instantiates chain of 6 encoder layers and 6 decoders layers. The decoder's self-attention masks embeddings at future timesteps to zero.}[4em]
\myfig[0.4\textwidth]{figs/transformer.png}

\myspace
\p \blue{Attention}. An attention function can be described as a mapping:
\begin{align}
	\text{Attn(query, }\{(k_1, v_1), \ldots, \}\text{)} 
	&\Rightarrow \sum_i fn(\text{query}, k_i) v_i
\end{align}
where the query, keys, values, and output are all vectors. 
\begin{compactitem}
	\item \green{Scaled Dot-Product Attention}.
	\begin{compactenum}
		\item \textbf{Inputs}: queries $q$, keys $k$ of dimension $d_k$, values $v$ of dimension $d_v$\marginnote{Appears that $d_q \equiv d_k$.}
		\item \textbf{Dot Products}: Compute $\forall k:~ (q \cdot k) / \sqrt{d_k}$. 
		\item \textbf{Softmax}: on each dot product above. This gives the weights on the values shown earlier.
	\end{compactenum}
	In practice, this is done simultaneously for all queries in a set via the following matrix equation:
	\graybox{
		\text{Attention}(Q, K, V) 
		&= \text{softmax}\left( 
		\frac{QK^T}{\sqrt{d_k}}
		\right) V
		}
	Note that this is identical to the standard dot-product attention mechanism, except for the \textit{scaling} factor (hence the name) of $1/\sqrt{d_k}$. The scaling factor is motivated by the fact that additive attention outperforms dot-product attention for large $d_k$ and the authors stipulate this is due to the softmax having small gradients in this case, due to the large dot products\footnote{
		Assume that $\vec q$ and $\vec k$ are vectors in $\R^d$ whose components are independent RVs with $\E{q_i} = \E{k_j} = 0$ $(\forall i, j)$, and $\Var{q_i} = \Var{k_j} = 1$ $(\forall i, j)$. Then
		\begin{align}
			\E{\vec q \dotp \vec k}
				&= \E{\sum_i^d q_i k_i} 
				= \sum_i^d \E{q_i k_i} 
				= \sum_i^d \E{q_i} \E{k_i} 
				= 0 \\
			\Var{\vec q \dotp \vec k}
				&= \Var{\sum_i^d q_i k_i} 
				= \sum_i^d \Var{q_i k_i} 
				= \sum_i^d \E{q_i^2 k_i^2} - \cancel{ \E{q_i k_i} } \\
				&= \sum_i^d \E{q_i^2} \E{k_i^2} 
				= \sum_i^d \Var{q_i} \Var{k_i} 
				= d
		\end{align}
	
		See \href{https://stats.stackexchange.com/a/318258}{this S.O answer} and/or \href{http://www.odelama.com/data-analysis/Commonly-Used-Math-Formulas}{these useful formulas} for more details.
	}. \\

	First, let's explicitly show which indices are being normalized over, since it can get confusing when presented with the highly vectorized version above. For a given input sequence of length $T$, and for the self-attention version where $K \eq Q \eq V \in \R^{T \times d_{k}}$, the output attention vector for timestep $t$ is explicitly (ignoring the $\sqrt{d_k}$ for simplicity)
	\begin{align}
		\text{Attention}(Q, K, V)_{\mgreen{t}}
			&= \left[ \text{softmax}\left( Q K^T \right) V \right]_{\mgreen{t}} \\
			&= \sum_{t'}^{T} \dfrac{ e^{\mgreen{Q_t} \cdot K_{t'}}  }{  \sum_{t''}^{T} e^{\mgreen{Q_t} \cdot K_{t''}}    } V_{t'}
	\end{align} 

	\Needspace{10\baselineskip}
	Next, the gradient of the $d$th softmax output w.r.t its inputs is 
	\begin{align}
		\pderiv{ \text{Softmax}_d(\vec x) }{x_j} 
			&= \text{Softmax}_d(\vec x) \left(    \delta_{dj} - \text{Softmax}_d(\vec x)   \right)
	\end{align}
	
	
	
	\item \green{Multi-Head Attention}. Basically just doing some number $h$ of parallel attention computations. Before each of these, the queries, keys, and values are linearly projected with different,
	learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions respectively (and then fed to their respective attention function). The $h$ outputs are then concatenated and once again projected, resulting in the final values.\marginnote{
		$$ W_i^Q \in \R^{d_{model} \times d_k} $$
		$$ W_i^K \in \R^{d_{model} \times d_k} $$
		$$ W_i^V \in \R^{d_{model} \times d_v} $$
		$$ W^O \in \R^{h d_{v} \times d_{model}} $$
		}[-1em]
	\graybox{
		\text{MultiHead}(Q, K, V)
		&= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O \\
		\text{where head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
		}
	The authors employ $h = 8$, $d_k = d_v = d_{model} / h = 64$. 
\end{compactitem}

The Transformer uses multi-headed attention in 3 ways:
\begin{compactenum}
	\item \textbf{Encoder-decoder attention}: the normal kind. Queries are previous decoder layer, and memory keys and values come from output of the [final layer of] encoder. 
	
	\item \textbf{Encoder self-attention}: all of the keys, values, and queries come from the previous \textit{layer} in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 
	
	\item \textbf{Decoder self-attention}: Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position (timestep). The masking is done on the inputs to the softmax, setting all inputs beyond the current timestep to $-\infty$. 
\end{compactenum}

\myspace
\p \blue{Other Components}. 
\begin{compactitem}
	\item \textbf{Position-wise Feed-Forward Networks (FFN)}: each layer of the encoder and decoder contains a FC FFN, applied to each position separately and identically:\marginnote{The FFN is linear $\rightarrow$ ReLU $\rightarrow$ linear.}[2em]
	\begin{align}
		\text{FFN}(x) = \max\left(0, x W_1 + b_1 \right) W_2 + b_2
	\end{align}
	
	\item \textbf{Embeddings and Softmax}: use learned embeddings to convert input/output tokens to vectors of dimension $d_{model}$, \textit{and} for the pre-softmax layer at the output of the decoder\footnote{In other words, they use the \underline{same} weight matrix for all three of (1) encoder input embedding, (2) decoder input embedding, and (3) (opposite direction) from decoder output to pre-softmax.}\marginnote{For inputs to encoder/decoder, the embedding weights are multiplied by $\sqrt{d_{model}}$}. 
	
	\item \textbf{Positional Encoding}: how the authors deal with the lack of recurrence (to make use of the sequence order). They \underline{add} a sinusoid function of the position (timestep) $pos$ and vector index $i$ to the input embeddings for the encoder and decoder\footnote{Note that the positional encodings must necessarily be of dimension $d_{model}$ to be summed with the input embeddings.}:
	\graybox{
		\text{PE}(pos, 2i) &= \sin\left( \frac{pos}{10000^{2i/d_{model}}} \right) \\
		\text{PE}(pos, 2i+1) &= \cos\left( \frac{pos}{10000^{2i/d_{model}}} \right)
		}
	The authors justify this choice:
	\begin{quote}
		{\small \textit{We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $\text{PE}_{pos+k}$ can be represented as a linear function of $\text{PE}_{pos}$.}}
	\end{quote}
\end{compactitem}


\bluesec{Summary of Add-ons}. Below is a list of all the little bells and whistles they add to the main components of the model that are easy to miss since they mention them throughout the paper in a rather unorganized fashion.
\begin{compactitem}
	\item Shared weights for encoder inputs, decoder inputs, and final softmax projection outputs. 
	\item Multiply the encoder and decoder input embedding [shared] weights by $\sqrt{d_{model}}$. \red{TODO}: why? Also this must be highly correlated with their decision regarding weight initialization (mean/stddev/technique). Add whatever they use here if they mention it.
	
	\item Adam optimizer with $\beta_1 \eq 0.9$, $\beta_2 \eq 0.98$, $\epsilon \eq 10^{-9}$.
	
	\item Learning rate schedule $\text{LR}(s) = d^{-0.5}_{model} \cdot \min \left(  s^{-0.5}, s \cdot w^{-1.5}  \right)  $ for global step $s$ and warmup steps $w \eq 4000$. 
	
	\item Dropout on sublayer outputs pre-layernorm-and-residual. Specifically, they \textit{actually} return LayerNorm(x + Dropout(Sublayer(x))). Use $P_{drop} = 0.1$. 
	
	\item Dropout the summed embeddings+positional-encodings for both encoder and decoder stacks.
	
	\item Dropout on softmax outputs. So do Dropout(Softmax(QK))V. 
	
	\item Label smoothing with $\epsilon_{ls} = 0.1$. 
\end{compactitem}



% ============================================================================================
\lecture{Papers and Tutorials}{Hierarchical Attention Networks}{September 06, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Yang et al., ``Hierarchical Attention Networks for Document Classification.''}

\blue{Overview}. Authors introduce the Hierarchical Attention Network (HAN) that is designed to capture insights regarding (1) the hierarchical structure of documents (words -> sentences -> documents), and (2) the context dependence between words and sentences. The latter is implemented by including two levels of attention mechanisms, one at the word level and one at the sentence level.

\myspace
\p \blue{Hierarchical Attention Networks}. Below is an illustration of the network. The first stage is familiar to sequence to sequence models - a bidirectional encoder for outputting sentence-level representations of a sequence of words. The HAN goes a step further by feeding this another bidirectional encoder for outputting document-level representations for sequences of sentences. 

\myfig[0.4\textwidth]{HAN.png}

The authors choose the GRU as their underlying RNN. For ease of reference, the defining equations of the GRU are shown below:
\begin{align}
h_t &= (1 - z_t) \odot h_{t - 1} + z_t \odot \tilde h_t \\
z_t &= \sigma \left( W_z x_t + U_z h_{t - 1} + b_z \right) \\
\tilde h_t &= \tanh \left( W_h x_t + r_t \odot (U_h h_{t - 1}) + b_h \right) \\
r_t &= \sigma \left( W_r x_t + U_r h_{t - 1} + b_r \right)
\end{align}

\myspace
\p \blue{Hierarchical Attention}. Here I'll overview the main stages of information flow.
\begin{compactenum}
	\item \textbf{Word Encoder}. Let the $t$th word in the $i$th sentence be denoted $w_{it}$. They embed the vectors with a word embedding matrix $W_e$, $x_{it} = W_e w_{it}$, and then feed $x_{it}$ through a bidirectional GRU to ultimately obtain $h_{it} := [\overrightarrow h_{it}; \overleftarrow{h_{it}}]$.
	
	\item \textbf{Word Attention}. Extracts words that are important to the meaning of the sentence and aggregates the representation of these informative words to form a sentence vector. 
	\graybox{
		u_{it} &= \tanh(W_w h_{it} + b_w) \\
		\alpha_{it} &= \frac{\exp(u_{it}^T u_w)}{\sum_t \exp(u_{it}^T u_w)} \\
		s_i &= \sum_t \alpha_{it} h_{it}
	}
	Note the \textit{context vector} $u_w$, which is shared for all words\footnote{To emphasize, there is only a single context vector $u_w$ in the network, period. The subscript just tells us that it is the word-level context vector, to distinguish it from the sentence-level context vector in the later stage.} and randomly initialized and jointly learned during the training process. 
	
	\item \textbf{Sentence Encoder}. Similar to the word encoder, but uses the sentence vectors $s_i$ as the input for the $i$th sentence in the document. Note that the output of this encoder, $h_i$ contains information from the neighboring sentences too (bidirectional) but focuses on sentence $i$. 
	
	\item \textbf{Sentence Attention}. For rewarding sentences that are clues to correctly classify a document. Similar to before, we now use a sentence level context vector $u_s$ to measure the importance of the sentences.
	\graybox{
		u_{i} &= \tanh(W_s h_{i} + b_s) \\
		\alpha_{i} &= \frac{\exp(u_{i}^T u_s)}{\sum_i \exp(u_{i}^T u_s)} \\
		v &= \sum_t \alpha_{i} h_{i}
	}
	where v is the document vector that summarizes all the information of sentences in a document.
\end{compactenum}

As usual, we convert $v$ to a normalized probability vector by feeding through a softmax:
\begin{align}
p = \text{softmax}(W_c v + b_c)
\end{align}

\myspace
\p \blue{Configuration and Training}. Quick overview of some parameters chosen by the authors:
\begin{compactitem}
	\item \textbf{Tokenization}: Stanford CoreNLP. Vocabulary consists of words occurring more than 5 times, all others are replaced with UNK token.
	
	\item \textbf{Word Embeddings}: train word2vec on the training and validation splits. Dimension of 200.
	
	\item \textbf{GRU}. Dimension of 50 (so 100 because bidirectional). 
	
	\item \textbf{Context vectors}. Both $u_w$ and $u_s$ have dimension of 100. 
	
	\item \textbf{Training}: batch size of 64, grouping documents of similar length into a batch. SGD with momentum of 0.9.
\end{compactitem}


% ============================================================================================
\lecture{Papers and Tutorials}{Joint Event Extraction via RNNs}{Oct 31, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Nguyen, Cho, and Grishman, ``Joint Event Extraction via Recurrent Neural Networks,'' (2016).}

\p \blue{Event Extraction Task}. Automatic Context Extraction (ACE) evaluation. Terminology:
\begin{compactitem}
	\item \green{Event}: something that happens or leads to some change of state.
	\item \green{Mention}: phrase or sentence in which an event occurs, including one trigger and an arbitrary number of arguments.
	\item \green{Trigger}: main word that most clearly expresses an event occurrence. 
	\item \green{Argument}: an entity mention, temporal expression, or value that serves as a participant/attribute with a specific role in an event mention.
\end{compactitem}

Example:\marginnote{\red{TRIGGER}\\\green{ARGUMENT}}[3em]
\begin{quote}
	In Baghdad, a \green{cameraman} \red{died}\{\textit{Die}\} when an American tank \red{fired}\{\textit{Attack}\} on the Palestine hotel.
\end{quote}
Each event subtype has its own set of roles to be filled by the event arguments. For example, the roles for the \textit{Die} event subtype include \textit{Place}, \textit{Victim}, and \textit{Time}. 

\myspace
\p \blue{Model}. 
\begin{compactitem}[-]
	\item \textbf{Sentence Encoding}. Let $w_i$ denote the $i$th token in a sentence. It is transformed into a real-valued vector $x_i$, defined as
	\begin{align}
		x_i := \left[ \text{GloVe}(w_i);~ \text{Embed}(\text{EntityType}(w_i));~ \text{DepVec}(w_i)    \right]
	\end{align}
	where ``Embed'' is an embedding we learn, and ``DepVec'' is the binary vector whose dimensions correspond to the possible relations between words in the dependency trees. 
	
	\item \textbf{RNN}. Bidirectional LSTM on the inputs $x_i$. 
	
	\item \textbf{Prediction}. Binary memory vector $G_i^{trg}$ for triggers; binary memory matrices $G_i^{arg}$ and $G_i^{arg/trg}$ for arguments (at each timestep i). At each time step $i$, do the following in order:
	\begin{compactenum}
		\item \underline{Predict trigger} $t_i$ for $w_i$. First compute the feature representation vector $R_i^{trig}$, defined as:
		\begin{align}
			R_i^{trig} &:= \left[ h_i;~ L_i^{trg};~ G_{i-1}^{trg} \right]
		\end{align}
		where $h_i$ is the RNN output, $L_i^{trg}$ is the local context vector for $w_i$, and $G_{i-1}^{trg}$ is the memory vector from the previous step. $L_i^{trg} := [\text{GloVe}(w_{i-d}); \ldots; \text{GloVe}(w_{i + d})]$ for some predefined window size $d$. This is then fed to a fully-connected layer with softmax activation, $\vec{F}^{trg}$, to compute the probability over possible trigger subtypes:
		\begin{align}
			P_{i;t}^{trg} &:= F_t^{trg}(R_i^{trg})
		\end{align}
		As usual, the predicted trigger type for $w_i$ is computed as $t_i = \argmax_t \left( P_{i;t}^{trg} \right)$. If $w_i$ is not a trigger, $t_i$ should predict ``\textit{Other}.''
		
		\item \underline{Argument role predictions}, $a_{i1}, \ldots, a_{ik}$, for all of the [already known] entity mentions in the sentence, $e_1, \ldots, e_k$ with respect to $w_i$. $a_{ij}$ denotes the argument role of $e_j$ with respect to [the predicted trigger of] $w_i$. If NOT($w_i$ is trigger AND $e_j$ is one of its arguments), then $a_{ij}$ is set to \textit{Other}. For example, if $w_i$ was the word ``died'' from our example sentence, we'd hope that its predicted trigger would be $t_i = Die$, and that \textbf{the entity associated with ``cameraman'' would get a predicted argument role of \textit{Victim}.}
		
		\lstset{language=Python}
		\begin{lstlisting}
		def getArgumentRoles(triggerType=t, entities=e):
			k = len(e)
			if isOther(t):
				return [Other] * k 
			else:
				for e_j in e:
					
		\end{lstlisting}

		\begin{align}
		R_{ij}^{arg} &:= \left[ h_i;~ h_{i_j};~ L_{ij}^{arg};~ B_{ij};~ G_{i-1}^{arg}[j]; G_{i-1}^{arg/trg}[j] \right]
		\end{align}

		\item \underline{Update memory}. TO BE CONTINUED...(moving onto another paper because this model is getting a \textit{bit} too contrived for my tastes. Also not a fan of the reliance on a dependency parse.)
	\end{compactenum}
\end{compactitem}


\begin{comment}
% ============================================================================================
\lecture{Papers and Tutorials}{Zero-Shot Learning for Event Extraction}{Oct 31, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize L. Huang, H. Ji, K. Cho, and C. Voss, ``Zero-Shot Learning for Event Extraction,'' (2017).}
\end{comment}



% ============================================================================================
\lecture{Papers and Tutorials}{Event Extraction via Bidi-LSTM Tensor NNs}{Oct 31, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Y. Chen, S. Liu, S. He, K. Liu, and J. Zhao, ``Event Extraction via Bidirectional Long Short-Term Memory Tensor Neural Networks.''}

\p \blue{Overview}. The task/goal is the event extraction task as defined in \textit{Automatic Content Extraction} (ACE). Specifically, given a text document, our goal is to do the following in order \textit{for each sentence}:
\begin{compactenum}
	\item Identify any event triggers in the sentence.
	\item If triggers found, predict their subtype. For example, given the trigger ``fired,'' we may classify it as having the \textit{Attack} subtype.
	\item If triggers found, identify their candidate argument(s). ACE defines an event argument as ``an entity mention, temporal expression, or value that is involved in an event.''
	\item For each candidate argument, predict its role: ``the relationship between an argument to the event in which it participates.''
\end{compactenum}

\myspace
\p \blue{Context-aware Word Representation}. Use pre-trained word embeddings for the input word tokens, the predicted trigger, and the candidate argument. Note: \textit{we assume we already have predictions for the event trigger $t$ and are doing a pass for one of (possibly many) candidate arguments $a$.}


\begin{compactenum}
	\item Embed each word in the sentence with pre-trained embeddings. Denote the embedding for $i$th word as $e(w_i)$. 
	
	\item Feed each $e(w_i)$ through a bidirectional LSTM. Denote the $i$th output of the forward LSTM as $c_l(w_{i+1})$ and the output of the backward LSTM at the same time step as $c_r(w_{i - 1})$. As usual, they take the general functional form:
	\begin{align}
	c_l(w_i) &= \overrightarrow{LSTM}\left(c_l(w_{i - 1}), e(w_{i-1}) \right) \\
	c_r(w_i) &= \overleftarrow{LSTM}\left(c_r(w_{i + 1}), e(w_{i+1}) \right) \\
	\end{align}
	
	\item Concatenate $e(w_i)$, $c_l(w_i)$, $c_r(w_i)$ together along with the embedding of the candidate argument $e(a)$ and predicted trigger $e(t)$. Also include the relative distance of $w_i$ to $t$ or (??) $a$, denoted as $pi$ for position information, and the embedding of the predicted event type $pe$ of the trigger. Denote this massive concatenation result as $x_i$:
	\begin{align}
		x_i := c_l(w_i) \oplus e(w_i) \oplus c_r(w_i) \oplus pi \oplus pe \oplus e(a) \oplus e(t) \label{fat-concat}
	\end{align}
	
\end{compactenum}

\myspace
\p \blue{Dynamic Multi-Pooling}. This is easiest shown by example. Continue with our example sentence:
\begin{quote}
	In California, \green{Peterson} was arrested for the \red{murder} of his wife and unborn son.
\end{quote}
where the colors are given for \textit{this specific case where murder is our predicted trigger and we are considering the candidate argument Peterson}\footnote{Yes, arrested could be another predicted trigger, but the network considers each possibility at separate times/locations in the architecture.}. Given our $n$ outputs from the previous stage, $y^{(1)} \in \R^{n \times m}$, where $n$ is the length of the sentence and $m$ is the size of that huge concatenation given in equation \ref{fat-concat}. We split our sentence by trigger and candidate argument, then (confusingly) redefine our notation as\marginnote{Peterson is the 3rd word, and murder is the 8th word.}[3em]
\begin{align}
	{y}^{(1)}_{1j} &\leftarrow \begin{bmatrix} y^{(1)}_{1j} & y^{(1)}_{2j} \end{bmatrix} \\
	{y}^{(1)}_{2j} &\leftarrow \begin{bmatrix} y^{(1)}_{3j} & \cdots & y^{(1)}_{7j} \end{bmatrix} \\
	{y}^{(1)}_{3j} &\leftarrow \begin{bmatrix} y^{(1)}_{8j} & \cdots & y^{(1)}_{nj} \end{bmatrix} 
\end{align}
where it's important to see that, for some $1 \le j \le m$, each new $y^{(1)}_{ij}$ is a \textit{vector} of length equal to the number of words in segment $i$. Finally, the dynamic multi-pooling layer, $y^{(2)}$, can be expressed as 
\graybox{
	y^{(2)}_{i,j} &:= \max \left( y^{(1)}_{i,j}\right) \qquad 1 \le i \le 3, ~ 1 \le j \le m
}
where the max is taken over each of the aforementioned vectors, leaving us with $ 3m $ values total. These are concatenated to form $y^{(2)} \in \R^{3m}$. 


\myspace
\p \blue{Output}. To predict of each argument role [for the given argument candidate], $y^{(2)}$ is fed through a dense softmax layer,
\begin{align}
	O &= W_2 y^{(2)} + b_2
\end{align}
where $W_2 \in \R^{n_1 \times 3m}$ and $n_1$ is the number of possible argument roles (including "None"). The authors also use dropout on $y^{(2)}$.



% ============================================================================================
\lecture{Papers and Tutorials}{Reasoning with Neural Tensor Networks}{Nov 2, 2017}
% ============================================================================================

\vspace{-1em}
{\footnotesize Socher et al., ``Reasoning with Neural Tensor Networks for Knowledge Base Completion''}

\p \blue{Overview}. Reasoning over relationships between two entities. Goal: predict the likely truth of additional facts based on existing facts in the KB. This paper contributes (1) the new NTN and (2) a new way to represent entities in KBs. Each relation is associated with a distinct model. Inputs to a given relation's model are pairs of database entities, and the outputs score how likely the pair has the relationship.

\myspace
\p \blue{Neural Tensor Networks for Relation Classification}. Let $e_1, e_2 \in \R^d$ be the vector representations of the two entities, and let $R$ denote the relation (and thus model) of interest. The NTN computes a score of how likely it is that $e_1$ and $e_2$ are related by $R$ via:\marginnote{$$ W_R^{[1:k]} \in \R^{d \times d \times k}$$ $$  V_R \in \R^{k \times 2d}$$}[1em]
\graybox{
	g(e_1, R, e_2) &= u_R^T \tanh\left( 
		e_1^T W_R^{[1:k]} e_2 + V_R \begin{bmatrix} e_1 \\ e_2 \end{bmatrix} + b_R
	\right)
	}
where the bilinear tensor product $e_1^T W_R^{[1:k]} e_2$ results in a vector $h \in \R^k$ with each entry computed by one slice $i = 1,\ldots,k$ of the tensor.
\begin{quote}
	{\itshape Intuitively, we can see each slice of the tensor as being responsible for one type of entity pair or instantiation
of a relation\textellipsis Another way to interpret each tensor slice is that it mediates the relationship between the two entity vectors differently.}
\end{quote}

\myspace
\p \blue{Training Objective and Derivatives}. All models are trained with \green{contrastive max-margin objective functions} and minimize the following objective:
\graybox{
	J(\matr{\Omega}) &= \sum_{i = 1}^{N} \sum_{c = 1}^{C} \max\left( 0,
		1 - g\left( T^{(i)} \right) + g\left( T_c^{(i)} \right) \right) + \lambda ||\matr{\Omega}||_2^2
	}
where $c$ is for ``corrupted'' samples, $T_c^{(i)} := \left(e_1^{(i)}, R^{(i)}, e_c^{(i)}\right)$. Notice that this function is minimized when the difference, $g\left( T^{(i)} \right) - g\left( T_c^{(i)} \right)$, is maximized. The authors used minibatched \green{L-BFGS} for optimization.



% ============================================================================================
\lecture{Papers and Tutorials}{Language to Logical Form with Neural Attention}{Nov 6, 2017}
% ============================================================================================


\vspace{-1em}
{\footnotesize Dong and Lapata, ``Language to Logical Form with Neural Attention,'' (2016)}

\p \blue{Sequence-to-Tree Model}. Variant of Seq2Seq that is more faithful to the compositional nature of meaning representations. It's schematic is shown below. The authors define a ``nonterminal'' $<n>$ token which indicates [the root of] a subtree.

\myfig[0.5\textwidth]{Seq2Tree.png}

where the author's have employed ``parent-feeding'': for a given subtree (logical form), at each timestep, the hidden vector of the parent nonterminal is concatenated with the inputs and fed into the LSTM (best understood via above illustration). 

\begin{quote}
	{\itshape After encoding input $q$, the hierarchical tree decoder generates tokens at depth 1 of the subtree corresponding to parts of logical form $a$. If the predicted token is $<n>$, decode the sequence by conditioning on the nonterminal's hidden vector. This process terminates when no more nonterminals are emitted. }
\end{quote}

\Needspace{10\baselineskip}
Also note that the output posterior probability over the encoded input $q$ is the product of subtree posteriors. For example, consider the decoding example in the figure below:
\myfig[0.4\textwidth]{Seq2TreeExample.png}

We would compute the output posterior as:
\begin{align}
	p(a \mid q) &= p(y_1 y_2 y_3 y_4 \mid q) p(y_5 y_6 \mid y_{\le 3},q)
\end{align}
The model is trained by minimizing log-likelihood over the training data, using RMSProp for optimization. At inference time, greedy search or beam search is used to predict the most probable output sequence.








% ============================================================================================
\lecture{Papers and Tutorials}{Seq2SQL: Generating Structured Queries from NL using RL}{Nov 6, 2017}
% ============================================================================================


\vspace{-1em}
{\footnotesize Zhong, Xiong, and Socher, ``Seq2SQL: Generating Structured Queries From Natural Language Using Reinforcement Learning''}

\p \blue{Overview}. Deep neural network for translating natural language questions to corresponding SQL queries. Outperforms state-of-the-art semantic parser.

\myspace
\p \blue{Seq2Tree and Pointer Baseline}. Baseline model is the Seq2Tree model from the previous note on Dong \& Lapata's (2016) paper. Authors here argue their output space is unnecessarily large, and employ the idea of pointer networks with augmented inputs. The input sequence is the concatenation of (1) the column names, (2) the limited vocabulary of the SQL language such as \texttt{SELECT}, \texttt{COUNT}, etc., and (3) the question\marginnote{$$ x_j^c \in \R^{T_j}$$}[2em].
\begin{align}
	x &:= \left[
		<col>;~
		x_1^c; x_2^c; \ldots; x_N^c;~
		<sql>;~
		x^s;~
		<question>;~
		x^q
	\right]
\end{align}
where we also insert special (``sentinel'') tokens to demarcate the boundaries of each section. The pointer network can then produce the SQL query by selecting exclusively from the input. Let $g_s$ denote the $s$th decoder [hidden] state, and $y_s$ denote the output (index/pointer to input query token).
\graybox{
	\mtgreen{[ptr net]} ~
	y_s = \argmax_t \left( \alpha_s^{ptr} \right) \quad \text{where} \quad
	\alpha_{s,t}^{ptr} = w^{ptr} \cdot \tanh\left( U^{ptr} g_s + V^{ptr} h_t \right) \label{seq2sql-baseline}
	}

\myspace
\p \blue{Seq2SQL}.
\begin{enumerate}
	\item \textbf{Aggregation Classifier}. Our goal here is to predict which aggregation operation to use out of \texttt{COUNT}, \texttt{MIN}, \texttt{MAX}, \texttt{NULL}, etc. This is done by projecting the attention-weighted average of encoder states, $\kappa^{agg}$, to $\R^{C}$ where $C$ denotes the number of unique aforementioned aggregation operations. The sequence of computations is summarized as follows:\marginnote{$$W^{agg} \in \R^{C \times T}$$ $$ \beta^{agg} \in \R^{C}$$}[4em]
	\begin{align}
		\alpha_t^{inp} 	&= w^{inp} \cdot h_t^{enc} \label{seq2sql-attn} \\
		\beta^{inp} 	&= \text{softmax}\left( \alpha^{inp} \right) \\
		\kappa^{agg} 	&= \sum_t^T \beta_t^{inp} h_t^{enc} \\
		\alpha^{agg} 	&= W^{agg} \tanh\left( V^{agg} \kappa^{agg} + b^{agg} \right) + c^{agg} \\
		\beta^{agg} 	&= \text{softmax}\left( \alpha^{agg} \right)
	\end{align}
	where $\beta_i^{agg}$ gives the probability for the $i$th aggregation operation. We use cross entropy loss $L^{agg}$ for determining the aggregation operation. Note that this part isn't really a sequence-to-sequence architecture. It's nothing more than an MLP applied to an attention-weighted average of the encoder states.
	
	
	\item \textbf{Get Pointer to Column}. A pointer network is used for identifying which column in the input representation should be used in the query. Recall that $x_{j,t}^c$ denotes the $t$th word in column $j$. We use the last encoder state for a given column's LSTM\footnote{Yes, we encode each column with an LSTM separately.} as its representation; $T_j$ denotes the number of words in the $j$th column.
	\begin{align}
		e_j^c = h_{j,T_j}^c \quad \text{where} \quad 
		h_{j,t}^c = \text{LSTM}\left( \text{emb}(x_{j,t}^c) , h_{j,t-1}^c \right)
	\end{align}
	To construct a representation for the question, compute another input representation $\kappa^{sel}$ using the same architecture (but distinct weights) as for $\kappa^{agg}$. As usual, we compute the scores for each column $j$ via:
	\begin{align}
		\alpha_j^{sel} &= W^{sel} \tanh\left( V^{sel} \kappa^{sel} + V^c e_j^c \right) \\
		\beta^{sel} &= \text{softmax}\left( \alpha^{sel} \right)
	\end{align}
	Similar to the aggregation, we train the \texttt{SELECT} network using cross entropy loss $L^{sel}$. 
	
	
	\item \textbf{WHERE Clause Pointer Decoder}. Recall from equation \ref{seq2sql-baseline} that this is a model with recurrent connections from its \textit{outputs leading back into its inputs}, and thus a common approach is to train it with \green{teacher forcing}\footnote{Teacher forcing is just a name for how we train the decoder portion of a sequence-to-sequence model, wherein we feed the \underline{ground-truth} output $y^{(t)}$ as input at time $t+1$ during training.}. However, since the boolean expressions within a \texttt{WHERE} clause can be swapped around while still yielding the same SQL query, reinforcement learning (instead of cross entropy) is used to \textit{learn a policy to directly optimize the expected correctness of the execution result}. Note that this also implies that we will be sampling from the output distribution at decoding step $s$ to obtain the next input for $s+1$ [instead of teacher forcing].
	
	\Needspace{19\baselineskip}
	\graybox{
		R\left( q(y), q_g \right) &= 
		\begin{cases}
			-2 & \text{if } q(y) \text{ is not a valid SQL query} \\
			-1 & \text{if } q(y) \text{ is a valid SQL query and executes to an incorrect result} \\
			+1 & \text{if } q(y) \text{ is a valid SQL query and executes to the correct result} 
		\end{cases} \\
		L^{whe} &= - \E[y]{ R\left( q(y), q_g \right) } \\
		\nabla L_{\Theta}^{whe}
		&= -\nabla_{\Theta}  \left( \E[ y \sim p_y ]{ R\left( q(y), q_g \right) } \right) \\
		&= - \E[ y \sim p_y ]{ R\left( q(y), q_g \right) \nabla_{\Theta} \sum_t \log p_y ( y_t ; \Theta ) } \\
		&\approx - R\left( q(y), q_g \right) \nabla_{\Theta} \sum_t   \log p_y ( y_t ; \Theta )
		}
		
	where 
	\begin{compactitem}[$\rightarrow$]
		\item $y = [y^1, y^2, \ldots, y^T]$ denotes the sequences of generated tokens in the \texttt{WHERE} clause.
		
		\item $q(y)$ denotes the query generated by the model.
		
		\item $q_g$ denotes the ground truth query corresponding to the question.
	\end{compactitem}
	and the gradient has been approximated in the last line using a single Monte-Carlo sample $y$. 
\end{enumerate}
Finally, the model is trained using gradient descent to minimize $L = L^{agg} + L^{sel} + L^{whe}$. 

\myspace
\p \blue{Speculations for Event Extraction}. I want to explore using this paper's model for the task of event extraction. Below, I've replaced some words (shown in green) from a sentence in the paper in order to formalize this as event extraction.
\begin{quote}
	{
		\itshape Seq2\green{Event} takes as input a \green{sentence} and the \green{possible event types} of an \green{ontology}. It generates the corresponding \green{event annotation}, which, during training, is \green{compared} against an \green{event template}. The result of the \green{comparison} is utilized to train the reinforcement learning algorithm\footnote{Original: Seq2SQL takes as input a question and the columns of a table. It generates the corresponding SQL query, which, during training, is executed against a database. The result of the execution is utilized as the reward to train the reinforcement learning algorithm.}.
	}
\end{quote}




% ============================================================================================
\lecture{Papers and Tutorials}{SLING: A Framework for Frame Semantic Parsing}{Nov 13, 2017}
% ============================================================================================


\vspace{-1em}
{\footnotesize M. Ringgaard, R. Gupta, F. Pereira, ``SLING: A framework for frame semantic parsing'' (2017)}

\myfig[0.6\textwidth]{sling.png}

\p \blue{Model}. 
\begin{compactitem}
	\item \textbf{Inputs}. [words; affixes; shapes]
	
	\item \textbf{Encoder}. 
	\begin{compactenum}
		\item Embed.
		\item Bidirectional LSTM. 
	\end{compactenum}
	
	\item \textbf{Inputs to TBRU}.
	\begin{compactitem}
			\item BLSTM [forward and backward] hidden state \underline{for the current token in the parser state}.
			
			\item \textbf{Focus}. Hidden layer activations corresponding to the transition steps that evoked/brought into \textit{focus} the top-$k$ frames in the attention buffer.
			
			\item \textbf{Attention}. Recall that we maintain an \green{attention buffer}: an ordered list of frames, where the order represents closeness to center of attention. The attention portion of inputs for the TBRU looks at the top-$k$ frames in the attention buffer, finds the phrases in the text (if any) that evoked them. The activations from the BLSTM for the last token of each of those phrases are included as TBRU inputs\footnote{Okay, how is this attention at all? Seems misleading to call it attention.}

			\item \textbf{History}. Hidden layer activations from the previous $k$ steps.
			
			\item \textbf{Roles}. Embeddings of $(s_i, r_i, t_i)$, where the frame at position $s_i$ in the attention buffer has a role (key) $r_i$ with frame at position $t_i$ as its value. Back-off features are added for the source roles $(s_i, r_i)$, target role $(r_i, t_i)$, and unlabeled roles $(s_i, t_i)$. 
	\end{compactitem}
	
	\item \textbf{Decoder} (TBRU). Outputs a softmax over possible transitions (actions). 
\end{compactitem}

\myspace
\p \blue{Transition System}. Below is the list of possible actions. Note that, since the system is trained to predict the correct \textit{frame graph} result, it isn't directly told what order it should take a given set of actions\footnote{This is important to keep in mind, since more than one sequence of actions can result in a given predicted frame graph.}.
\begin{compactitem}
	\item \textbf{SHIFT}. Move to next input token.
	\item \textbf{STOP}. Signal that we've reached end of parse.
	\item \textbf{EVOKE(type, num)}. New frame of \textbf{type} from next \textbf{num} tokens in the input; placed at front of attention buffer.
	\item \textbf{REFER(frame, num)}. New mention from next \textbf{num} tokens, evoking existing \textbf{frame} from attention buffer. Places at front.
	\item \textbf{CONNECT(source-frame, role, target-frame)}. Inserts \textbf{(role, target-frame)} slot into \textbf{source-frame}, move \textbf{source-frame} to front.
	\item \textbf{ASSIGN(source-frame, role, value)}. Same as CONNECT, but with primitive/constant \textbf{value}.
	\item \textbf{EMBED(target-frame, role, type)}. New frame of \textbf{type}, and inserts \textbf{(role, target-frame)} slot. New frame placed to front.
	\item \textbf{ELABORATE(source-frame, role, type)}. New frame of \textbf{type}. Inserts \textbf{(role, new-frame)} slot to \textbf{source-frame}. New frame placed at front.
\end{compactitem}

\myspace
\p \blue{Evaluation}. Need some way of comparing an annotated document with its gold-standard annotation. This is done by constructing a virtual graph where the document is the start node. It is then connected to the spans (which are presumably nodes themselves), and the spans are connected to the frames they evoke. Frames that refer to other frames are given corresponding edges between them. Quality is computed by aligning the golden and predicted graphs and computing precision, recall, and F1. Specifically, these scores are computed separately for spans, frames, frame types, roles linking to other frames (referred to here as just ``roles''), and roles that link to global constants (referred to here as just ``labels''). Results are shown below.
\myfig[0.6\textwidth]{sling_eval.png}





% ============================================================================================
\lecture{Papers and Tutorials}{Poincar\'{e} Embeddings for Learning Hierarchical Representations}{Nov 16, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize M. Nickel and D. Kiela, ``Poincar\'{e} Embeddings for Learning Hierarchical Representations'' (2017)}


\p \blue{Introduction}. Dimensionality of embeddings can become prohibitively large when needed for complex data. Authors focus on mitigating this problem for large datasets whose objects can be organized according to a latent hierarchy\footnote{This begs the question: how useful would a Poincar\'{e} embedding be for situations where this assumption isn't valid?}. They propose to compute embeddings in a particular model of hyperbolic space, the \red{Poincar\'{e} ball model}, claiming it is well-suited for gradient-based optimization (they make use of \red{Riemannian optimization}).

\myspace
\p \blue{Prerequisite Math}. Recall that a hyperbola is a set of points, such that for any point $P$ of the set, the absolute difference of the distances $|PF_1|$, $|PF_2|$ to two fixed points $F_1$, $F_2$ (the foci), is constant, usually denoted by $2a$, $a > 0$. We can define a hyperbola by this set of points or by its canonical form, which are both given, respectively, as:
\begin{align}
	&H = \{ P | ~ | |PF_2| - |PF_1| | = 2a \} \\
	&\frac{x^2}{a^2} - \frac{y^2}{b^2} = 1
\end{align}
where $b^2 := c^2 - a^2$, $(\pm a, 0)$ are the two vertices, and $(\pm c, 0)$ are the two foci. Cannon et al. define $n$-dimensional hyperbolic space by the formula
\begin{align}
	H^n = \{x   \in \R^{n+1} : ~ x * x = -1   \}
\end{align}
where $*$ denotes the non-euclidean inner product (subtracts last term; same as Minkowski sapce-time). Notice that this is the defining equation for a  \href{http://mathworld.wolfram.com/Two-SheetedHyperboloid.html}{hyperboloid of two sheets}, and Cannon et al. says ``usually we deal only with one of the two sheets.'' Hyperbolic spaces are well-suited to model hierarchical data, since both circle length and disc area grow \textit{exponentially} with $r$.

\myspace
\p \blue{Poincar\'{e} Embeddings}. Let $\mathcal{B}^d = \{\vec x \in \R^d \mid ||\vec x|| < 1\}$ be the open d-dimensional unit ball. The \green{Poincar\'{e} ball} model of hyperbolic space corresponds then to the Riemannian manifold\footnote{All five analytic models of hyperbolic geometry in Cannon et al. are differentiable manifolds with a Riemannian metric. A \green{Riemannian metric} $\mathrm{d}s^2$ on Euclidean space $\R^n$ is a function that assigns at each point $p \in \R^n$ a positive definite symmetric inner product on the tangent space at $p$, this inner product varying differentiably  with the point $p$. If $x_1, \ldots, x_n$ are the standard coordinates in $\R^n$, then $\mathrm{d}s^2$ has the form $\sum_{i,j} g_{ij} \mathrm{d}x_i \mathrm{d}x_j$, and the matrix ($g_{ij}$) depends differentiably on $x$ and is positive definite and symmetric. 
	
} $(\mathcal{B}^d, g_{\vec x})$, where
\begin{align}
	g_{\vec x} &= \left( \frac{2}{1 - ||\vec x||^2} \right)^2 g^E
\end{align}
is the \green{Riemannian metric tensor}, and $g^E$ denotes the Euclidean metric tensor. The distance between two points $\vec u, \vec b \in \mathcal{B}^d$ is given as
\graybox{
	d(\vec u, \vec v) &= \text{arccosh}\left( 1 + 
		2 \frac{   ||\vec u - \vec v ||^2 }{ (1 - ||\vec u ||^2) (1 - ||\vec v||^2)}   \right)
	}
The boundary of the ball corresponds to the sphere $S^{d-1}$ and is denoted by $\partial\mathcal{B}$. Geodesics in $\mathcal{B}^d$ are then circles that are orthogonal to $\partial\mathcal{B}$. To compute Poincar\'{e} embeddings for a set of symbols $\mathcal{S} = \{x_i\}_{i=1}^{n}$, we want to find embeddings $\Theta = \{\vec[i]{\theta}\}_{i=1}^{n}$, where $\vec[i]{\theta} \in \mathcal{B}^d$. Given some loss function $\mathcal{L}$ that encourages semantically similar objects to be close as defined by the Poincar\'{e} distance, our goal is to solve the optimization problem
\graybox{
	\Theta' \leftarrow \argmin_{\Theta} \mathcal{L}(\Theta) \qquad s.t.~ \forall \vec[i]{\theta} \in \Theta: ||\vec[i]{\theta}|| < 1	
}

\myspace
\p \blue{Optimization}. Let $\mathcal{T}_{\theta}\mathcal{B}$ denote the \red{tangent space} of a point $\vec{\theta} \in \mathcal{B}^d$. Let $\nabla_R \in \mathcal{T}_{\vec{\theta}}\mathcal{B}$ denote the \red{Riemannian gradient} of $\mathcal{L}(\vec{\theta})$, and $\nabla_E$ the Euclidean gradient of $\mathcal{L}(\vec{\theta})$. Using \red{RSGD}, parameter updates take the form
\graybox{
	\vec[t + 1]{\theta} &= \mathfrak{R}_{\vec[t]{\theta}} \left(
	 - \eta_t \nabla_R \mathcal{L}(\vec[t]{\theta})	\right)
	}
where $\mathfrak{R}_{\vec[t]{\theta}}$ denotes the \red{retraction} onto $\mathcal{B}$ at $\vec{\theta}$ and $\eta_t$ denotes the learning rate at time $t$. 






% ============================================================================================
\lecture{Papers and Tutorials}{Enriching Word Vectors with Subword Information (FastText)}{Nov 17, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ``Enriching Word Vectors with Subword Information'' (2017)}

\myspace
\p \blue{Overview}. Based on the skipgram model, but where each word is represented as a bag of character $n$-grams. A vector representation is associated \textit{each} character $n$-gram; words being represented as the \textit{sum}\footnote{It would be interesting to explore other aggregation operations than just summation.} of these representations.

\myspace
\p \blue{Skipgram with Negative Sampling}. Since this is based on skipgram, recall the objective of skipgram which is to maximize:
\begin{align}
	\sum_{t = 1}^T \sum_{c \in \mathcal{C}_t} \log \Prob{w_c \mid w_t}
\end{align}
for a sequence of words $w_1, \ldots w_T$. One way of parameterizing $\Prob{w_c \mid w_t}$ is by computing a softmax over a scoring function $s: (w_t, w_c) \mapsto \R$,
\begin{align}
	\Prob{w_c \mid w_t} &= \frac{
			e^{s(w_t, w_c)}  }{
			\sum_{j = 1}^{W} e^{s(w_t, j)}
			}
\end{align}
However, this implies that, given $w_t$, we only predict one context word $w_c$. Instead, we can frame the problem as a set of independent binary classification tasks, and independently predict the presence/absence of context words. Let $\ell: x \mapsto \log(1 + e^{-x})$ denote the standard logistic negative log-likelihood. Our objective is:
\begin{align}
	\sum_{t = 1}^T \left[
		\sum_{c \in \mathcal{C}_t} \ell(s(w_t, w_c)) + \sum_{n \in \mathcal{N}_{t,c}} \ell(-s(w_t, n))
	\right]
\end{align}
where $\mathcal{N}_{t,c}$ is a set of negative examples sampled from the vocabulary. A common scoring function involves associating a distinct input vector $u_w$ and output vector $v_w$ for each word $w$. Then the score is computed as $s(w_t, w_c) = \vec[w_t]{u}^T \vec[w_c]{v}$. 

\myspace
\p \blue{FastText}. Main contribution is a different scoring function $s$ that utilizes subword information. Each word $w$ is represented as a bag of character $n$-grams. Special symbols $<$ and $>$ delimit word boundaries, and the authors also insert the special sequence containing the full word (with the delimiters) in its bag of $n$-grams. The word \textit{where} is thus represented by first building its bag of $n$-grams, for the choice of $n=3$:
\begin{align}
	\text{where} ~ \longrightarrow ~  \{    
		<wh,~~ whe,~~ her,~~ ere,~~ re>,~~ <where>
	\}
\end{align}
Such a set of $n$-grams for a word $w$ is denoted $\mathcal{G}_w$. Each $n$-gram $g$ for a word $w$ has its own vector $\vec[g]{z}$, and the final vector representation of $w$ is the sum of these. The scoring function becomes
\graybox{
	s(w, c) &= \sum_{g \in \mathcal{G}_w} \vec[g]{z}^T \vec[c]{v}
	}





% ============================================================================================
\lecture{Papers and Tutorials}{DeepWalk: Online Learning of Social Representations}{Nov 17, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize B. Perozzi, R. Al-Rfou, and S. Skiena, ``DeepWalk: Online Learning of Social Representations,'' (2014).}

\p \blue{Problem Definition}. Classifying members of a social network into one or more categories. 
\vspace{-1em}
\begin{quote}
	{\itshape Let $G = (V, E)$, where $V$ are the members of the network, and $E$ be its edges, $E \subseteq (V \times V)$. Given a partially labeled social network $G_L = (V, E, X, Y)$, with attributes $X \in \R^{|V| \times S}$ where $S$ is the size of the feature space for each attribute vector, and $Y \in \R^{|V| \times |\mathcal{Y}|}$, $\mathcal{Y}$ is the set of labels.}
\end{quote}
\vspace{-1em}
In other words, the elements of our training dataset, $(X, Y)$, are the members of the social network, and we want to label each member, represented by a vector in $\R^S$, with one or more of the $|\mathcal{Y}|$ labels. We aim to learn features that capture the graph structure \textit{independent} of the labels' distribution, and to do so in an unsupervised fashion.

\myspace
\p \blue{Learning Social Representations}. We want the representations to be adaptable, community-aware, low-dimensional, and continuous. The authors' method learns representations for vertices from a stream of short random walks, optimized with techniques from language modeling.
\begin{compactitem}
	\item \textbf{Random Walks}. Denote a random walk rooted at vertex $v_i$ as $\mathcal{W}_{v_i}$, where the $k$th visited vertex is chosen at random from the neighbors of the $(k-1)^{th}$ visited vertex, and so on. Motivation for their use here is that they're ``the foundation of a class of \textit{output sensitive} algorithms which use them to compute local community structure information in time sublinear to the size of the input graph.''
	
	\item \textbf{Language Modeling}. Authors present a generalization of language modeling, which traditionally aims to maximize $\Prob{w_n \mid w_0, \ldots, w_{n-1}}$ over all words in a training corpus. The motivation of the generalization is to explore the graph through a stream of short random walks. The walks are thought of as short sentences/phrases in a special language, and we want to estimate the probability of observing vertex $v_i$ given all previous vertices so far in the random walk. Since we want to learn a latent social representation of each vertex, and not simply a probability distribution over node co-occurrences, we condition on the \textit{embeddings} of visited nodes in this latent space (rather than the nodes themselves directly)
	\begin{align}
		\Prob{v_i \mid \Phi(v_1), \Phi(v_2), \ldots, \Phi(v_{i - 1})} 
	\end{align}
	where, in practice, the mapping function $\Phi$ is represented by a $|V| \times d$ matrix of free parameters (an embedding matrix). Since this becomes infeasible to compute as the walk length grows, the authors opt for an approach resembling CBOW: minimizing the NLL of vertices in the the context of a given vertex.
	\begin{align}
		\min_{\Phi}~~ - \log \Prob{v_{i-w}, \ldots, v_{i-1}, v_{i+1}, \ldots, v_{i+w} \mid \Phi(v_i) }
	\end{align}
	Remember that, here, $v_j$ is the $j$th vertex visited in some given random walk. 
\end{compactitem}

\myspace
\p \blue{DeepWalk Algorithm}. Below is a conceptual summary of the procedure, followed by a figure/illustration of the formal algorithm definition.
\begin{compactenum}
	\item \textbf{Inputs}. Graph $G(V, E)$, window size $w$, embedding size $d$, walks per vertex $\gamma$, walk length $t$. 
	
	\item \textbf{Random Walk}. For each vertex $v_i$, compute $\mathcal{W}_{v_i} := RandomWalk(G, v_i, t)$. 
	
	\item \textbf{Updates}. Upon finishing a walk, $\mathcal{W}_{v_i}$, run skipgram on the sequence of walked vertices to update the embedding matrix $\Phi$. 
	
	\item \textbf{Outputs}. The embedding matrix $\Phi \in \R^{|V| \times d}$. 
\end{compactenum}

\myfig[0.6\textwidth]{deepwalk_algo.png}

where $SkipGram(\Phi, W_{v_i}, w)$ performs SGD updates on $\Phi$ to minimize $-\log\Prob{u_k \mid \Phi(v_j)}$ for each visited $v_j$, for each $u_k$ in the ``context'' of $v_j$. Notice that a binary tree $T$ is build from the set of vertices $V$ (line 2) -- this is done as preparation for computing each $\Prob{u_k \mid \Phi(v_j)}$ via a \textit{hierarchical softmax}, to reduce computational burden of its partition function. Finally, a visual overview of the DeepWalk algorithm is shown below.
\myfig[0.8\textwidth]{deepwalk_vis.png}
The authors use this algorithm, combined with a one-vs-rest logistic regression implementation by LibLinear, for various multiclass multilabel classification tasks.  





% ============================================================================================
\lecture{Papers and Tutorials}{Review of Relational Machine Learning for Knowledge Graphs}{Dec 6, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize Nickel, Murphy, Tresp, and Gabrilovich, ``Review of Relational Machine Learning for Knowledge Graphs,'' (2015).}

\p \blue{Introduction}. Paper discusses latent feature models such as tensor factorization and multiway neural networks, and mining observable patterns in the graph. In \green{Statistical Relational Learning (SRL)}, the representation of an object can contain its relationships to other objects. The main goals of SRL include:
\begin{compactitem}
	\item Prediction of missing edges (relationships between entities).
	\item Prediction of properties of nodes.
	\item Clustering nodes based on their connectivity patterns.
\end{compactitem}
We'll be reviewing how SRL techniques can be applied to large-scale \green{knowledge graphs} (KGs), i.e. graph structured knowledge bases (KBs) that store factual information in the form of relationships between entities.

\myspace
\p \blue{Probabilistic Knowledge Graphs}. Let $\mathcal{E} = \{e_1, \ldots, e_{N_e} \}$ be the set of all entities and $\mathcal R = \{r_1, \ldots, r_{N_r}\}$ be the set of all relation types in a KG. We model each \textit{possible} triple $x_{ijk} = (e_i, r_k, e_j)$ as a binary random variable $y_{ijk} \in \{0, 1\}$ that indicates its existence. The full tensor $\matr Y \in \{0, 1\}^{N_e \times N_e \times N_r}$ is called the \textit{adjacency tensor}, where each possible realization of $\matr Y$ can be interpreted as a possible world.\\

\p Clearly, $\matr Y$ will be large and sparse in most applications. Ideally, a relational model for large-scale KGs should scale at most linearly with the data size, i.e., linearly in the number of entities $N_e$, linearly in the number of relations $N_r$, and linearly in the number of \textit{observed} triples $|\mathcal D| = N_d$.

\myspace
\p \blue{Types of SRL Modesls}. The presence or absence of certain triples in relational data is correlated with (i.e. predictive of) the presence or absence of certain other triples. In other words, the random variables $y_{ijk}$ are correlated with each other. There are three main ways to model these correlations:
\begin{compactenum}
	\item \textbf{Latent feature models}: Assume all $y_{ijk}$ are conditionally independent given latent features associated with the subject, object and relation type and additional parameters.
	
	\item \textbf{Graph feature models}: Assume all $y_{ijk}$ are conditionally independent given observed graph features and additional parameters.
	
	\item \textbf{Markov Random Fields}: Assume all $y_{ijk}$ have local interactions.
\end{compactenum}
The first two model classes predict the existence of a triple $x_{ijk}$ via a score function $f(x_{ijk}; \Theta)$ which represents the model's confidence that a triple exists given the parameters $\Theta$. The conditional independence assumptions can be written as
\graybox{
	\Prob{\matr Y \mid \mathcal D, \Theta}
	&= \prod_{i=1}^{N_e} \prod_{j=1}^{N_e} \prod_{k=1}^{N_r} \text{Ber}\left(
		y_{ijk} \mid \sigma\left(  f(x_{ijk}; \Theta)  \right)  \right)
}
where $\text{Ber}$ is the Bernoulli distribution\footnote{Notation used:\begin{align}
	\text{Ber}(y\mid p) = \begin{cases}
		p & \text{if } y = 1 \\
		1 - p & \text{if } y = 0
	\end{cases}
\end{align}}. Such models will be referred to as \textit{probabilistic models}. We will also discuss \textit{score-based models}, which optimize $f(\cdot)$ via maximizing the margin between existing and non-existing triples.

\myspace
\p \blue{Latent Feature Models}. We assume the variables $y_{ijk}$ are conditionally independent given a set of global latent features and parameters. All LFMs explain triples (observable facts) via latent features of entities\footnote{It appears that ``latent'' is being used here synonymously with "not directly observed in the data".}. One task of all LFMs is to infer these [latent] features automatically from the data.
\begin{itemize}
	\item \textbf{RESCAL}: a bilinear model. Models the score of a triple $x_{ijk}$ as 
	\begin{align}
		f_{ijk}^{RESCAL}
			&:= \vec[i]{e}^T \matr[k]{W} \vec[j]{e} = \sum_{a = 1}^{H_e} \sum_{b = 1}^{H_e} w_{abk} e_{ia} e_{jb}
	\end{align}
	where the entity vectors $\vec[i]{e} \in \R^{H_e}$ and $H_e$ denotes the number of latent features in the model. The parameters of the model are $\Theta = \{ \{  \vec[i]{e} \}_{i=1}^{N_e}, \{ \matr[k]{W} \}_{k = 1}^{N_r} \}$. Note that entities have the same latent representation regardless of whether they occur as subjects or objects in a relationship (shared representation), \textit{thus allowing the model to capture global dependencies in the data}. We can make a connection to \green{tensor factorization} methods by seeing that the equation above can be written compactly as 
	\begin{align}
		\matr[k]{F} &= \matr{E} \matr[k]{W} \matr{E}^T
	\end{align}
	where $\matr[k]{F} \in \R^{N_e \times N_e}$ is the matrix holding all scores for the $k$-th relation, and the $i$th row of $\matr{E} \in \R^{N_e \times H_e}$ holds the latent representation of $\vec[i]{e}$.
	
	\item \textbf{Multi-layer perceptrons}. We can rewrite RESCAL as
	\begin{align}
		f_{ijk}^{RESCAL} &:= \vec[k]{w}^T \vec[i,j]{\phi}^{RESCAL} \\
		\vec[i,j]{\phi}^{RESCAL} &:= \vec[j]{e} \otimes \vec[i]{e}
	\end{align}
	where $\vec[k]{w} = \text{vec}(\matr[k]{W})$ (vector of size $H_e^2$ obtained by stacking columns of $\matr[k]{W}$). The authors extend this to what they call the E-MLP (E for entity) model:
	\begin{align}
		f_{ijk}^{E-MLP} &:= \vec[k]{w}^T \vec{g}(\vec[ijk]{h}^a) \\
		\vec[ijk]{h}^a &:= \matr[k]{A}^T \vec[ij]{\phi}^{E-MLP} \\
		\vec[ij]{\phi}^{E-MLP} &:= [\vec[i]{e}; \vec[j]{e}]
	\end{align}
\end{itemize}


\myspace
\p \blue{Graph Feature Models}. Here we assume that the existence of an edge can be predicted by extracting features from the observed edges in the graph. In contrast to LFMs, this kind of reasoning explains triples directly from the observed triples in the KG.
\begin{compactitem}[-]
	\item \textbf{Similarity measures for uni-relational data}. Link prediction in graphs that consist only of a single relation (e.g. (Bob, isFriendOf, Sally) for a social network). Various \green{similarity indices} have been proposed to measure similarity of entities, of which there are three main classes:
	\begin{compactenum}
		\item \textbf{Local} similarity indices: Common Neighbors, Adamic-Adar index, Preferential Attachment derive entity similarities from number of common neighbors.
		
		\item \textbf{Global} similarity indices: Katz index, Leicht-Holme-Newman index (ensembles of all paths bw entities). Hitting Time, Commute Time, PageRank (random walks). 
		
		\item \textbf{Quasi-local} similarity indices: Local Katz, Local Random Walks.
	\end{compactenum}
	
	\item \textbf{Path Ranking Algorithm} (\green{PRA}): extends the idea of using random walks of bounded lengths for predicting links in multi-relational KGs. Let $\pi_L(i,j,k,t)$ denote a path of length $L$ of the form  $e_i \xrightarrow{r_1} e_2 \xrightarrow{r_2} e_3 \cdots \xrightarrow{r_L} e_j$, where $t$ represents the sequence of edge types $t = (r_1, r_2, \ldots, r_L)$. We also require there to be a direct arc $e_i \xrightarrow{r_k} e_j$, representing the existence of a relationship of type $k$ from $e_i$ to $e_j$. Let $\Pi_L(i,j,k)$ represent the \underline{set} of all such paths of length $L$, ranging over path types $t$. \\
	
	\p We can compute the probability of following a given path by assuming that at each step we follow an outgoing link uniformly at random. Let $\Prob{\pi_L(i,j,k,t)}$ be the probability of the path with type $t$. \textit{The key idea in PRA is to use these path probabilities as features for predicting the probabilities of missing edges}. More precisely, the feature vector and score function (logistic regression) are as follows:
	\graybox{
		\vec[ijk]{\phi}^{PRA} &= [\Prob{\pi}: \pi \in \Pi_L(i,j,k)] \\
		f_{ijk}^{PRA} &:= \vec[k]{w}^T \vec[ijk]{\phi}^{PRA}	
	}
\end{compactitem}


\red{TODO}: Finish...


% ============================================================================================
\lecture{Papers and Tutorials}{Fast Top-K Search in Knowledge Graphs}{Dec 6, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize S. Yang, F. Han, Y. Wu, X. Yan, ``Fast Top-K Search in Knowledge Graphs.''}

\p \blue{Introduction}. Task: Given a knowledge graph $G$, scoring function $F$, and a graph query $Q$, top-k subgraph search over $G$ returns $k$ answers with the highest matching scores. An example is searching for movie makers (directors) worked with ``Brad'' and have won awards, illustrated below:
\myfig[0.4\textwidth]{TopKQuery.png}

Clearly, it would be extremely inefficient to enumerate all possible matches and then rank them.

\myspace
\p \blue{Preliminaries/Terminology}.
\begin{compactitem}
	\item \textbf{Queries}. A query [graph] is defined as $Q = (V_Q, E_Q)$. Each query node in $Q$ provides information/constraints about an entity, and an edge between two nodes specifies the relationship or the connectivity constraint posed on the two nodes. $Q*$ denotes a star-shaped query, which is basically a graph that looks like a star (central node with tree-like structure radially outward). 
	
	\item \textbf{Subgraph Matching}. Given a graph query $Q$ and a knowledge graph $G$, a \underline{match} $\phi(Q)$ of $Q$ in $G$ is a subgraph of $G$, specified by a one-to-one matching function $\phi$. It maps each node $u$ (edge $e=(u, u')$) in $Q$ to a node match $\phi(u)$ (edge match $\phi(e)=(\phi(u), \phi(u'))$) in $\phi(Q)$. \\
	
	\p The matching score between query $Q$ and its match $\phi(Q)$ is
	\begin{align}
		F(\phi(Q)) &= \sum_{v \in V_Q} F_V(v, \phi(v)) + \sum_{e \in E_Q} F_E(e, \phi(e)) \\
		F_V(v, \phi(v)) &= \sum_i \alpha_i f_i(v, \phi(v)) \\
		F_E(e, \phi(e)) &= \sum_j \beta_j f_j(e, \phi(e))
	\end{align}
\end{compactitem}

\myspace
\p \blue{Star-Based Top-K Matching}. 
\begin{compactenum}
	\item \textbf{Query decomposition}: Given query $Q$, STAR decomposes $Q$ to a set of star queries $\mathcal Q$. A star query contains a pivot node and a set of leaves as its neighbors in $Q$. 
	
	\item \textbf{Star querying engine}: Generate a set of top matches for each star query $\mathcal Q$. 
	
	\item \textbf{Top-k rank join}. The top matches for multiple star queries are collected and joined to produce top-k complete matches of $Q$. 
\end{compactenum}


\begin{comment}


% ============================================================================================
\lecture{Papers and Tutorials}{Generalized Neural Graph Embedding with Matrix Factorization}{Nov 17, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize J. Guo, L. Xu, X. Huang, E. Chen, ``Generalized Neural Graph Embedding with Matrix Factorization,'' (2017).}

\end{comment}









\begin{comment}
% ============================================================================================
\lecture{Papers and Tutorials}{Fast Linear Model for Knowledge Graph Embeddings}{Nov 17, 2017}
% ============================================================================================
\vspace{-1em}
{\footnotesize A. Joulin, E. Grave, P. Bojanowski, M. Nickel, and T. Mikolov, ``Fast Linear Model for Knowledge Graph Embeddings,'' (2017).}
\end{comment}



% ============================================================================================
\lecture{Papers and Tutorials}{Dynamic Recurrent Acyclic Graphical Neural Networks (DRAGNN)}{Jan 19, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Kong et al., ``DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks,'' (2017).}

\p \blue{Transition Systems}. Define a transition system $\mathcal{T} \triangleq \{ \mathcal{S}, \mathcal{A}, t \}$, where
\begin{compactitem}
	\item $\mathcal{S} = \mathcal{S}(x)$ is a set of states, where that set depends on the input sequence $x$.
	\item A special start state $s^{\dagger} \in \mathcal{S}(x)$.
	\item A set of allowed decisions $\mathcal{A}(s, x) ~ \forall s \in \mathcal{S}(x)$.
	\item A transition function $t(s, d, x)$ returning a new state $s'$ for any decision $d \in \mathcal{A}(s, x)$.  
\end{compactitem}
The authors then define a \green{complete structure} as a sequence of state/decision pairs $(s_1, d_1)\ldots(s_n, d_n)$ such that $s_1 = s^{\dagger}$, $d_i \in \mathcal{A}(s_i)$ for $i = 1, \ldots, n$ and $s_{i+1} = t(s_i, d_i)$, where $n = n(x)$ is the number of decisions for input x\footnote{The authors state that we are only concerned with complete structures that have the same number of decisions $n(x)$ for the same input x.}. We'll use transition systems to map inputs $x$ into a sequence of output symbols $d_1, \ldots, d_n$. 

\myspace
\p \blue{Transition Based Recurrent Networks}. When combining transition systems with recurrent networks, we will refer to them as \green{Transition Based Recurrent Units (TBRU)}, which consist of:
\begin{compactitem}
	\item Transition system $\mathcal{T}$. 
	\item Input function $\vec{m}(s): \mathcal{S} \mapsto \R^K$ that maps states to some fixed-size vector representation (e.g. an embedding lookup operation). 
	\item Recurrence function $\vec{r}(s) : \mathcal{S} \mapsto \mathbb{P}\{ 1, \ldots, i - 1 \}$ that maps states to a set of previous time steps, where $\mathbb{P}$ is the power set. Note that $|\vec{r}(s)|$ may vary with $s$. We use $\vec{r}$ to specify state-dependent recurrent links in the unrolled computation graph. 
	\item The RNN cell $\vec[s]{h} \leftarrow \mathbf{RNN}(\vec{m}(s), \{ \vec[i]{h} \mid i \in \vec{r}(s) \})$. 
\end{compactitem}

\myspace
\p \blue{Example: Sequential tagging RNN}. Let $\vec{x} = \{ x_1, \ldots, x_n \}$ be a sequence of input tokens. Let the $i$th output, $d_i$, be a tag from some predefined set of tags $\mathcal{A}$. Then our model can be defined as:
\begin{compactitem}
	\item Transition system: $\mathcal{T} = \{~  s_i\hspace{-0.5ex}=\hspace{-0.5ex}\mathcal{S}(x_i)\hspace{-0.5ex}=\hspace{-0.5ex}\{1,\ldots,d_{i-1} \}, ~~ \mathcal{A}, ~~ t(s_i, d_i, x_i)\hspace{-0.5ex}=\hspace{-0.5ex}s_{i+1}\hspace{-0.5ex}=s_i\hspace{-0.5ex}+\hspace{-0.5ex}\{d_i\}  ~ \}$. 
	\item Input function:  $\vec{m}(s_i) = embed(x_i)$.
	\item Recurrence function: $\vec{r}(s_i) = \{i - 1\}$ to connect the network to the previous state. 
	\item RNN cell: $\vec[i]{h} \leftarrow LSTM(\vec{m}(s_i) \mid \vec{r}(s_i)=\{i-1\}  )$. 
\end{compactitem}

\myspace
\p \blue{Example: Parsey McParseface}. 
\begin{compactitem}
	\item Transition system: the \green{arc-standard} transition system, defined in image below\footnote{Image taken from ``Transition-Based Parsing'' by Joakim Nivre. Note that ``right-headed'' means ``goes from left to right'' or ``headed \textit{to} the right''.}. 
		\myfig[0.6\textwidth]{figs/arc_standard_alg.png}
		
		so the state contains all words and partially built trees (stack) as well as unseen words (buffer). 
		
	\item Input function: $\vec{m}(s_i)$ is the concatenation of 52 feature embeddings extracted from tokens based on their positions in the stack and the buffer.
	\item Recurrence function: $\vec{r}(s_i)$ is empty, as this is a feed-forward network. 
	\item RNN cell: a feed-forward MLP (so not an RNN...). 
\end{compactitem}

\myspace
\p \blue{Inference with TBRUs}. To predict the output sequence $\{d_1, \ldots, d_n\}$ given input sequence $\vec{x} = \{x_1, \ldots, x_n\}$, do:
\begin{compactenum}
	\item Initialize $s_1 = s^{\dagger}$. 
	\item For $i = 1, \ldots, n$:
	\begin{compactenum}
		\item Compute $\vec[i]{h} = \mathbf{RNN}(\vec{m}(s_i), \{  \vec[j]{h} \mid j \in \vec{r}(s_i)    \}  )$. 
		
		\item Update transition state:
		\begin{align}
			d_i &\leftarrow \argmax_{d \in \mathcal{A}(s_i)} \vec[d]{w}^T \vec[i]{h} \\
			s_{i+1} &\leftarrow t(s_i, d_i)
		\end{align}
		\red{NOTE}: This defines a locally normalized training procedure, whereas Andor et al. of Syntaxnet clearly conclude that their globally normalized model is the preferred choice.
	\end{compactenum}
\end{compactenum}


\myspace
\p \blue{Combining multiple TBRUs}. We connect multiple TBRUs with different transition systems via $\vec{r}(s)$. 
\begin{compactenum}
	\item We execute a list of $T$ TBRU components sequentially, so that each TBRU advances a global step counter.
	\item \textit{Each} transition state, $s^{\tau}$, from the $\tau$'th component has access the \textit{terminal} states from every prior transition system, and the recurrence function $\vec{r}(s^{\tau})$ for any given component can pull hidden activations from every prior one as well.
\end{compactenum}

\myspace
\p \blue{Example: Multi-task bi-directional tagging}. Say we want to do both POS and NER tagging (indices start at 1). 
\begin{compactitem}
	\item Left-to-right: $\mathcal{T} = $ \textit{shift-only}, $\vec{m}(s_i) = \vec[i]{x}$, $\vec{r}(s_i) = \{i - 1\}$. 
	\item Right-to-left: $\mathcal{T} =$ \textit{shift-only}, $\vec{m}(s_{n+i}) = \vec[(n-i)+1]{x}$, $\vec{r}(s_{n+i}) = \{n + i - 1\}$. 
	\item POS Tagger: $\mathcal{T}_{POS} =$ \textit{tagger}, $\vec{m}(s_{2n+i}) = \{\}$, $\vec{r}(s_{2n+i}) = \{i, (2n - i) + 1\}$
	\item NER Tagger: $\mathcal{T}_{NER} =$ \textit{tagger}, $\vec{m}(s_{3n+i}) = \{\}$, $\vec{r}(s_{3n+i}) = \{i, (2n - i) + 1, 2n + i \}$
\end{compactitem}
which illustrates the most important aspect of the TBRU:
\vspace{-0.8em}
\begin{quote}
	{\itshape 
		A TBRU can serve as both an encoder for downstream tasks and a decoder for its own task simultaneously.
	}
\end{quote}
For this example, the POS Tagger served as both an encoder for the NER task as well as a decoder for the POS task.

\myspace
\p \blue{Training a DRAGNN}. Assume training data consists of examples $\vec{x}$ along with gold decision sequences for a given TBRU in the DRAGNN. Given decisions $d_1, \ldots, d_N$ from prior components $1, \ldots, T-1$, the log-likelihood for training the $T$'th TBRU along its gold decision sequence $d_{N+1}^{\star}, \ldots, d_{N+n}^{\star}$ is then:
\graybox{
	L(\vec{x}, ~ d_{N + 1 : N+n}^{\star} ; ~ \theta )	
	&= \sum_{i} \log \Prob{
			d_{N+i}^{\star} \mid d_{1:N}, ~ d_{N+1:N+i-1}^{\star}; ~ \theta
		}
}
During training, the entire input sequence is unrolled and \green{backpropagation through structure} is used for gradient computation.

\myspace\Needspace{18\baselineskip}
\subsub{More Detail: Arc-Standard Transition System}

The arc-standard transition system is mentioned a lot, but with little detail. Here I'll synthesize what I find from external resources. The literature defines the states in a transition system slightly differently than the DRAGNN paper. Here we'll define them as a \green{configuration} $c = (\Sigma, B, A)$ triplet, where
\begin{compactitem}
	\item $\Sigma$ is the \textbf{stack} of tokens in $x$ that we've [partially] processed.
	\item $B$ is the \textbf{buffer} of remaining tokens in $x$ that we need to process. 
	\item $A$ is a set of \textbf{arcs} $(w_i, w_j, \ell)$ that link $w_i$ to $w_j$, and label the arc/link as $\ell$. 
\end{compactitem}
So, in the arc-standard transition system figure presented with Parsey McParseface earlier, 
\begin{compactitem}
	\item SHIFT just means ``move the head element of the buffer to the tail element of the buffer''. 
	
	\item Left-arc just means ``make a link \textit{from} the tail element of the stack \textit{to} the element before it. Remove the pointed-to element from the stack.''
	
	\item Right-arc just means ``make a link \textit{from} the element before the tail element in the stack \textit{to} the tail element. Remove the pointed-to element from the stack.''
\end{compactitem}


% ============================================================================================
\lecture{Papers and Tutorials}{Neural Architecture Search with Reinforcement Learning}{April 01, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize B. Zoph and Q. Le, ``Neural Architecture Search with Reinforcement Learning,'' (2017).}

\myfig[0.4\textwidth]{figs/nas.png}

\p \blue{Controller RNN}. Generates architectures with a predefined number of layers, which is increased manually as training progresses. At convergence, validation accuracy of the generated network is recorded. Then, the controller parameters $\theta_c$ are optimized to maximized the expected validation accuracy over a batch of generated architectures. 

\myspace
\p \blue{Reinforcement Learning} to learn the controller parameters $\theta_c$. Let $a_{1:T}$ denote a list of actions taken by the controller\footnote{Note that $T$ is not necessarily the number of layers, since a single generated layer can correspond to multiple actions (e.g. stride height, stride width, num filters, etc.).}, which defines a generated architecture. We denote the resulting validation accuracy by $R$, which is the reward signal for our RL task. Concretely, we want our controller to maximize its expected reward, $J(\theta_c)$:
\graybox{
	J(\theta_c) &= \E[P(a_{1:T};~ \theta_c)]{R}	
}
Since the quantity $\nabla_{\theta_c} R$ is non-differentiable\footnote{$R$ is a function of the action sequence $a_{1:T}$ and the parameters $\theta_c$, and implicitly depends on the samples used for the validation set. Clearly, we do not have access to an analytical form of $R$, and computing numerical gradients via small perturbations of $theta_c$ is computationally intractable.}, we use a \green{policy gradient} method to iteratively update $\theta_c$. All this means is that we instead compute gradients over the softmax outputs (the action probabilities), and use the value of $R$ as a simple weight factor.
\begin{align}
	\nabla_{\theta_c} J(\theta_c) 
	&= R \sum_{t = 1}^{T}   \E[P(a_{1:T};~ \theta_c)]{ \nabla_{\theta_c} \log P(a_t \mid a_{1:t-1}; \theta_c) }	\label{nas_1} \\
	&\approx \inv{m} \sum_{k = 1}^{m} R_k \sum_{t = 1}^{T} \nabla_{\theta_c} \log P(a_t \mid a_{1:t-1}; \theta_c) \label{nas_2}
\end{align}
where the second equation is the empirical approximation (batch-average instead of expectation) over a batch of size $m$, an unbiased estimator for our gradient\footnote{It is unbiased for the same reason that any average over samples $x$ drawn from a distribution $P(x)$ is an unbiased estimator for $\E[P]{x}$.}. Also note that we \textit{do} have access to the distribution $P(a_{1:T}; \theta_c)$ since it is \textit{defined} to be the joint softmax probabilities of our controller, given its parameter values $\theta_c$ (i.e. this is not a $p_{data}$ vs $p_{model}$ situation). The approximation \ref{nas_1} is an unbiased estimator for \ref{nas_2}, but has high variance. To reduce the variance of our estimator, the authors employ a baseline function $b$ that does not depend on the current action:
\begin{align}
 \inv{m} \sum_{k = 1}^{m} \sum_{t = 1}^{T} \nabla_{\theta_c} \log P(a_t \mid a_{1:t-1}; \theta_c) (R_k - b)
\end{align}




% ============================================================================================
\lecture{Papers and Tutorials}{Joint Extraction of Events and Entities within a Document Context}{April 26, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize B. Yang and T. Mitchell, ``Joint Extraction of Events and Entities within a Document Context,'' (2016).}

\p \blue{Introduction}. Two main reasons that state-of-the-art event extraction systems have difficulties: 
\begin{compactenum}
	\item They extract events and entities in separate stages. 
	\item They extract events independently from each individual sentence, ignoring the rest of the document. 
\end{compactenum}
This paper proposes an approach that simultaneously extracts events and entities within a document context. They do this by first decomposing the problem into 3 tractable subproblems:
\begin{compactenum}
	\item Learning the dependencies between a single event [trigger] and all of its potential arguments.
	\item Learning the co-occurrence relations between events across the document.
	\item Learning for entity extraction.
\end{compactenum} 
and then combine these learned models into a joint optimization framework. 

\myspace
\p \blue{Learning Within-Event Structures}. For now, assume we have some document $x$, a set of candidate event triggers $\mathcal{T}$, and a set of candidate entities $\mathcal{N}$. Denote the set of entity candidates that are potential arguments for trigger candidate $i$ as $\mathcal{N}_i$. The joint distribution over the possible trigger types, roles, and entities for those roles, is given by\marginnote{All $f_i$ also depend on $i, x$. In addition, all $f_i$ except $f_1$ depend on the current $j$ in the summation.}[2em]
\graybox{
	&\Prob[\vec{\theta}]{ t_i, \vec[i]{r}, \vec{a} \mid i, \mathcal{N}_i, x }
	\propto \\
	&\exp\left( \vec[1]{\theta}^T \mgreen{f_1}(t_i) +
	\sum_{j \in \mathcal{N}_i} \left[ \vec[2]{\theta}^T \mgreen{f_2}(r_{ij}) +
	\vec[3]{\theta}^T f_3 (t_i, r_{ij}) +
	\vec[4]{\theta}^T \mgreen{f_4} (a_j) + 
	\vec[5]{\theta}^T f_5 (r_{ij}, a_j)\right] \right) 
}

where each $f_i$ is a feature function, and I've colored the unary feature functions green. The unary features are tabulated in Table 1 of the original paper. They use simple indicator functions $1_{t,r}$ and $1_{r, a}$ for the pairwise features. They train using maximum-likelihood estimates with L2 regularization:
\begin{align}
	\vec{\theta}^* &= \argmax_{\vec\theta} \mathcal{L}(\vec\theta) - \lambda ||\vec\theta||_2^2 \\
	\mathcal{L}(\vec\theta) &= \sum_i \log \left( \Prob[\vec{\theta}]{ t_i, \vec[i]{r}, \vec{a} \mid i, \mathcal{N}_i, x } \right)
\end{align}
and use \green{L-BFGS} to optimize the training objective. 

\myspace
\p \blue{Learning Event-Event Relations}. A pairwise model of event-event relations in a document. Training data consists of all pair of trigger candidates that co-occur in the same sentence or are connected by a co-referent subject/object if they're in different sentences. Given a trigger candidate pair $(i, i')$, we estimate the probabilities for their event types $(t_i, t_{i'})$ as
\graybox{
	\Prob[\phi]{t_i, t_{i'} \mid x, i, i'} \propto \exp \left( 
		\phi^T g(t_i, t_{i'}, x, i, i')
	\right)
}
where $g$ is a feature function that depends on the trigger candidate pair and their context. In addition to re-using the trigger features in Table 1 of the paper, they also introduce relational trigger features:
\begin{compactenum}
	\item whether they're connected by a conjunction dependency relation
	\item whether they share a subject or an object
	\item whether they have the same head word lemma
	\item whether they share a semantic frame based on FrameNet. 
\end{compactenum}
As before, they using L-BFGS to compute the maximum-likelihood estimates of the parameters $\phi$. 


\myspace
\p \blue{Entity Extraction}. Trained a standard linear-chain \green{CRF} using the BIO scheme. Their CRF features:
\begin{compactenum}
	\item current words and POS tags
	\item context words in a window of size 2
	\item word type such as all-capitalized, is-capitalized, all-digits
	\item Gazetteer-based entity type if the current word matches an entry in the gazetteers collected from Wikipedia. 
	\item pre-trained word2vec embeddings for each word
\end{compactenum}

\myspace
\p \blue{Joint Inference}. Allows information flow among the 3 local models and finds globally-optimal assignments of all variables. Define the following objective:
\graybox{
	\max_{ \vec t, \vec r, \vec a} \sum_{i \in \mathcal{T}} E(t_i, \vec[i]{r}, \vec{a})
	+ \sum_{i, i' \in \mathcal{T}} R(t_i, t_{i'})
	+ \sum_{j \in \mathcal{N}} D(a_j)	
}
where
\begin{compactitem}
	\item The first term is the sum of confidence scores for individual event mentions from the within-event model. 
	\begin{align}
		E(t_i, \vec[i]{r}, \vec{a})
		= \log p_{\theta}(t_i) + \sum_{j \in \mathcal{N}_i} \left[ \log p_{\theta}(t_i, r_{ij}) + \log p_{\theta}(r_{ij}, a_j) \right]
	\end{align}
	
	\item The second term is the sum of confidence scores for event relations based on the pairwise event model.
	\begin{align}
		R(t_i, t_{i'}) = \log p_{\phi}(t_i, t_{i'} \mid i, i', x)
	\end{align}
	
	\item The third term is sum of confidence scores for entity mentions, where
	\begin{align}
		D(a_j) = \log p_{\psi}(a_j \mid j, x)
	\end{align}
	and $p_{\psi}(a_j \mid j, x)$ is the marginal probability derived from the linear-chain CRF. 
\end{compactitem}
The optimization is subject to agreement constraints that enforce the overlapping variables among the 3 components to agree on their values. The joint inference problem can be formulated as an \green{integer linear problem (ILP)}\footnote{From Wikipedia: An integer linear program in canonical form: maximize $c^T x$ subject to $A x \le b, x \ge 0, x \in \mathbb{Z}^n$}. To solve it efficiently, they find solutions for the relaxation of the problem using a \green{dual decomposition algorithm $AD^3$}. 




% ============================================================================================
\lecture{Papers and Tutorials}{Globally Normalized Transition-Based Neural Networks}{April 27, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize D. Andor et al., ``Globally Normalized Transition-Based Neural Networks,'' (2016).}


\p \blue{Introduction}. Authors demonstrate that simple FF neural networks can achieve comparable or better accuracies than LSTMs, as long as they are \green{globally normalized}. They don't use any recurrence, but perform \green{beam search} for maintaining multiple hypotheses and introduce global normalization with a \green{CRF} objective to overcome the label bias problem that locally normalized models suffer from. 

\myspace 
\p \blue{Transition System}. Given an input sequence $x$, define:
\begin{compactitem}
	\item Set of states $S(x)$.
	\item Start state $s^{\dagger} \in S(x)$.
	\item Set of decisions $\mathcal{A}(s, x)$ for all $s \in S(x)$. 
	\item Transition function $t(s, d, x)$ returning new state $s'$ for any decision $d \in A(s, x)$.  
\end{compactitem}
The scoring function $\rho(s, d; \theta)$, which gives the score for decision $d$ in state $s$, will be defined:
\begin{align}
	\rho(s, d; \theta) = \phi(s; \theta^{(l)}) \cdot \theta^{(d)}
\end{align}
which is just the familiar logits computation for decision $d$. $\theta^{(l)}$ are the parameters of the network excluding the parameters at the final layer, $\theta^{(d)}$. $\phi(s; \theta^{(l)})$ gives the representation for state $s$ computed by the neural network under parameters $\theta^{(l)}$.

\myspace
\p \blue{Global vs. Local Normalization}. 

\begin{compactitem}
	\item \textbf{Local}. Conditional probabilities $\Prob{d_j \mid s_j; \theta }$ are normalized locally over the scores for each possible action $d_j$ from the current state $s_j$. 
	\begin{align}
		\Prob[L]{d_{1:n}} 
		&= \prod_{j = 1}^{n} \Prob{d_j \mid s_j; \theta } 
		= \dfrac{   \exp \left( \sum_{j = 1}^{n} \rho  ( s_j, d_j ; \theta  ) \right)
			}{ \mpurple{ \prod_{j = 1}^{n} Z_L (s_j; \theta) }  } \\
		Z_L(s_j; \theta)
		&= \sum_{ d' \in \mathcal{A}(s_j) } \exp \left( \rho ( s_j, d'; \theta )  \right)
	\end{align}
	Beam search can be used to attempt to find the action sequence with highest probability. 
	
	\item \textbf{Global}. In contrast, a CRF defines:
	\begin{align}
		\Prob[G]{d_{1:n}}
		&= \dfrac{   \exp \left( \sum_{j = 1}^{n} \rho  ( s_j, d_j ; \theta  ) \right)
		}{ \mpurple{Z_G (\theta)}  } \\
		Z_G(\theta) 
		&=
		\sum_{d_{1:n}' \in \mathcal{D}_n } \exp \left( 
			\jnsum \rho ( s_j', d_j'; \theta )
		\right)
	\end{align}
	where $\mathcal{D}_n$ is the set of all valid sequences of decisions of length $n$. The inference problem is now to find
	\begin{align}
		\argmax_{d_{1:n} \in \mathcal{D}_n} \Prob[G]{d_{1:n}}
		&= \argmax_{d_{1:n} \in \mathcal{D}_n} \jnsum \rho(s_j, d_j; \theta) 
	\end{align}
	and we can also use beam search to approximately find the argmax. 
\end{compactitem}


\myspace
\p \blue{Training}. SGD on the NLL of the data under the model. The NLL takes a different form depending on whether we choose a locally normalized model vs a globally normalized model. 
\begin{compactitem}
	\item \textbf{Local}. 
	\begin{align}
		L_{local}(d_{1:n}^*; \theta )
		&= - \ln \Prob[L]{d_{1:n}^*; \theta} \\
		&=  -  \sum_{j = 1}^{n} \rho  ( s_j^*, d_j^* ; \theta  ) + \jnsum \ln Z_L (s_j^*; \theta) 
	\end{align}
	
	\item \textbf{Global}. 
	\begin{align}
		L_{global}(d_{1:n}^* ; \theta) 
		&=  - \ln \Prob[G]{d_{1:n}^*; \theta} \\
		&= -  \sum_{j = 1}^{n} \rho  ( s_j^*, d_j^* ; \theta  )  + \ln Z_G ( \theta) 
	\end{align}
\end{compactitem}
To make learning tractable for the globally normalized model, the authors use \green{beam search with early updates}, defined as follows. Keep track of the location of the gold path\footnote{The gold path is the predicted sequence that matches the true labeled sequence, up to the current timestep.} in the beam as the prediction sequence is being constructed. If the gold path is not found in the beam after step $j$, run one step of SGD on the following objective:
\begin{align}
L_{global-beam}(d^*_{1:j}, \theta) 
&= -\sum_{t=1}^{j} \rho(d^*_{1:t-1}, d^*_t ; \theta) 
- \ln \sum_{d'_{1:j} \in \mathcal{B}_j} \exp\left( 
\sum_{t = 1}^j \rho(d'_{1:t-1}, d'_t; \theta)
\right)
\end{align}
where $\mathcal{B}_j$ contains all paths in the beam at step $j$, \textit{and} the gold path prefix $d^*{1:j}$. If the gold path remains in the beam throughout decoding, a gradient step is performed using $\mathcal{B}_T$, the beam at the end of decoding. When training the global model, the authors \underline{first pretrain\footnote{All layers except the softmax layer.} using the local objective function, and then perform additional training steps}\newline\underline{using the global objective function.}.

\myspace
\p \blue{The Label Bias Problem}. Locally normalized models often have a very weak ability to revise earlier decisions. Here we will prove that \textbf{globally normalized models are strictly more expressive than locally normalized models}\footnote{This is for conditional models only.}. Let $\mathcal{P}_L$ denote the set of all possible distributions $p_L(d_{1:n} \mid x_{1:n})$ under the local model as the scores $\rho$ vary. Let $\mathcal{P}_G$ be the same, but for the global model.\marginnote{We are assuming that both $P_L$ and $P_G$ consist of log-linear distributions of scoring functions $\rho(d_{1:t-1}, d_t, x_{1:t})$}[2em]
\begin{center}
	\green{Theorem 3.1} {\itshape $\mathcal{P}_L$ is a strict subset\footnote{Note that $\subset$ and $\subsetneq$ \href{https://math.stackexchange.com/questions/1191715}{mean the same thing}. Matter of notational preference/being explicit/etc.} of $\mathcal{P}_G$, that is $\mathcal{P}_L \subsetneq \mathcal{P}_G$. }
\end{center}
In other words, a globally normalized model can model any distribution that a locally normalized one can, but the converse is not true. I've worked through the proof below.
\begin{example}[Proof: $P_L \subsetneq P_G$]
	\textbf{Proof that $P_L \subseteq P_G$}. For any locally normalized model with scores $\rho_L(d_{1:t-1}, d_{t}, x_{1:t})$, we can define a corresponding $p_G$ over scores
	\begin{align}
		\rho_G(d_{1:t-1}, d_t, x_{1:t}) 
			&= \ln p_L(d_t \mid d_{1:t-1}, x_{1:t}) 
	\end{align}
	By definition, this means that $p_G(d_{1:t} \mid x_{1:t}) = p_L(d_{1:t} \mid x_{1:t})$. 
	
	\textbf{Proof that $P_G \nsubseteq P_L$}. A proof by example. Consider a dataset consisting entirely of one of the following tagged sequences:
	\begin{align}
		\vec{x} = abc, &\qquad \vec{d} = ABC \\
		\vec{x} = abe, &\qquad \vec{d} = ADE  
	\end{align}
	Similar to a typical linear-chain CRF, let $\mathcal{T}$ denote the set of observed label transitions, and let $\mathcal{E}$ denote the set of observed $(x_t, d_t)$ pairs. Let $\alpha$ be the single scalar parameter of this simple model, where
	\begin{align}
		\rho(d_{1:t-1}, d_t, x_{1:t})
			&= \alpha \left( \ind_{(d_{t-1}, d_t) \in \mathcal T}
			+ \ind_{(x_t, d_t) \in \mathcal E} \right)
	\end{align}
	for all $t$. This results in the following distributions $p_G$ and $p_L$, evaluating on input sequence of length $3$
	\begin{align}
		p_G(d_{1:3} \mid x_{1:3})
			&= \frac{
				\exp\left( \alpha \sum_{t=1}^{3} ( \ind_{(d_{t-1}, d_t) \in \mathcal T}
				+ \ind_{(x_t, d_t) \in \mathcal E} )  \right)
				}{
				Z_G(x_{1:3})
			} \\
		p_L(d_{1:3} \mid x_{1:3})
			&= p_L(d_1 \mid x_1) p_L(d_2 \mid d_1, x_{1:2})  p_L(d_3 \mid d_{1:2}, x_{1:3})
	\end{align}
	where I've written $p_L$ as a product over its local CPDs because it reveals the core observation that the proof is based on: \textit{for any given subsequence $(d_{1:t-1}, x_{1:t})$, the local CPD is constrained to satisfy $\sum_{d_t} p_L(d_t \mid d_{1:t-1}, x_{1:t}) = 1$}. With this, the following comparison of $p_G$ and $p_L$ for large $\alpha$ completes the proof of $P_G \nsubseteq P_L$:
	\begin{align}
		\lim_{\alpha \rightarrow \infty} &p_G(ABC \mid abc) 
			= \lim_{\alpha \rightarrow \infty} p_G(ADE \mid abe) = 1 \\
		&p_L(ABC \mid abc) + p_L(ADE \mid abe)  \le 1 \quad (\forall \alpha) 
	\end{align}
	
	$\therefore P_L \subsetneq P_G$.
\end{example}


% ============================================================================================
\lecture{Papers and Tutorials}{An Introduction to Conditional Random Fields}{April 30, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Sutton et al., ``An Introduction to Conditional Random Fields,'' (2012).}



\p \blue{Graphical Modeling} (2.1). Some notation. Denote factors as $\psi_a(\vec[a]{y})$ where $1 \le a \le A$ and $A$ is the total number factors. $\vec[a]{y}$ is an assignment to the subset $Y_a \subseteq Y$ of variables associated with $\psi_a$. The value returned by $\psi_a$ is a non-negative scalar that can be thought of as a measure of how compatible the values $\vec[a]{y}$ are with each other. Given a collection of subsets $\{Y_a\}_{a=1}^{A}$ of $Y$, an \green{undirected graphical model} is the set of all distributions that can be written as
\begin{align}
	p(y) &= \inv{Z} \prod_{a=1}^A \psi_a(\vec[a]{y}) \\
	Z &= \sum_{\vec{y}} \prod_{a=1}^A \psi_a(\vec[a]{y})
\end{align}
for any choice of \green{factors} $\mathcal{F} = \{\psi_a\}$ that have $\psi_a(\vec[a]{y}) \ge 0$ for all $\vec[a]{y}$. The sum for the \green{partition function}, $Z$, is over all possible assignments $\vec{y}$ of the variables $Y$. We'll use the term \green{\textit{random field}} to refer to a particular distribution among those defined by an undirected model\footnote{i.e. a particular set of factors.}. \\

\p We can represent the factorization with a \green{factor graph}: a bipartite graph $G = (V, F, E)$ in which one set of nodes $V = \{1, 2, \ldots, |Y| \}$ indexes the RVs in the model, and the set of nodes $F= \{1, 2, \ldots, A\}$ indexes the factors. A connection between a variable node $Y_s$ for $s \in V$ to some factor node $\psi_a$ for $a \in F$ means that $Y_s$ is one of the arguments of $\psi_a$. It is common to draw the factor nodes as squares, and the variable nodes as circles.


\myspace
\p \blue{Generative versus Discriminative Models} (2.2). Naive Bayes is generative, while logistic regression (a.k.a maximum entropy) is discriminative. Recall that Naive Bayes and logistic are defined as, respectively,
\begin{align}
	p(y, \vec{x}) &= p(y) \prod_{k = 1}^{K} p(x_k \mid y) \\
	p(y \mid \vec{x}) &= \inv{Z(\vec{x})} \exp \left( \sum_{k = 1}^K \theta_{k} f_k(y, \vec{x})  \right)
\end{align}
where the $f_k$ in the definition of logistic regression denote the feature functions. We could set them, for example, as $f_{y', j} (y, \vec{x}) = 1_{y' = y} x_j$. \\

An example generative model for sequence prediction is the \green{HMM}. Recall that an HMM defines
\begin{align}
	p(\vec{y}, \vec{x}) &= \prod_{t = 1}^{T} p(y_t \mid y_{t-1}) p(x_t \mid y_t) 
\end{align}
where we are using the dummy notation of assuming an initial-initial state $y_0$ clamped to 0 and begins every state sequence, so we can write the initial state distribution as $p(y_1 \mid y_0)$. \\

\p We see that the generative models, like naive Bayes and the HMM, define a family of joint distributions that factorizes as $p(y, x) = p(y)p(x \mid y)$. Discriminative models, like logistic regression, define a family of conditional distributions $p(y \mid x)$. The main conceptual difference here is that a conditional distribution $p(y \mid x)$ doesn't include a model of $p(x)$. The principal advantage of discriminative modeling is that it's better suited to include rich, overlapping features. Discriminative models like CRFs make conditional independence assumptions both (1) among $y$ and (2) about how the $y$ can depend on $x$, but do \textit{not} make conditional independence assumptions among $x$.  \\

\p The difference between NB and LR is due \textit{only} to the fact that NB is generative and LR is discriminative. Any LR classifier can be converted into a NB classifier with the same decision boundary, and vice versa. In other words, NB defines the same family as LR, if we interpret NB generatively as 
\begin{align}
	p(y, \vec{x}) &= \dfrac{
		\exp \left( \sum_k \theta_k f_k(y, \vec{x}) \right) 
		}{
		\sum_{\widetilde{y}, \widetilde{x} } \exp \left( \sum_k \theta_k f_k(\widetilde y, \widetilde x)  \right)
		}
\end{align}
and train it to maximize the conditional likelihood. Similarly, if the LR model is interpreted as above, and trained to maximize the joint likelihood, then we recover the same classifier as NB.

\myfig[0.7\textwidth]{figs/intro_crf_2_4.png}


\myspace\Needspace{2\baselineskip}
\p \blue{Linear-Chain CRFs} (2.3). Key point: the conditional distribution $p(\vec{y} \mid \vec{x})$ that follows from the joint distribution $p(\vec{y}, \vec{x})$ of an HMM is in fact a CRF with a particular choice of feature functions. First, we rewrite the HMM joint in a form that's more amenable to generalization:\marginnote{HMM joint distribution}[3em]
\begin{align}
	p(\vec{y}, \vec{x}) &= \inv{Z} 
		\prod_{t =1}^{T} \exp \left( 
			\sum_{i,j \in S} \theta_{i,j} \ind_{  \{ y_t=i, y_{t-1}=j  \} }   
			+ \sum_{i \in S, o \in O} \mu_{o,i} \ind_{ \{  y_t=i, x_t=o \}  }
		\right) \\
	&= \inv{Z} \prod_{t = 1}^{T} \exp \left(
			\sum_{k = 1}^{K} \theta_k f_k(y_t, y_{t - 1}, x_t)
		\right)
\end{align}
and the latter provides the more compact notation\footnote{Note how we collapsed the summations over $i,j$ and $i,o$ to simply $k$. This is purely notational. Each value $k$ can be mapped to/from a unique $i,j$ or $i,o$ in the first version. Also note that, necessarily, each feature function $f_k$ in the latter version maps to a specific indicator function $\ind$ in the first.}. We can use Bayes rule to then write $p(\vec{y} \mid \vec{x})$, which would give us a particular kind of linear-chain CRF that only includes features for the current word's identity. The general definition of linear-chain CRFs is given below:
\vspace{-0.7em}
\begin{quote}
	{\small\itshape
		Let $Y, X$ be random vectors, $\theta = \{\theta_k\} \in \R^K$ be a parameter vector, and $\mathcal{F} = \{f_k(y, y', \vec[t]{x})\}_{k = 1}^{K}$ be a set of real-valued feature functions. Then a \green{linear-chain conditional random field} is a distribution $p(\vec{y} \mid \vec{x})$ that takes the form:}
	\graybox{
		p(\vec{y} \mid \vec{x} ) &= \inv{Z(\vec x)} 
		\prod_{t = 1}^{T} \exp \left(
		\sum_k	\theta_k f_k(y_t, y_{t - 1}, \vec[t]{x}) \right) \\
		Z(\vec{x}) &= \sum_{\vec y} \prod_{t = 1}^{T} \exp \left(
		\sum_k	\theta_k f_k(y_t, y_{t - 1}, \vec[t]{x}) \right) 
	}
\end{quote}
Notice that a linear chain CRF can be described as a factor graph over $\vec{x}$ and $\vec{y}$, where each local function (factor) $\psi_t$ has the special log-linear form:
\begin{align}
	\psi_t(y_t, y_{t-1}, x_t) &= \exp \left(
	\sum_k \theta_k f_k(y_t, y_{t - 1}, x_t)
	\right)
\end{align}


\myspace
\p \blue{General CRFs} (2.4). Let $G$ be a factor graph over $X$ and $Y$. Then $(X, Y)$ is a conditional random field if for any value $\vec{x}$ of $X$, the distribution $p(\vec{y} \mid \vec{x})$ factorizes according to $G$. If $F = \{\psi_a\}$ is the set of $A$ factors in $G$, then the conditional distribution for a CRF is
\begin{align}
	p(\vec{y} \mid \vec{x}) &= \inv{Z(\vec{x})} \prod_{a = 1}^{A} \psi_a(\vec[a]{y}, \vec[a]{x})
\end{align}
It is often useful to require that the factors be log-linear over a prespecified set of feature functions, which allows us to write the conditional distribution as
\begin{align}
	p(\vec{y} \mid \vec{x}) &= \inv{Z(\vec{x})} \prod_{\psi_a \in F}
		\exp\left( \sum_{k =1}^{K(a)}  \theta_{a,k}f_{a,k}(\vec[a]{y}, \vec[a]{x} ) \right)
\end{align}
In addition, most models rely extensively on \textbf{parameter tying}\footnote{Note how, for CRFs, the actual parameters $\theta$ are tightly coupled with the feature functions $f$.}. To denote this, we partition the factors of $G$ into $\mathcal{C} = \{C_1, C_2, \ldots, C_P\}$, where each $C_p$ is a \green{clique template}: a set of factors sharing a set of feature functions $\{f_{p,k}(\vec[c]{x}, \vec[c]{y})\}_{k=1}^{K(p)}$ and a corresponding set of parameters $\vec[p]{\theta} \in \R^{K(p)}$. A CRF that uses clique templates can be written as 
\begin{align}
	p(\vec{y} \mid \vec{x}) &= \inv{Z(\vec x)} \prod_{C_p \in \mathcal{C}} \prod_{\psi_c \in C_p} \psi_c(\vec[c]{x}, \vec[c]{y}; \vec[p]{\theta}) \\
	&= \inv{Z(\vec x)} \prod_{C_p \in \mathcal{C}} \prod_{c \in C_p}
		\exp\bigg\{ \sum_{k = 1}^{K(p)} \theta_{p,k}f_{p,k}(\vec[c]{x}, \vec[c]{y}) \bigg\}
\end{align}
In a linear-chain CRF, typically one uses one clique template $C_0 = \{\psi_t\}_{t=1}^{T}$. Again, each factor in a given template \textit{shares the same feature functions and parameters}, so the previous sentence means that we reuse the set of features and parameters for each timestep. 

\myspace
\p \blue{Feature engineering} (2.5). 
\begin{compactitem}
	\item \textbf{Label-observation features}. When our label variables are discrete, the features $f_{p,k}$ of a clique template $C_p$ are ordinarily chosen to have a particular form:
	\begin{align}
		f_{p,k}(\vec[c]{y}, \vec[c]{x}) = 1_{\{ \vec[c]{y} = \vec[c]{\widetilde y} \}} q_{p,k}(\vec[c]{x})
	\end{align}
	and we refer to the functions $q_{p,k}(\vec[c]{x})$ as \textit{observation functions}.
	
	\item \textbf{Unsupported features}. Many observation-label pairs may never occur in our training data (e.g. having the word ``with'' being associated with label ``CITY''). Such features are called unsupported features, and can be useful since often their weights will be driven negative, which can help prevent the model from making predictions in the future that are far from what was seen in the training data.
	
	\item \textbf{Edge-Observation and Node-Observation features}: the two most common types of label-observation features. Edge-observation features are for the transition factors, while node-observation features are the form introduced for label-observation features above.
	\begin{align}
		\mtgreen{[edge-obs]} \quad f(y_t, y_{t - 1}, \vec[t]{x}) &= q_m(\vec[t]{x})\ind_{y_t=y,y_{t-1}=y'} \quad (\forall y, y' in \mathcal{Y}, \forall m) \\
		\mtgreen{[node-obs]} \quad f(y_t, y_{t - 1}, \vec[t]{x}) &= \ind_{y_t=y,y_{t-1}=y'} \quad (\forall y, y' in \mathcal{Y})
	\end{align}
	and both use the same $f(y_t, \vec[t]{x}) = q_m(\vec[t]{x}) \ind_{y_t = y}$ $(\forall y, \in \mathcal{Y}, \forall m)$. Recall that $m$ is the index into our set of observation features\footnote{In CRFSuite, the observation features are all the attributes we define, and any features that use both label and observation are defined within CRFSuite itself.}.
	
	\item \textbf{Feature Induction}. The model begins with a number of base features, and the training procedure adds conjunctions of those features.
\end{compactitem}


\myspace\Needspace{20\baselineskip}
\subsub{Inference (Sec. 4)}
\myspace

\p There are two inference problems that arise:
\begin{compactenum}
	\item Wanting to predict the labels of a new input $\vec{x}$ using the most likely labeling $\vec{y}^* = \argmax_{\vec y} p(\vec{y} \mid \vec{x})$. 
	
	\item Computing marginal distributions (during parameter estimation, for example) such as node marginals $p(y_t \mid \vec x)$ and edge marginals $p(y_t, y_{t -1} \mid \vec x)$. 
\end{compactenum}
For linear-chain CRFs, the \green{forward-backward algorithm} is used for computing marginals, and the \green{Viterbi algorithm} for computing the most probable assignment. We'll first derive these for the case of HMMs, and then generalize to the linear-chain CRF case. 

\myspace
\p \blue{Forward-backward algorithm (HMMs)}. An efficient technique for computing marginals. We begin by writing out $p(\vec x)$, and using the distributive law to convert the sum of products to a product of sums\marginnote{We define the $\psi_t$ as	$p(y_t \mid y_{t-1}) p(x_t \mid y_t)$}[3em]:
\begin{align}
	p(\vec x) &= \sum_{\vec y} p(\vec x, \vec y) \\
	&= \sum_{\vec y} \prod_{t = 1}^{T} \psi_t(y_t, y_{t - 1}, x_t) \\
	&= \sum_{y_1} p(y_1)p(x_1 \mid y_1) \sum_{y_2} p(y_2 \mid y_1) p(x_2 \mid y_2) \sum_{y_3} \cdots \sum_{y_T} p(y_T \mid y_{T-1}) p(x_T \mid y_{T}) \\
	&= \sum_{y_1} \psi_1(y_1, x_1) \sum_{y_2} \cdots \sum_{y_T} \psi_T(y_T, y_{T - 1}, x_T) \\
	&= \sum_T \sum_{T - 1} \psi_T(y_T, y_{t - 1}, x_T) \sum_{y_{T - 2}} \cdots \sum_{y_1} \psi_1(y_1, x_1) \label{crf-4-3}
\end{align}
We see that we can save an exponential amount of work by caching the inner sums as we go. Let $M$ denote the number of possible states for the $y$ variables. We define a set of $T$ \green{forward variables} $\alpha_t$, each of which is a vector of size $M$:
\begin{align}
	\alpha_t(j) &\triangleq p(\slice[t]{x}, y_t = j) \label{alpha-1} \\
	&= \sum_{\slice[t-1]{y}} p(\slice[t]{x}, \slice[t-1]{y}, y_t = j) \label{alpha-2} \\
	&= \sum_{\slice[t-1]{y}} \psi_t(j, y_{t - 1}, x_t) \prod_{t' = 1}^{t' - 1} \psi_{t'}(y_{t'}, y_{t - 1}, x_{t'}) \\
	&= \sum_{i \in S} \psi_t(j, i, x_t) \sum_{\slice[t-2]{y}} \psi_{t-1}(y_{t -1}, y_{t -2}, x_{t -1}) \prod_{t' = 1}^{t - 2} \psi_{t'}(y_{t'}, y_{t - 1}, x_{t'}) \\
	&= \sum_{i \in S} \psi_t(j, i, x_t) \alpha_{t - 1}(i)
\end{align}
where $S$ is the set of M possible states. By recognizing that $p(\vec x) = \sum_{j \in S}\sum_{\slice[t-1]{y}} p(\slice[t]{x}, \slice[t-1]{y}, j)$, we can rewrite $p(\vec x)$ as
\begin{align}
	p(\vec x) &= \sum_{j \in S} \alpha_T(j) \label{alpha-3}
\end{align}

Notice how in the step from equation \ref{alpha-1} to \ref{alpha-2}, we marginalized over all possible $y$ subsequences that could've been aligned with $\slice[t]{x}$. We will repeat this pattern to derive the backward recursion for $\beta_t$, which is the same idea except now we go from $T$ backward until some $t$ (instead of going from 1 \textit{forward} until some $t$).\marginnote{We initialize $\beta_T(i) = 1$.}[4em]
\begin{align}
	\beta_t(i) &\triangleq p(\slice[T][t+1]{x} \mid y_t = i) \\
	&= \sum_{\slice[T][t+1]{y}} \psi_{t + 1}(y_{t + 1}, i, x_{t + 1}) \prod_{t' = t+2}^{T} \psi_{t'}(y_{t'}, y_{t'-1}, x_{t'}) \\
	&= \sum_{y_{t + 1}} \psi_{t + 1}(y_{t + 1}, i, x_{t + 1}) \beta_{t+1}(y_{t + 1})
\end{align}
Similar to how we obtained equation \ref{alpha-3}, we can rewrite $p(\vec x)$ in terms of the $\beta$:
\begin{align}
	p(\vec x) &= \beta_0(y_0) \triangleq \sum_{y_1} \psi_1(y_1,  y_0, x_1) \beta_1(y_1) \label{beta-1}
\end{align}
We can then combine the definition for $\alpha$ and $\beta$ to compute marginals of the form $p(y_{t - 1}, y_t \mid \vec{x})$:
\graybox{
	p(y_{t - 1}, y_t \mid \vec{x}) &=
	\inv{p(\vec x)} \alpha_{t -1}(y_{t-1}) \psi_t(y_t, y_{t-1}, x_t) \beta_t(y_t) \label{fw-bw-marginals}	\\
	\text{where}\quad p(\vec x) &=  \sum_{j \in S} \alpha_T(j)  =  \beta_0(y_0) 
}
In summary, the forward-backward algorithm consists of the following steps:
\begin{compactenum}
	\item Compute $\alpha_t$ for all $t$ using equation \ref{alpha-3}. 
	\item Compute $\beta_t$ for all $t$ using equation \ref{beta-1}. 
	\item Return the marginal distributions computed from equation \ref{fw-bw-marginals}. 
\end{compactenum}

\myspace
\p \blue{Viterbi algorithm (HMMs)}. For computing $\vec{y}^* = \argmax_{\vec y} p(\vec y \mid \vec x)$. The derivation is nearly the same as how we derived the forward-backward algorithm, except now we've replaced the summations in equation \ref{crf-4-3} with maximization. The analog of $\alpha$ for viterbi are defined as:
\begin{align}
	\delta_t(j) &= \max_{\slice[t-1]{y}} \psi_t(j, y_{t -1}, x_t) \prod_{t' = 1}^{t - 1} \psi_{t'}(y_{t'}, y_{t' - 1}, x_{t'}) \\
	&= \max_{i \in S} \psi_t(j, i, x_t) \delta_{t - 1}(i)
\end{align}
and the maximizing assignment is computed by a backwards recursion,
\graybox{
	y_T^* &= \argmax_{i \in S} \delta_T(i) \\
	y_t^* &= \argmax_{i \in S} \psi_t(y_{t + 1}^*, i, x_{t + 1}) \delta_t(i) \text{ for } t < T
}
Computing the recursions for $\delta_t$ and $y_t^*$ together is the \textit{Viterbi algorithm}. 


\myspace
\p \blue{Forward-backward and Viterbi for linear-chain CRF}. Generalizing to the linear-chain CRF, where now
\begin{align}
	p(\vec y \mid \vec x) &= \inv{Z(\vec x)} \prod_{t = 1}^{T} \psi_t(y_t, y_{t - 1}, x_t) \\
	\text{where}\quad \psi_t(y_t, y_{t - 1}, x_t) &= \exp \left\{    
		\sum_k \theta_k f_k(y_t, y_{t -1}, x_t) \right\}
\end{align}
and the results for the forward-backward algorithm become
\graybox{
	p(y_{t - 1}, y_t \mid \vec{x}) &=
	\inv{Z(\vec x)} \alpha_{t -1}(y_{t-1}) \psi_t(y_t, y_{t-1}, x_t) \beta_t(y_t)	\\
	p(y_t \mid \vec x) &= \inv{Z(\vec x)} \alpha_t(y_t) \beta_t(y_t) \\
	\text{where}\quad 
	Z(\vec x) &=  \sum_{j \in S} \alpha_T(j)  =  \beta_0(y_0) 
}
Note that the interpretation is also slightly different. The $\alpha$, $\beta$, and $\delta$ variables should only be interpreted with the factorization formulas, and \textit{not} as probabilities. Specifically, use
\begin{align}
	\alpha_t(j) 
		&= \sum_{\slice[t-1]{y}} \exp \left\{\sum_k \theta_k f_k(j, y_{t -1}, x_t) \right\} 
			\prod_{t' = 1}^{t' - 1} \exp \left\{\sum_k \theta_k f_k(y_{t'}, y_{t' -1}, x_{t'}) \right\}  \\
	\beta_t(i) &= \sum_{\slice[T][t+1]{y}}  \exp \left\{\sum_k \theta_k f_k(y_{t+1}, i, x_{t+1}) \right\} \prod_{t' = t+2}^{T}  \exp \left\{\sum_k \theta_k f_k(y_{t'}, y_{t' -1}, x_{t'}) \right\}  \\
	\delta_t(j) &= \max_{\slice[t-1]{y}}  \exp \left\{\sum_k \theta_k f_k(j, y_{t-1}, x_t) \right\} \prod_{t' = 1}^{t - 1}  \exp \left\{\sum_k \theta_k f_k(y_{t'}, y_{t' -1}, x_{t'}) \right\} 
\end{align}


\myspace 
\p \blue{Markov Chain Monte Carlo} (MCMC). The two most popular classes of approximate inference algorithms are \green{Monte Carlo} algorithms and \green{variational} algorithms. In what follows, we drop the CRF-specific notation and refer to the more general joint distribution
\begin{align}
	p(\vec y) = Z^{-1} \prod_{a \in F} \psi_a(\vec[a]{y})
\end{align}
that factorizes according to some factor graph $G = (V, F)$. MCMC methods construct a Markov chain whose state space is the same as that of $Y$, and sample from this chain to approximate, e.g., the expectation of some function $f(\vec y)$ over the distribution $p(\vec y)$. MCMC algorithms aren't commonly applied in the context of CRFs, since parameter estimation by maximum likelihood requires calculating marginals many times. 


\myspace\Needspace{20\baselineskip}
\subsub{Parameter Estimation (Sec. 5)}
\myspace

\p \blue{Maximum Likelihood for Linear-Chain CRFs}. Since we're modeling the conditional distribution with CRFs, we use the \green{conditional log likelihood} $\ell(\vec{\theta})$ with l2-regularization:
\begin{align}
	\ell(\vec{\theta}) &= \sum_{i = 1}^{N} \log p(\vec{y}^{(i)} \mid \vec{x}^{(i)} ; \vec{\theta}) - \inv{2\sigma^2}\sum_{k = 1}^{K} \theta_k^2 \\
	&= \sum_{i = 1}^{N}\sum_{t = 1}^{T}\sum_{k = 1}^{K} \theta_k f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)}) - \sum_{i = 1}^{N} \log Z(\vec{x}^{(i)})  - \inv{2\sigma^2}\sum_{k = 1}^{K} \theta_k^2
\end{align}
with regularization parameter $1/2\sigma^2$. The partial derivatives are 
\begin{align}
	\pderiv{\ell}{\theta_k} &= \sum_{i,t} f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)})
		- \sum_{i,t,y,y'} f_k(y, y', \vec[t]{x}^{(i)}) p(y, y' \mid \vec{x}^{(i)})
		- \frac{\theta_k}{\sigma^2}
\end{align}
and the derivation for the partial derivative of $\log(Z(x))$ is
\begin{align}
	\pderiv{\log Z(x)}{\theta_k} 
	&= \inv{Z(x)} \pderiv{}{\theta_k} \left[ \sum_{\slice[T]{y}} \prod_{t = 1}^{T} \exp\left\{ \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}  \right] 
	\\
	&= \inv{Z(x)} \sum_{\slice[T]{y}} \pderiv{}{\theta_k}  \exp\left\{ \sum_t \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}
	\\
	&= \inv{Z(x)} \sum_{\slice[T]{y}} \sum_t f_k(y_t, y_{t-1}, x_t)  \exp\left\{ \sum_t \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}  
	\\
	&= \sum_t \sum_{y_t} \sum_{y_{t-1}} f_k(y_t, y_{t-1}, x_t) \left[ \sum_{\slice[t-2]{y}} \sum_{\slice[T][t+1]{y}} \inv{Z(x)}  \exp\left\{ \sum_t \sum_k \theta_k f_k(y_t, y_{t-1}, x_t)  \right\}  \right] 
	\\
	&= \sum_t \sum_{y} \sum_{y'} f_k(y, y', x_t) p(y_t=y, y_{t-1}=y' \mid x)
\end{align}

We can rewrite this in the form of expectations. For now, let $\widetilde{p}(\vec y, \vec x)$ denote the \textit{empirical distribution}, and let $\hat{p}(\vec y \mid \vec x; \theta)\widetilde{p}(\vec x)$ denote the \textit{model distribution}.
\begin{align}
	\pderiv{\ell}{\theta_k}
		&= \E[\vec{x},\vec{y} \sim \widetilde{p}(\vec{y},\vec{x})]{\sum_t f_k(y_t, y_{t-1}, x_t)}
			- \E[\vec{x}, \vec y \sim \hat{p}(\vec y, \vec x)]{\sum_t f_k(y_t, y_{t-1}, x_t)}
\end{align}

\begin{algorithm}[Procedure: Training Linear-Chain CRFs]
	Here I summarize the main steps involved during a parameter update.\\

	\textbf{Inference}. We need to compute the log probabilities for each instance in the dataset, under the current parameters. We will need them when evaluating $Z(\vec x)$ and the marginals $p(y, y' \mid \vec x)$ when computing gradients.
	\begin{compactenum}
		\item Initialize $\alpha_1(j) = \exp\{ \sum_k \theta_k f_k(j, y_0, x_1) \}$ ($y_0$ is the fixed initial state) and $\beta_T(i) = 1$. 
		
		\item For all $t$, compute:
		\begin{align}
		\alpha_t(j) &= \sum_{i \in S} \exp\left\{ \sum_k \theta_k f_k(j, i, x_t)\right\}  \alpha_{t - 1}(i)  \\
		\beta_t(j) &= \sum_{i \in S}   \exp\left\{ \sum_k \theta_k f_k(i, j, x_{t+1})\right\} \beta_{t+1}(i)
		\end{align}
		
		\item Compute the marginals:
		\begin{align}
		p(y_t, y_{t-1}, \mid \vec{x}) &=
		\inv{Z(\vec x)} \alpha_{t -1}(y_{t-1}) \psi_t(y_t, y_{t-1}, x_t) \beta_t(y_t)	\\
		p(y_t \mid \vec x) &= \inv{Z(\vec x)} \alpha_t(y_t) \beta_t(y_t) \\
		\text{where}\quad 
		Z(\vec x) &=  \sum_{j \in S} \alpha_T(j)  =  \beta_0(y_0) 
		\end{align}
	\end{compactenum}
	
	\textbf{Gradients}. For each parameter $\theta_k$, compute:
	\begin{align}
	\pderiv{\ell}{\theta_k} &= \sum_{i,t} f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)})
	- \sum_{i,t,y,y'} f_k(y, y', \vec[t]{x}^{(i)}) p(y, y' \mid \vec{x}^{(i)})
	- \frac{\theta_k}{\sigma^2}
	\end{align}
\end{algorithm}


\myspace \p \blue{CRF with latent variables}. Now we have additional latent variables $\vec z$:
\begin{align}
	p(\vec y, \vec z \mid \vec x) &= \inv{Z(\vec x)} \prod_t \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x}) 
\end{align}
Since we observe $\vec y$ during training, what if we instead treat this as a CRF with both $\vec x$ \textit{and} $\vec y$ observed?
\begin{align}
	p(\vec z \mid \vec y, \vec x) 
		&= \inv{Z(\vec y, \vec x)} \prod_t \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x}) \\
	Z(\vec y, \vec x) 
		&= \sum_{\vec z} \prod_t  \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x})
\end{align}
We can use the same inference algorithms as usual to compute $Z(\vec y, \vec x)$, and the key result is that we can now write
\graybox{
	p(\vec y \mid \vec x)
		&= \inv{Z(\vec x)} \sum_{\vec z} \prod_t  \psi_t(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x})
		= \frac{Z(\vec y, \vec x)}{Z(\vec x)}
}
Finally, we can write the gradient as\footnote{This uses the trick $$ \deriv{f}{\theta} = f(\theta) \deriv{\log f}{\theta} $$}
\begin{align}
	\pderiv{\ell}{\theta_k}
		&= \sum_{\vec z} p(\vec z \mid \vec y, \vec x) \pderiv{}{\theta_k}\left[ \log p(\vec y, \vec z \mid \vec x) \right]\\
		&= \sum_t \sum_{\vec[t]{z}} \left[ p(\vec[t]{z} \mid \vec y, \vec x) f_k(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x})
			-  \sum_{y, y'} p(\vec[t]{z}, y, y' \mid \vec[t]{x}) f_k(y_t, y_{t-1}, \vec[t]{z}, \vec[t]{x}) \right]
\end{align}
where I've assumed there are no connections between any $\vec[t]{z}$ and $\vec[t' \ne t]{z}$.



\myspace
\p \blue{Stochastic Gradient Methods}. Now we compute gradients for a single example, and for a linear-chain CRF:
\begin{align}
	\pderiv{\ell_i}{\theta_k} 
		&= \sum_{t} f_k(y_t^{(i)}, y_{t-1}^{(i)}, \vec[t]{x}^{(i)})
			- \sum_{t,y,y'} f_k(y, y', \vec[t]{x}^{(i)}) p(y, y' \mid \vec{x}^{(i)})
			- \frac{\theta_k}{N \sigma^2}
\end{align}
which corresponds to parameter update (remember: we are using the LL, not the negative LL):
\begin{align}
	\theta^{(m)}
		&= \theta^{(m-1)} + \alpha_m \nabla \ell_i(\theta^{(m-1)})
\end{align}
where $m$ denotes this is the $m$th update of the training process.








\myspace\Needspace{20\baselineskip}
\subsub{Related Work and Future Directions (Sec. 6)}
\myspace

\p \blue{MEMMs}. Maximum-entropy Markov models. Essentially a Markov model in which the transition probabilities are given by logistic regression. Formally, a MEMM is defined by
\begin{align}
	p_{MEMM}(\vec y \mid \vec x) &= \prod_{t = 1}^{T} p(y_t \mid y_{t -1}, \vec x) \\
	p(y_t \mid y_{t-1}, \vec x) &= \frac{ \exp\bigg\{  \sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, \vec[t]{x}) \bigg\} }{ Z_t(y_{t-1}, \vec x) } \\
	Z_t(y_{t-1}, \vec x) &= \sum_{y'} \exp\bigg\{  \sum_{k=1}^{K} \theta_k f_k(y_t, y_{t-1}, \vec[t]{x}) \bigg\}
\end{align}
which has some important differences compared to the linear-chain CRF. Notice how maximum-likelihood training of MEMMs does \textit{not} require performance inference over full output sequences $\vec y$, because $Z_t$ is a simple sum over the labels at a single position. MEMMs, however, suffer from \textit{label bias}, while CRFs do not.

\myspace
\p \blue{Bayesian CRFs}. Instead of predicting the optimal labeling $y_{ML}^*$ for input sequence $x$ with maximum likelihood (ML), we can instead use a fully Bayesian (B) approach, both of which are shown below for comparison:
\begin{align}
	y_{ML}^* &\leftarrow arg\,max_y p(y \mid x; \hat{\theta}) \\
	y_{B}^* &\leftarrow arg\,max_y \mathbb{E}_{\theta \sim p(\theta \mid x)}(p(y \mid x; \theta))
\end{align}
Unfortunately, computing the exact integral (the expectation) is usually intractable, and we must resort to approximate methods like MCMC.  






% ============================================================================================
\lecture{Papers and Tutorials}{Co-sampling: Training Robust Networks for Extremely Noisy Supervision}{May 02, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Han et al., ``Co-sampling: Training Robust Networks for Extremely Noisy Supervision,'' (2018).}

\p \blue{Introduction}. The authors state that current methodologies [for training networks under noisy labels] involves estimating the \green{noise transition matrix} (which they don't define). Patrini et al. (2017) define the matrix as follows:
\begin{center}
	\begin{small}
		{\itshape 
			Denote by $T \in [0, 1]^{c \times c}$ the noise transition matrix specifying the probability of one label being flipped to another, so that $\forall i, j ~ T_{ij} \triangleq \Prob{\widetilde y = e^j \mid y = e^i}$. The matrix is row-stochastic\footnote{Each row sums to 1.} and not necessarily symmetric across the classes.
		}
	\end{small}
\end{center}


\myspace
\p \blue{Algorithm}. Authors propose a learning paradigm called \green{Co-sampling}. They maintain two networks $f_{w_1}$ and $f_{w_2}$ simultaneously. For each mini-batch data $\hat{\mathcal D}$, each network selects $\mathcal{R}_T$ small-loss instances as a ``clean'' mini-batch $\hat{\mathcal D}_1$ and $\hat{\mathcal D}_2$, respectively. Each of the two networks then uses the clean mini-batch data to update the parameters $w_2$ ($w_1$) of its \textit{peer} network.
\begin{compactitem}
	\item Why small-loss instances? Because deep networks tend to fit clean instances first, then noisy/harder instances progressively after. 
	\item Why two networks? Because if we just trained a single network on clean instances, we would not be robust in extremely high-noise rates, since the training error would accumulate if the selected instances are not ``fully clean.''
\end{compactitem}
The Co-sampling paradigm algorithm is shown below.

\myfig[0.6\textwidth]{figs/cosampling_alg_1.png}





% ============================================================================================
\lecture{Papers and Tutorials}{Hidden-Unit Conditional Random Fields}{June 30, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Maaten et al., ``Hidden-Unit Conditional Random Fields,'' (2011).}

\p \blue{Introduction}. Three key advantages of CRFs over HMMs:
\begin{compactenum}
	\item CRFs don't assume that the observations are conditionally independent given the hidden (or target if linear-chain CRF) states. 
	
	\item CRFs don't suffer from the label bias problems of models that do local probability normalization\footnote{See the introduction in my CRF notes for recap of label-bias.}. 
	
	\item For certain choices of factors, the negative conditional L.L. is convex. 
\end{compactenum}
The hidden-unit CRF (HUCRF), similar to discriminative restricted Boltzmann machines (RBMs), has binary stochastic hidden units that are conditionally independent given the data and the label sequence. By exploiting the conditional independence properties, we can efficiently compute:
\begin{compactenum}
	\item The exact gradient of the C.L.L. 
	
	\item The most likely label sequence. 
	
	\item The marginal distributions over label sequences. 
\end{compactenum}

\myspace
\p \blue{Hidden-Unit CRFs}. At each time step $t$, the HUCRF employs $H$ binary stochastic hidden units $\vec[t]{z}$. It models the conditional distribution as 
\graybox{
	p(\vec y \mid \vec x) &= \inv{\matr{Z}(\vec x)} \sum_{\vec z} \exp\left( E( \vec x, \matr z, \vec y ) \right) \\
	\begin{split}
	E(\vec x, \vec z, \vec y) &= 
		\sum_{t = 2}^T \big[ \vec[t - 1]{y}^T \matr{A} \vec[t]{y} \big] + 
		\sum_{t = 1}^T \big[ \vec[t]{x}^T \matr{W} \vec[t]{z} + 
							 \vec[t]{z}^T \matr{V} \vec[t]{y} + 
							 \vec{b}^T \vec[t]{z} + \vec{c}^T \vec[t]{y}  \big] \\
	&~~+ \vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau}
	\end{split}
}
Since the hidden units are conditionally independent given the data and labels, the hidden units can be marginalized out one-by-one. This, along with the nice property that the hidden units have binary elements, allows us to write $p(\vec y \mid \vec x)$ without writing any $\vec[t]{z}$ explicitly, as shown below:
\begin{align}
\begin{split}
	p(\vec y \mid \vec x) 
	= \frac{ \exp\{\vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau} \} }{ Z(\vec x) }
		&\prod_{t=1}^{T}\bigg[ 
	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}           \\
	&\quad \prod_{h = 1}^{H} \sum_{z_h \in \{0, 1\}} \exp\{ z_h b_h + z_h \vec[h]{w}^T \vec[t]{x} + z_h \vec[h]{v}^T \vec[t]{y}  \}	\bigg]
\end{split}\\
\begin{split}
	= \frac{ \exp\{\vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau} \} }{ Z(\vec x) }
	&\prod_{t=1}^{T}\bigg[ 
	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}           \\
	&\quad \prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)	\bigg]
\end{split}
\end{align}
For inference, we'll need an algorithm for computing the marginals $p(y_t \mid \vec x)$ and $p(y_t, y_{t - 1} \mid \vec x)$. The equations are essentially the same as the forward-backward formulas for the linear-chain CRF, but with summations over $\vec z$:
\graybox{
	p(y_t, y_{t-1} \mid \vec x) &\propto \alpha_{t - 1}(y_{t - 1}) \bigg[ \sum_{\vec[t]{z}} \Psi_t(\vec[t]{x}, \vec[t]{z}, y_t, y_{t - 1}) \bigg] \beta_{t}(y_t)	
	\\
	p(y_t \mid \vec x) &\propto \alpha_t(y_t) \beta_t(y_t) \\
	\\
	\alpha_t(j) &= \sum_{i \in \mathcal{Y}} \sum_{\vec[t]{z}} \Psi_t(\vec[t]{x}, \vec[t]{z}, j, i) \alpha_{t - 1}(i)
	\\
	\beta_t(j) &= \sum_{i \in \mathcal{Y}} \sum_{\vec[t+1]{z}} \Psi_{t+1}(\vec[t+1]{x}, \vec[t+1]{z}, i, j) \beta_{t+1}(i)
}


\myspace 
\p \blue{Training}. The conditional log likelihood for a single example $(\vec x, \vec y)$ is (bias and initial-state terms omitted) 
\begin{align}
	\mathcal{L} &= \log p(\vec y \mid \vec x) \\
	&= \sum_{t = 1}^{T} \log \left( 
		\sum_{\vec[t]{z}} \Psi_t\left( \vec[t]{x}, \vec[t]{z}, y_{t-1}, y_t)  \right)
	\right) - \log Z(\vec x) \\
	\text{where}\qquad \Psi_t &:= \exp\{  y_{t-1} \matr{A} y_t + \vec[t]{x}^T \matr{W} \vec[t]{z} + \vec[t]{z}^T \matr{V} y_t  \}
\end{align}
Let $\Upsilon = \{\matr W, \matr V, \vec b, \vec c, \}$ be the set of model parameters. The gradient w.r.t. the data-dependent\footnote{The data-dependent parameters are each individual element of the elements of $\Upsilon$. ``Data'' here means $(\vec x, \vec y)$. Notice that $\Upsilon$ does not include $\matr A$, $\vec{\pi}$, or $\vec{\tau}$.} parameters $v \in \Upsilon$ is given by
\begin{align}
	\pderiv{\mathcal L}{v} 
	&= \sum_{t=1}^{T} \left[
		\sum_{k \in \mathcal{Y}} \left(
			\left(  \ind_{y_t = k}   -  p(y_t = k \mid \vec x) \right) \sum_{h = 1}^{H} \sigma(o_{hk}(\vec[t]{x})) \pderiv{o_{hk}(\vec[t]{x})}{ v }
		\right)
	\right] 
	\\
	\text{where}\qquad
	o_{hk}(\vec[t]{x}) &= b_h + c_k + V_{hk} + \vec[h]{w}^T \vec[t]{x}
\end{align}
Unfortunately, the negative CLL is \textbf{non-convex}, and so we are only guaranteed to converge to a \textit{local} maximum of the CLL. 


\subsub{Detailed Derivations}

Unfortunately, the paper leaves out a lot of details regarding derivations and implementations. I'm going to work through them here. First, a recap of the main equations, and with all biases/initial states included. Not leaving anything out\footnote{The equation for $p(\vec y \mid \vec x)$ from the paper, and thus here, is technically incorrect. The term $\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}$ should not be included in the product over $t$ for $t=1$.}
\begin{small}
\begin{align}
	p(\vec y \mid \vec x) 
		&= \frac{\exp\left\{ \vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau}  \right\}}{Z(\vec x)} \prod_{t=1}^{T}\bigg[ 
			\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}  
			\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)	\bigg] \\
	NLL 
		&= - \sum_{i=1}^{N} \log p(\vec{y}^{(i)} \mid \vec{x}^{(i)}) \\
		&= - \sum_{i=1}^{N} \left[ \sum_{t = 1}^{T} \log \left( \sum_{\vec[t]{z}} \psi_t(\vec[t]{x}, \vec[t]{z}, \vec[t-1]{y}, \vec[t]{y}) \right) 
		   - \log Z(\vec{x}^{(i)}) \right] 
\end{align}
\end{small}

%\begin{comment}
The above formula for $p(\vec y \mid \vec x)$ implies something that will be very useful:
\begin{align}
	\sum_{\vec[t]{z}} \psi_t(\vec[t]{x}, \vec[t]{z}, \vec[t-1]{y}, \vec[t]{y}) 
		&= 	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}  
		\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)
\end{align}
Using the generalization of the product rule for derivatives over N products, we can derive that
\begin{align}
 \pderiv{}{v} \prod_h (1 + \exp\{ o(h) \}) &= \left[ \prod_h (1 + \exp\{ o(h) \})\right] \left[ \sum_h \sigma\left(o\left(h\right)\right) \pderiv{o(h)}{v} \right]
\end{align}
Which means the derivatives of $\sum_z \psi$ for the data-dependent params $v_{dat}$ and transition params $v_{tr}$, are:
\begin{align}
	\pderiv{\sum_{\vec[t]{z}} \psi_t}{v_{dat}} 
		&=  \left[ \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right]  \sum_{\vec[t]{z}} \psi_t \\
	\pderiv{\sum_{\vec[t]{z}} \psi_t}{v_{tr}}
		&= \left[ \pderiv{}{v_{tr}}  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right] \sum_{\vec[t]{z}} \psi_t 
\end{align}
which also conveniently means that
\begin{align}
	\pderiv{}{v_{dat}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
		&=  \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \\
	\pderiv{}{v_{tr}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
		&=  \pderiv{}{v_{tr}}  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} 
\end{align}
% \end{comment}
 
I'll now proceed to derive the gradients of negative (conditional) log-likelihood for the main parameters. We can save some time by getting the base formula for any of the gradients with respect to a specific single parameter $v$:
\begin{small}
\begin{align}
	\pderiv{NLL}{v} 
		&=  - \sum_{i=1}^{N} \left[ \sum_{i = 1}^{T} \pderiv{}{v} \log \left( \sum_{\vec[t]{z}} \psi_t(\vec[t]{x}^{(i)}, \vec[t]{z}, \vec[t-1]{y}^{(i)}, \vec[t]{y}^{(i)}) \right) 
		- \pderiv{}{v} \log Z(\vec{x}^{(i)}) \right] \\
	\pderiv{\log Z(\vec{x}^{(i)})}{v}
		&= \inv{Z(\vec{x}^{(i)} )} \pderiv{}{v} \sum_{\slice[T]{y}} \widetilde{p}(\vec[1]{y},\ldots,\vec[T]{y} \mid \vec{x}^{(i)}) \\
	\pderiv{}{v}  \widetilde{p}(\vec[1]{y},\ldots,\vec[T]{y} \mid \vec{x}^{(i)}) 
		&= \pderiv{}{v} \prod_t \sum_{\vec[t]{z}} \psi_t \\
		&= \left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
			\sum_t \frac{\pderiv{}{v} \sum_{\vec[t]{z}}  \psi_t }{ \sum_{\vec[t]{z}} \psi_t   }
		 \right]
\end{align}
\end{small}
where I've done some regrouping on the last line to be more gradient-friendly.

\begin{example}[Data-dependent parameters] 

All params $v$ that are not transition params. 

\begin{align}
	\pderiv{NLL}{v_{dat}} 
		&=  - \sum_{i=1}^{N} \left[ \sum_{t = 1}^{T} \pderiv{}{v_{dat}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
		- \pderiv{}{v} \log Z(\vec{x}^{(i)}) \right] \\
		&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
		 - \inv{Z(\vec{x}^{(i)})} \sum_{\slice[T]{y}}   
		 \left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
		 \sum_t \frac{\pderiv{}{v} \sum_{\vec[t]{z}}  \psi_t }{ \sum_{\vec[t]{z}} \psi_t   }
		 \right]
		  \right] \\ 
		&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
				- \inv{Z(\vec{x}^{(i)})} \sum_{\slice[T]{y}}   
				\left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
				\sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
				\right]
				\right] \\
		&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
				- \sum_t \sum_y \sum_{y'} \left[\sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right] \xi_{t,y,y'}
				\right] \\
		&= - \sum_{i=1}^{N} \left[ \sum_t \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
				- \sum_t \sum_y \left[\sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right] \gamma_{t,y}
				\right] \\
		&= - \sum_{i=1}^{N} \left[ \sum_t \left( \sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}}
		 - \sum_y \left[\sum_h \sigma\left(o\left(h, y_t\right)\right) \pderiv{o(h, y_t)}{v_{dat}} \right] \gamma_{t,y}
		\right) \right] \\
		&= - \sum_{i=1}^{N} \left[ \sum_t \sum_y \left(    \left(
			\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sum_h   \sigma\left(o(h, y_t)\right) \pderiv{o(h, y_t)}{v_{dat}} \right) \right] 
\end{align}
NOTE: Although I haven't thoroughly checked the last few steps, they are required to be true in order to match the paper's results.
\end{example}
 


\begin{example}[Transition parameters]
\begin{align}
	\pderiv{NLL}{v_{tr}} 
		&=  - \sum_{i=1}^{N} \left[ \sum_{t = 1}^{T} \pderiv{}{v_{tr}} \log\left( \sum_{\vec[t]{z}} \psi_t \right) 
		- \pderiv{}{v_{tr}} \log Z(\vec{x}^{(i)}) \right] \\
		&= -\sum_i^N \left[ 
			\sum_t \pderiv{}{v_{tr}} \left[ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right]
			- \pderiv{}{v_{tr}} \log Z(\vec{x}^{(i)}) 
			\right]\\
		&= - \sum_{i=1}^{N} \left[ \sum_t  \pderiv{}{v_{tr}} \left[ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right]
			- \inv{Z(\vec{x}^{(i)})} \sum_{\slice[T]{y}}   
			\left[ \prod_t \sum_{\vec[t]{z}} \psi_t \right] \left[
			\sum_t  \pderiv{}{v_{tr}} \left[  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right]
			\right]
			\right] \\
		&= - \sum_{i=1}^{N} \left[ \sum_t \sum_y \left(    \left(
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		 \pderiv{}{v_{tr}} \left[  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right] 
		 \right)\right] 
\end{align}
\end{example}



\begin{example}[Boundary parameters]
	\begin{align}
	\pderiv{NLL}{\pi_\ell} 
	&=  - \sum_{i=1}^{N} \left[ \ind_{y_1 = \ell} - \gamma_{1, \ell} \right] \\
	\pderiv{NLL}{\tau_\ell} 
		&=  - \sum_{i=1}^{N} \left[ \ind_{y_T = \ell} - \gamma_{T, \ell} \right]
	\end{align}
\end{example}


The results of each of the boxes above are summarized below, for the case of $N=1$ to save space.
\graybox{
	\pderiv{NLL}{v_{dat}} 
		&= -\sum_t \sum_y \left( \left(
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sum_h   \sigma\left(o(h, y)\right) \pderiv{o(h, y)}{v_{dat}} \right) \\
	\pderiv{NLL}{v_{tr}} 
		&= -\sum_t \sum_y \left(    \left(
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\pderiv{}{v_{tr}} \left[  \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y} \right] 
		\right) \\
	\pderiv{NLL}{\pi_\ell} 
		&=  - \sum_{i=1}^{N} \left[ \ind_{y_1 = \ell} - \gamma_{1, \ell} \right] \\
	\pderiv{NLL}{\tau_\ell} 
		&=  - \sum_{i=1}^{N} \left[ \ind_{y_T = \ell} - \gamma_{T, \ell} \right]
}

Now I'll further go through and show how the equations simplify for each type of data-dependent parameter.
\begin{align}
	\pderiv{NLL}{W_{c,h}}
		&= -\sum_t \sum_y \left( \left(
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sum_{h'}   \sigma\left(o(h', y)\right) \pderiv{}{W_{c,h}} \left( b_{h'} + c_y + V_{h',y} + \vec[h']{w}^T \vec[t]{x}  \right) \right) \\
		&= -\sum_t \sum_y \left( \left(
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sum_{h'}  \sigma\left(o(h, y)\right) \ind_{h=h'}\ind_{c \in \vec[t]{x}} \right) \\
		&= -\sum_t \sum_y \left( 
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		 \sigma\left(o(h, y)\right) \ind_{c \in \vec[t]{x}}  \\
	\pderiv{NLL}{V_{h,y}} 
		&= -\sum_t \left( 
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sigma\left(o(h, y)\right)   \\
	\pderiv{NLL}{b_h}
		&= -\sum_t \sum_y \left( 
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sigma\left(o(h, y)\right)  \\
	\pderiv{NLL}{c_y}
		&= -\sum_t  \left( 
		\ind_{y_t = y} -  \gamma_{t,y} \right) 
		\sum_h \sigma\left(o(h, y)\right)   \\
\end{align}


\myspace
\p \blue{Alternative Approach}. The above was a bit more cumbersome than needed. I'll now derive it in an easier way. 

\begin{small}
	\begin{align}
	p(\vec y \mid \vec x) 
	&= \frac{\exp\left\{ \vec[1]{y}^T \vec{\pi} + \vec[T]{y}^T \vec{\tau}  \right\}}{Z(\vec x)} \prod_{t=1}^{T}\bigg[ 
	\exp\{ \vec{c}^T \vec[t]{y} + \vec[t-1]{y}\matr{A}\vec[t]{y}   \}  
	\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)	\bigg] \\
	&= \frac{\exp\{ I + T \}}{Z(\vec x)} \prod_{t=1}^{T}
	\prod_{h = 1}^{H} \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right)
	\\
	NLL 
		&= - \sum_{i=1}^{N} \log p(\vec{y}^{(i)} \mid \vec{x}^{(i)}) \\
		&= - \sum_{i=1}^{N} \left[ 
		I + T + 
		\sum_{t = 1}^{T} \sum_{h=1}^{H} \left[ \log \left( 1 + \exp\{ b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y}  \} \right) \right]
		- \log Z(\vec{x}^{(i)})
		\right]
	\end{align}
\end{small}

Now, focusing on the regular log-likelihood for a single example, we have
\begin{align}
	\pderiv{\mathcal{L}_i}{v}
		&= \pderiv{}{v} \log p(\vec{y}^{(i)}, \vec{x}^{(i)}) \\
		&= \pderiv{}{v} \left[ 
			I + T + 
			\sum_{t, h} \log\left( 1 + \exp\left\{
				b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y} 
				\right\}\right) - 
			\log Z(\vec{x}^{(i)})
			\right] \\
	\pderiv{\log Z(\vec{x}^{(i)})}{v} 
		&= \inv{Z(\vec{x}^{(i)})}   \sum_{\slice[T]{y}} \pderiv{}{v} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)}) \\
		&= \inv{Z(\vec{x}^{(i)})}   \sum_{\slice[T]{y}} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)}) \pderiv{}{v} \log\widetilde{p}(\vec{y} \mid \vec{x}^{(i)})
\end{align}
as our base formula for partial derivatives. 

\begin{example}[Transition parameters]
	\begin{align}
	\pderiv{\mathcal{L}_i}{A_{i,j}} 
	&= \pderiv{}{A_{i,j}} \left[ 
		I + T + 
		\sum_{t, h} \log\left( 1 + \exp\left\{
		b_h + \vec[h]{w}^T \vec[t]{x} + \vec[h]{v}^T \vec[t]{y} 
		\right\}\right) - 
		\log Z(\vec{x}^{(i)})
		\right] \\
	&= \pderiv{}{A_{i,j}} \left[ I + T \right]
		- \sum_{\slice[T]{y}}  p(\vec{y} \mid \vec{x}^{(i)}) \pderiv{}{A_{i,j}} \left[ 
			y_1^T \pi + y_T^T \tau + \sum_t y_{t-1}Ay_t \right] \\
	&= \sum_t \ind_{y_{t-1}^{(i)}=i} \ind_{y_{t}^{(i)}=j}
		- \sum_{\slice[T]{y}} \inv{Z(\vec{x}^{(i)})} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)}) \sum_{t=1}^{T} \ind_{y_{t-1}=i} \ind_{y_{t}=j} \\
	&= \sum_t \ind_{y_{t-1}^{(i)}=i} \ind_{y_{t}^{(i)}=j}
		- \sum_t \sum_{y_t} \sum_{y_{t-1}}   \ind_{y_{t-1}=i} \ind_{y_{t}=j} \sum_{ \slice[t-2][1]{y} } \sum_{  \slice[T][t+1]{y}} \inv{Z(\vec{x}^{(i)})} \widetilde{p}(\vec{y} \mid \vec{x}^{(i)})
	\end{align}
\end{example}










% ============================================================================================
\lecture{Papers and Tutorials}{Pre-training of Hidden-Unit CRFs}{June 30, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Kim et al., ``Pre-training of Hidden-Unit CRFs,'' (2018).}


\p \blue{Model Definition}. The Hidden-Unit CRF (HUCRF) accepts the usual observation sequence $\vec x = x_1, \ldots, x_n$, and associated label sequence $\vec y = y_1, \ldots y_n$ for training. The HUCRF also has a hidden layer of binary-valued $\vec z = z_1 \ldots z_n$. It defines the joint probability
\graybox{
	p_{\theta, \gamma}(\vec y, \vec z \mid \vec x) &= \dfrac{
		\exp\big( \vec{\theta}^T \Phi(\vec x, \vec z) + \vec{\gamma}^T \Psi(\vec z, \vec y)  \big)
		}{
		\sum_{\vec{z}', \vec{y}' \in \mathcal{Y}(\vec x, \vec{z}')} 
		\exp\big( \vec{\theta}^T \Phi(\vec x, \vec{z}') + \vec{\gamma}^T \Psi(\vec{z}', \vec{y}')  \big)
		}
}
where
\begin{compactitem}
	\item $\mathcal{Y}(\vec x, \vec z)$ is the set of all possible label sequences for $\vec x$ and $\vec z$. 
	
	\item $\Phi(\vec x, \vec z) = \jnsum \phi(x, j, z_j)$
	
	\item $\Psi(\vec z, \vec y) = \jnsum \psi(z_j, y_{j-1}, y_j)$. 
\end{compactitem}
Also note that we model $(z_i \perp z_{j \ne i} \mid \vec{x}, \vec{y})$. 

\myspace 
\p \blue{Pre-training HUCRFs}. Since the objective for HUCRFs is non-convex, we should choose a better initialization method than random initialization. This is where pre-training comes in, a simple 2-step approach:
\begin{compactenum}
	\item Cluster observed tokens from $M$ unlabeled sequences and treat the clusters as labels to train an intermediate HUCRF. Let $C(u^{(i)})$ be the sequence of cluster assignments/labels for the unlabeled sequence $u^{(i)}$. We compute:
	\begin{align}
		(\theta_1, \gamma_1) &\approx \argmax_{\theta, \gamma} \sum_{i = 1}^{M} \log p_{\theta, \gamma}(C(u^{(i)}) \mid u^{(i)} )
	\end{align}
	
	\item Train a final model on the labeled data $\{  (x^{(i)}, y^{(i)}  ) \}_{i = 1}^{N}$, using $\theta_1$ as an initialization point:
	\begin{align}
		(\theta_2, \gamma_2) &\approx \argmax_{\theta, \gamma} \sum_{i=1}^{N} \log p_{\theta, \gamma}(y^{(i)} \mid x^{(i)})
	\end{align}
\end{compactenum}
Note that pre-training only defines the initialization for $\theta$, the parameters between $\vec x$ and $\vec z$. We still train $\gamma$, the parameters from $\vec{z}$ to $\vec y$, from scratch. 

\myspace
\p \blue{Canonical Correlation Analysis} (CCA). A general technique that we will need to understand as a prerequisite for the multi-sense clustering approach (defined in the next section). Given $n$ samples of the form $(x^{(i)}, y^{(i)})$, where each $x^{(i)} \in \{0, 1\}^d$ and $y^{(i)} \in \{0, 1\}^{d'}$, CCA returns \textit{projection matrices} $A \in \R^{d \times k}$ and $B \in \R^{d' \times k}$ that we can use to project the samples to $k$ dimensions:
\begin{align}
	x &\longrightarrow A^T x \\
	y &\longrightarrow B^T y 
\end{align}
The CCA algorithm is outlined below.
\begin{example}[Algorithm: CCA]
	\begin{compactenum}
		\item Calculate $\matr{D} \in \R^{d \times d'}$, $\vec u \in \R^d$, and $\vec v \in \R^{d'}$ as follows:
		\begin{align}
			D_{i,j} &= \sum_{l = 1}^{n} \ind_{x_i^{(l)} = 1}  \ind_{y_j^{(l)} = 1} \\
			u_i &= \sum_{l = 1}^{n}  \ind_{x_i^{(l)} = 1} \\
			v_i &= \sum_{l = 1}^{n}  \ind_{y_i^{(l)} = 1}
		\end{align}
		
		\item Define $\hat{\Omega} = \text{diag}(\vec u)^{-1/2} ~ \matr{D} ~ \text{diag}(\vec v)^{-1/2}$. 
		
		\item Calculate rank-$k$ SVD $\hat{\Omega}$. Let $\matr U \in \R^{d \times k}$ and $\matr V \in \R^{d' \times k}$ contain the left and right, respectively, singular vectors for the largest $k$ singular values. 
		
		\item Return $A = \text{diag}(\vec u)^{-1/2} \matr U$ and $B = \text{diag}(\vec v)^{-1/2} \matr V$. 
	\end{compactenum}
\end{example}

\myspace
\p \blue{Multi-sense clustering}. For each word type, use CCA to create a set of context embeddings corresponding to all occurrences of that word type. Then, cluster these embeddings with $k$-means. Set the number of word senses $k$ to the number of label types occurring in the labeled data.\\

\red{TODO}: finish this note




% ============================================================================================
\lecture{Papers and Tutorials}{Structured Attention Networks}{July 07, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Kim et al., ``Structured Attention Networks,'' (2017).}

\p \blue{Background: Attention Networks}. Goal: produce a context $c$ based on input sequence $x$ and query $q$. We assume we have an attention distribution\footnote{Also called the ``alignments''. It is the output of the softmax layer of attention scores in the majority of cases.} $z \sim p(z \mid x, q)$. Interpret $z$ as a categorical latent variable over $T$ categories, where $T = len(x)$ is the length of the input sequence. We can then compute the context, $c = \E[z \sim p(z \mid x, q)]{f(x, z)}$, where $f(x, z)$ is an \textit{annotation function}\footnote{In all applications I've seen, $f(x, z) = x_z$.}. 
\vspace{-0.6em}
\begin{quote}
	{\small\itshape
		We interpret the attention mechanism as taking the expectation of an annotation function $f(x, z)$ with respect to a latent variable $z \sim p$, where $p$ is parameterized to be a function of $x$ and $q$.
	}
\end{quote}
For comparisons later on with the traditional attention mechanism, here it is:
\begin{align}
	c &= \sum_t^T p(z = t \mid x, q) \vec[t]{x} \label{eq:san-1} \\
	p(z = t \mid x, q) &= \text{softmax}(\theta_t) 
\end{align}
where usually $x$ is the sequence of hidden state of the encoder RNN, q is the hidden state of the decoder RNN at the most recent time step, $z$ gives the source position to be attended to, and $\theta_t = \text{score}(x_t, q)$.

\myspace
\p \blue{Structured Attention}. In a \green{structured attention model}, $z$ is now a \textit{vector} of discrete random variables $z_1, \ldots, z_m$ and the attention distribution $p(z \mid x, q)$ is now modeled as a \textit{conditional random field}, specifying the structure of the $z$ variables. We also assume now that the annotation function $f$ factors into clique annotation functions $f(x, z) = \sum_C f_C(x, z_C)$, where the summation is over the $C$ factors, $\psi_C$, of the CRF. Our context vector takes the form:
\begin{align}
	c = \E[z \sim p(z \mid x, q)]{f(x, z)} &= \sum_{C} \E[z \sim p(z_C \mid x, q)]{f_C(x, z_C)} \\
	p(z \mid x, q) &= \inv{Z(x, q)} \prod_C \psi_C(z_C) 
\end{align}

\myspace 
\p \blue{Example 1: Subsequence Selection}. Let $m = T$, and let each $z_i \in \{0, 1\}$ be a binary R.V. Let $f(x, z) = \sum_{t}^{T} f_t(x, z_t) = \sum_t^T \vec[t]{x} \ind_{z_t=1}$\footnote{Ok, so equivalently, $z^T x$, i.e. the indicator function can just be replace by $z_t$ here}. This yields the context vector,
\begin{align}
	c = \E[z_1, \ldots, z_T]{f(x, z)} = \sum_t^T p(z_t = 1 \mid x, q) \vec[t]{x} \label{eq:san-2}
\end{align}
Although this looks similar to equation \ref{eq:san-1}, we haven't yet revealed the functional form for $p(z \mid x, q)$. Two possible choices:\marginnote{The factor $\psi$ for the CRF is \textbf{NOT} the same as the factor for the Bernoulli!}[3em]
\begin{align}
\mtgreen{Linear-Chain CRF:}&\qquad p(z_1, \ldots, z_T\mid x, q) = \inv{Z(x, q)} \prod_t^T \psi_t(z_t, z_{t-1}) \\
\mtgreen{Bernoulli:}&\qquad p(z_1,\ldots, z_T \mid x, q) = \prod_t^T p(z_t = 1 \mid x, q) = \prod_t^T \sigma(\psi_t(z_t))
\end{align}
These show why equation \ref{eq:san-2} is fundamentally different than equation \ref{eq:san-1}:
\begin{compactitem}
	\item It allows for multiple inputs (or no inputs) to be selected for a given query.
	\item We can incorporate structural dependencies across the $z_t$'s. 
\end{compactitem}
Also note that all methods can use potentials from the same neural network or RNN that takes x and q as input. By this we mean, for example, that we can take the same parameters we'd use when computing the scores in our attention layer, and reinterpret them as e.g. CRF parameters. Then, we can compute the marginals $p(z_t \mid x)$ using the forward-backward algorithm\footnote{This is different than the simple softmax we usually use in an attention layer, which does not model any interdependencies between the $z_t$. The marginals we end up with when using the CRF originate from a \textit{joint} distribution over the entire sequence $z_1, \ldots z_T$. This seems potentially incredibly powerful. Need to analyze in depth.}.
\vspace{-0.5em}
\begin{quote}
	{\small\itshape Crucially this generalization from vector softmax to forward-backward is just a \textbf{series of differentiable steps}, and we can compute gradients of its output (marginals) with respect to its input (potentials), allowing the structured attention model to be trained end-to-end as part of a deep model.
	}
\end{quote}




% ============================================================================================
\lecture{Papers and Tutorials}{Neural Conditional Random Fields}{July 08, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Do and Artieres, ``Neural Conditional Random Fields,'' (2010).}


\p \blue{Neural CRFs}. Essentially, we feed the input sequence $\vec x$ through a feed-forward network whose output layer has a linear activation function. The output layer is then connected with the target variable sequence $\rvec Y$. In other words, instead of feeding instances $\vec x$ of the observation variables $\rvec X$, we feed the hidden layer activations of the NN. This results in the conditional probability\marginnote{We can set $\Phi_c = \Phi$ for a shared-weights approach}[2em]
\graybox{
	p(\vec y \mid \vec x) &\propto \prod_{c \in C} e^{-E_c(\vec x, \vec[c]{y}, \vec w) }
	= \prod_{c \in C} e^{  \langle \vec[c]{w}^{\vec[c]{y}}, \vec[c]{\Phi}(\vec x, \vec[NN]{w})   \rangle   }
}
where
\begin{compactitem}
	\item $\vec[NN]{w}$ are the weights for the NN. 
	\item $\vec[c]{w}^{\vec[c]{y}}$ are the weights (for clique $c$) for the CRF. 
	\item $ \vec[c]{\Phi}(\vec x, \vec[NN]{w}) $ is the output of the NN. It symbolizes the high-level feature representation of the input $\vec x$ at clique $c$ computed by the NN. 
\end{compactitem}
The authors refer to the linear output layer (containing the CRF weights) as the \textit{energy outputs}. For the sake of writing this in more familiar notation for the linear-chain CRF case, here is the above equation translated for the case where  each clique corresponds to a timestep $t$ of the input sequence and is either a label-label clique or a state-label clique. 
\begin{align}
p(\vec y \mid \vec x) &= \inv{Z(x)} \prod_t^T \exp\left\{ - E_t(\vec x, \vec[t]{y}, \vec[t-1]{y}, \vec w) \right\} \\
	&= \inv{Z(x)} \prod_t^T \exp\left\{ -E_{loc}(\vec x, t, y_t, \vec w) - E_{trans}(\vec x, t, y_{t-1}, y_t, \vec w)  \right\} \\
\end{align}
where the authors are using a blanket $\vec w$ to denote all model parameters\footnote{Also note that the authors allow for utilizing the input sequence $\vec x$ in the transition energy function, $E_{trans}$, although usually we implement $E_{trans}$ using only $y_{t-1}$ and $y_{t}$.}.


\myspace
\p \blue{Initialization \& Fine-Tuning}. The hidden layers of the NN are initialized layer-by-layer in an unsupervised manner using RBMs. It's important to note that the hidden layers of the NN consist of binary units. Then, using the pre-trained hidden layers, the CRF layer is initialized by training it in the usual way, and keeping the pretrained NN weights fixed. \\

Next, fine-tuning is used to learn all parameters globally.
\begin{align}
	\pderiv{L(\vec w)}{\vec w} 
		&= \inv{n}\sum_{i=1}^n \pderiv{L_i(\vec w)}{\vec w} \\
	\pderiv{L_i(\vec w)}{\vec w} 
		&= \pderiv{L_i(\vec w)}{\matr{E}(\vec{x}^{(i)})} \pderiv{\matr{E}(\vec{x}^{(i)})}{\vec w} \\
	\left[ \matr{E}(\vec{x}^{(i)}) \right]_t	
		&= E_{loc}(\vec x, t, y_t, \vec w) + E_{trans}(\vec x, t, y_{t-1}, y_t, \vec w) \\
\end{align}
where $\pderiv{\matr[i]{E}}{\vec w}$ is the Jacobian matrix of the NN outputs for input sequence $\vec{x}^{(i)}$ w.r.t. weights $\vec w$. By setting $\pderiv{L_i(\vec w)}{\matr[i]{E}}$ as backprop errors of the NN output units, we can backpropagate and get $\pderiv{L_i(\vec w)}{\vec w}$ using the chain rule over the hidden layers.






% ============================================================================================
\lecture{Papers and Tutorials}{Bidirectional LSTM-CRF Models for Sequence Tagging}{July 08, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Huang et al., ``Bidirectional LSTM-CRF Models for Sequence Tagging,'' (2015).}

\p \blue{BI-LSTM-CRF Networks}. Consider the matrix of scores $f_{\theta}(\vec x)$ for input sentence $\vec x$. The element $\left[ f_{\theta} \right]_{\ell, t}$ gives the score for label $\ell$ with the $t$-th word. This is output by the LSTM network parameterized by $\theta$. We let $[A]_{i,j}$ denote the transition score from label $i$ to label $j$ within the CRF. The total set of parameters is denoted $\widetilde{\theta} = \theta \cup \matr{A}$. The total score for input sentence $\vec x$ and predicted label sequence $y$ is then 
\begin{align}
	s(\vec x, \vec y, \widetilde{\theta}) &= \sum_t^T \left(   A_{y_{t-1}, y_t} + \left[ f_{\theta}  \right]_{y_t, t}  \right)
\end{align}


\myspace
\p \blue{Features}. The authors incorporate 3 distinct types of input features:
\begin{compactitem}
	\item \textbf{Spelling features}. Various standard lexical/syntactical features. One-hot encoded as usual.
	\item \textbf{Context features}. Unigram, bigram, and sometimes tri-gram features. One-hot encoded.
	\item \textbf{Word embeddings}. Distinct from the word features. Use a pretrained embedding for each word. 
\end{compactitem}
Although it's not entirely clear, it appears they concatenate all of the aforementioned features together as input to the BI-LSTM. This necessarily means they are learning an embedding for the one-hot encoded spelling and word features. They also add direct connections from the input to the CRF for the spelling and word features. \\

EDIT: they may actually replace the one-hot encoded word features with the word embeddings. Unclear.












% ============================================================================================
\lecture{Papers and Tutorials}{Relation Extraction: A Survey}{July 16, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Pawar et al., ``Relation Extraction: A Survey,'' (December 2017).}



\p \blue{Feature-based Methods}. For each entity pair $(e_1, e_2)$, generate a set of features and train a classifier to predict the relation\footnote{If there are N unique relations for our data, it is common to train the classifier to predict $2N$ total relations, to handle both possible orderings of relation arguments.}. Some useful features are shown in the figure below.
	\myfig[0.6\textwidth]{figs/re_feature_types.png}
	
	Authors found that SVMs outperform MaxEnt (logistic reg) classifiers for this task.
	

\myspace
\p \blue{Kernel methods}. Instead of explicit feature engineering, we can design kernel functions for computing similarities between representations of two relation instances\footnote{Recall that kernel methods are for the general optimization problem 
	\begin{align}
		\min_w \insum \text{loss}\left( \jnsum \alpha_j \phi(x_j)^T \phi(x_i), y_i  \right) + \lambda \jnsum \knsum \alpha_j \alpha_k \phi(x_j)^T\phi(x_k)
	\end{align}
which changes the direct focus from feature engineering to ``similarity'' engineering.
} (a relation instance is a triplet of the form $(e_1, e_2)$), and SVM for the classification. \\

\Needspace{10\baselineskip}
One approach is the \green{sequence kernel}. We represent each relation instance as a sequence of feature vectors:
$$
	(e_1, e_2) \rightarrow (\vec[1]{f}, \ldots, \vec[N]{f})
$$
where $N$ might be e.g. the number of words between the two entities, and the dimension of each $f$ is the same, and could correspond to e.g. POS tag, NER tag, etc. More formally, define the \textit{generalized subsequence kernel}, $K_n(s, t, \lambda)$, that computes some number of weighted subsequences $u$ such that
\begin{compactitem}
	\item There exist index sequences $ii := (i_1, \ldots i_n)$ and $jj := (j_1, \ldots j_n)$ of length $n$ such that 
	\begin{align}
		u_i &\in \vec[i]{s} \qquad \forall i \in  ii \\
		u_j &\in \vec[j]{t} \qquad \forall j \in jj \\ 
	\end{align}
	
	\item The weight of $u$ is $\lambda^{l(ii) + l(jj)}$, where $l(x) = \max(x) - \min(x)$ and $0 < \lambda < 1$. Sparser (more spaced out) subsequences get lower weight.
\end{compactitem}
The authors then provide the recursion formulas for $K$, and describe some extensions of sequence kernels for relation extraction.\\


\green{Syntactic Tree Kernels}.\marginnote{Syntactic Tree Kernels}[2em] Structural properties of a sentence are encoded by its constituent parse tree. The tree defines the syntax of the sentence in terms of constituents such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), POS tags (NN, VB, IN, etc.) as non-terminals and actual words as leaves. The syntax is usually governed by Context Free Grammar (CFG). Constructing a constituent parse tree for a given sentence is called \textit{parsing}. The \purple{Convolution Parse Tree Kernel} $K_T$ can be used for computing similarity between two syntactic trees.\\

\green{Dependency Tree Kernels}. For grammatical relations between words in a sentence. Words are the nodes and dependency relations are the edges (in the tree), typically from dependent to parent. In the \purple{relation instance representation}, we use the smallest subtree containing the entity pair of a given sentence. Each node is augmented with additional features like POS, chunk, entity level (name, nominal, pronoun), hypernyms, relation argument, etc.  Formally, an \textit{augmented dependency tree} is defined as a tree $T$ where\marginnote{Dependency Tree Kernels}[-3em]
\begin{compactitem}
	\item Each node $t_i$ has features $\phi(t_i) = \{v_1, \ldots, v_d \}$. 
	
	\item Let $t_i[\vec c]$ denote all children of $t_i$, and let $Pa(t_i)$ denote its parent. 
	
	\item For comparison of two nodes we use:
	\begin{compactitem}
		\item \textbf{Matching function} $m(t_i, t_j)$: equal to 1 if some important features are shared between $t_i$ and $t_j$, else 0. 
		\item \textbf{Similarity function} $s(t_i, t_j)$: returns a positive real similarity score, and defined as
		\begin{align}
			s(t_i, t_j)
				&= \sum_{v_q \in \phi(t_i)}\sum_{v_r \in \phi(t_j)} \text{Compat}(v_q, v_r)
		\end{align}
		over some compatibility function between two feature values. 
	\end{compactitem}
\end{compactitem}
Finally, we can define the overall dependency tree kernel $K(T_1, T_2)$ for similarity between trees $T_1$ and $T_2$ as follows. Let $r_i$ denote the root node of tree $T_i$.\marginnote{
	$$a_1 \le a_2 \le \ldots \le a_n$$
	$$ d(\vec a) \triangleq a_n - a_1 + 1 $$
	$$ 0 < \lambda < 1 $$
}[1em]
\graybox{
	K(T_1, T_2) 
		&= \begin{cases}
			0 & \text{if } m\left(r_1, r_2\right) = 0 \\
			s(r_1, r_2) + K_c(r_1[\vec c], r_2[\vec c]) & \text{otherwise}
		\end{cases}\\
	K_c(t_i[\vec c], t_j[\vec c])
		&= \sum_{\vec a, \vec b, l(\vec a)=l(\vec b)} \lambda^{d(\vec a) + d(\vec b)} K(t_i[\vec a], t_j[\vec b])
}
The interpretation is that, whenever a pair of matching nodes is found, \textit{all} possible matching subsequences\footnote{Note that a summation over subsequences of a sequence $\vec a$, denoted here as $\sum_{\vec a}$, expands to $\{a_1, \ldots a_n, a_1 a_2, a_1 a_3, \ldots a_1 a_n, a_1 a_2 a_3, \ldots, a_2 a_5 a_6, \ldots \}$ and so on and so forth.} of their children are found. Two subsequences $\vec a$ and $\vec b$ are said to ``match'' if $m(a_i, b_1) = 1 (\forall i < n)$. Similar to the sequence kernel seen earlier, $\lambda$ is a decay factor that penalizes sparser subsequences.






















% ============================================================================================
\lecture{Papers and Tutorials}{Neural Relation Extraction with Selective Attention over Instances}{July 16, 2018}
% ============================================================================================
\vspace{-1em}
{\footnotesize Lin et al., ``Neural Relation Extraction with Selective Attention over Instances,'' (2016).}

\p \blue{Introduction}. A common distant supervision approach for RE is aligning a KB with text. For any $(e_1, r, e_2)$ in the KB, it assumes that all text mentions of $(e_1, e_2)$ express the relation $r$. Of course, this assumption will often not be true. This motivates the notion of \green{multi-instance learning}, wherein we predict whether a set of instances $\{x_1, \ldots x_n\}$ (each of which contain mention(s) of $(e_1, e_2)$) imply the existence of $(e_1, r, e_2)$ being true.

\myspace
\p \blue{Input Representation}. Each instance sentence $x$ is tokenized into a sequence of words. Each word $w_i$ is transformed into a concatenation $\vec[i]{w} \in \R^{d}$ ($d = d_w + 2 d_{pos}$), 
\begin{align}
	\vec[i]{w} := [\text{word2Vec}(w_i); \text{dist}(w_i, e_1); \text{dist}(w_i, e_2)]
\end{align} 
where dist(a, b) returns the [embedded] relative distance (num tokens) between a and b in the given sentence (positive integer)\footnote{More specifically, it is an embedded representation of the relative distance. To actually implement it,  you'd first shift the relative distances such that they begin at 0, and learn 2 * window\_size + 1 embedding vectors for each of the possible position offsets. Anything outside the window is embedded into the zero vector.}.

\myspace
\p \blue{Convolutional Network}. We use a CNN to encode a sentence of embeddings $\{ \vec[1]{w}, \ldots, \vec[T]{w}  \}$ into a single sentence vector representation $\vec x$. Denote the kernel/filter/window size as $\ell$ and the number of words in the given sentence as $T$. Let $\vec[i]{q} \in \R^{\ell \cdot d}$ denote the vector for the $i$th window,
\begin{align}
	\vec[i]{q} &= \vec[i-\ell+1:i]{w} \qquad (1 \le i \le T + \ell - 1)
\end{align}
and let $\matr{Q} \in \R^{(T + \ell - 1) \times \ell \cdot d}$ be defined such that row $\matr[i]{Q} = \vec[i]{q}^T$. 
It follows that, for convolution matrix $\matr W \in \R^{K \times (\ell \cdot d)}$, the output of the $k$th filter, and subsequent max-pooling, is
\begin{align}
	\vec[k]{p} &= \left[ \matr W \matr{Q}^T \right]_k + \vec  b \\
	\left[\vec x\right]_k &= \left[
		\max(\vec[k1]{p});
		\max(\vec[k2]{p});
		\max(\vec[k3]{p})
	\right]
\end{align}
where we've divided $\vec[k]{p}$ into three segments, corresponding to before entity 1, middle, and after entity 2 of the given sentence. The sentence vector $\vec x \in \R^{3K}$ is the concatenation of all of these, after feeding through a non-linear activation function like a ReLU. 

\myspace
\p \blue{Selective Attention over Instances}. An attention mechanism is employed over all $n$ sentence instances $x_i$ for some candidate entity pair $(e_1, e_2)$. The output is a \textbf{set vector} $\vec s$, a real-valued vector representation of the set of instances, where
\begin{align}
	\vec s &= \sum_i \alpha_i \vec[i]{x} \\
	\alpha_i &= \text{softmax}(\vec[i]{x} \matr{A} \vec{r})
\end{align}
is an attention-weighted sum over the instance embeddings. Note that they constrain $\matr A$ to be diagonal. Finally, the predictive distribution is defined as
\graybox{
	p(r \mid \mathcal{S}, (e_1, e_2) \theta)
		&= \text{softmax}\left(\matr{M}\vec{s} + \vec{d} \right)_r 	
}
where $\mathcal{S}$ is the set of $n$ sentences for the given entity pair $(e_1, e_2)$. 






% ============================================================================================
\lecture{Papers and Tutorials}{On Herding and the Perceptron Cycling Theorem}{July 31, 2018}
% ============================================================================================

\vspace{-1em}
{\footnotesize Gelfand et al., ``On Herding and the Perceptron Cycling Theorem,'' (2010).}


\p \blue{Introduction}. Begin with the familiar learning rule of Rosenblatt's perceptron, after some \textit{incorrect} prediction $\hat{y_i} = \text{sgn}(\vec{w}^T \vec[i]{x})$,
\begin{align}
	\vec{w} \leftarrow \vec{w} + \vec[i]{x}(y_i - \hat{y_i})
\end{align}
which has the effect that a subsequent prediction on $\vec[i]{x}$ will (before taking the sign) be $||\vec[i]{x}||_2^2$ closer to the correct side of the hyperplane. The \green{perceptron cycling theorem} (PCT) states that if the data is \textit{not} linearly separable, the weights will still remain bounded and won't diverge to infinity. \textbf{This paper shows that the PCT implies that certain moments are conserved on average}. Formally, their result says that, for some $N$ number of iterations over samples selected from training data (with replacement)\footnote{Unclear whether this is only for samples that corresponded to an update, or just all samples during training.},
\graybox{
	\bigg|\bigg| 
		\inv{N} \sum_{i}^{N} \vec[i]{x} y_i - \inv{N} \sum_i^N \vec[i]{x}\hat{y_i}
	\bigg|\bigg|	
	\sim 
	\mathcal{O}\left(\inv{N}\right)
}
where it's important to remember that $\hat{y_i}$ here is the prediction for $\vec[i]{x}$ when it was encountered at that training iteration. This result shows that perceptron learning generate predictions that correlate with the input attributes the same way as the true labels do, and [the correlations] converge to the sample mean with a rate of 1/N. This also hints at why averaged perceptron algorithms (using the average of weights across training) makes sense, as opposed to just selected the best weights. This paper also shows that \textit{supervised perceptron algorithms and unsupervised herding algorithms can all be derived from the PCT}. \\

Below are some theorems that will be used throughout the paper. Let $\{\vec[t]{w}\}$ be a sequence of vectors $\vec[t]{w} \in \R^{D}$, each generated according to iterative updates $\vec[t+1]{w} = \vec[t]{w} + \vec[t]{v}$, where $\vec[t]{v}$ is an element of a \textit{finite} set $\matr V$, and the norm of $\vec[t]{v}$ is bounded: $\max ||\vec[t]{v}|| = R < \infty$. 
\begin{definition}
	\green{PCT}: $\forall t \ge 0$: If $\vec[t]{w}^T \vec[t]{v} \le 0$, $\exists M > 0$ s.t. $||\vec[t]{w} - \vec[0]{w}|| < M$. \\
	\green{Convergence Thm}: If PCT holds, then $||\tfrac{1}{T} \sum_{t=1}^T \vec[t]{v} || \sim \mathcal{O}{\tfrac{1}{T}}$. 
\end{definition}


\myspace
\p \blue{Herding}. Consider a fully observed Markov Random Field (MRF) over $m$ variables, each of which can take on an integer value in the range $[1, K]$. In herding, our energy function and weight updates for observation $\vec x$ (over all $m$ variables in $\mathcal X$),
\graybox{
	E(\vec x) 
		&= - \vec{w}^T \phi(\vec x) \\
	\vec[t+1]{w} 
		&= \vec[t]{w} + \bar{\phi} - \phi(\vec[t]{x}^*) \\ 
	\text{where}\qquad \bar{\phi} 
		&= \E[\vec{x}^{(i)} \sim p_{data}]{\phi(\vec{x}^{(i)})} \\
	\text{and}\qquad
	\vec[t]{x}^* &= \argmax_{\vec x} \vec[t]{w}^T \phi(\vec x)
}
What if we consider more complicated features that depend on the weights $\vec w$? This situation may arise in e.g. models with hidden units $\vec z$, where our feature function would take the form $\phi(\vec x, \vec z)$, and we always select $\vec z$ via
\begin{align}
	\vec{z}(\vec x, \vec w)
		&= \argmax_{\vec{z}'} \vec{w}^T \phi(\vec x, \vec{z}')
\end{align}
and therefore our feature function $\phi$ depends on weights $\vec w$ through $\vec z$. In this case, our herding update terms from above take the form
\begin{align}
	\bar{\phi}
		&= \E[\vec{x}^{(i)} \sim p_{data}]{\phi(\vec{x}^{(i)}, \vec{z}(\vec{x}^{(i)}, \vec{w})  )} \\
	\vec[t]{x}^*, \vec[t]{z}^*
		&= \argmax_{\vec x, \vec z} \vec[t]{w}^T \phi(\vec x, \vec z)
\end{align}

\myspace
\p \blue{Conditional Herding}. Main contribution of this paper. It's basically identical to regular herding, but now we decompose $\vec x$ into inputs and outputs $(\vec x, \vec y)$ for interpreting in a discriminative setting. In the paper, they express $\vec{w}^T \phi(\vec x, \vec y, \vec z)$ identically as a discriminative RBM. The parameter update for mini-batch $\mathcal{D}_t$ is given by 
\graybox{
	\vec[t+1]{w} 
		&= \vec[t]{w} + \frac{\vec\eta}{|\mathcal{D}_t|}	
			\sum_{(\vec{x}^{(i)}, \vec{y}^{(i)}) \in \mathcal{D}_t} \left(
				\phi(\vec{x}^{(i)}, \vec{y}^{(i)}, \vec{z}) -
				\phi(\vec{x}^{(i)}, \vec{y}^*, \vec{z}^*)
			\right)
}








% ===========================================================================================
\lecture{Papers and Tutorials}{Non-Convex Optimization for Machine Learning}{August 12, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize P. Jain and P. Kar, ``Non-convex Optimization for Machine Learning,'' (2017).}

\p \blue{Convex Analysis} (2.1). First, let's summarize some definitions.

\textbf{Convex Combination}
\begin{definition}[-1.em]
	A \green{convex combination} of a set of $n$ vectors $\vec[i]{x} \in \R^p$, $i=1\ldots n$ is a vector $\vec[\theta]{x} := \insum \theta_i \vec[i]{x}$, where $\theta_i \ge 0$ and $\insum \theta_i = 1$.\\ 
	\underline{My interp}: A weighted average where the weights can be interpreted as probability mass associated with each vector.
\end{definition}

\textbf{Convex Set}. Sets that contain all [points in] line segments that join any 2 points in the set.
\begin{definition}[-2.0em]
	A set $\mathcal C$ is called a \green{convex set} if $\forall \vec x, \vec y \in \mathcal C$ and $\lambda \in [0, 1]$, we have that $(1 - \lambda) \vec x + \lambda \vec y \in \mathcal C$ as well. 
\end{definition}

\begin{example}[Proving conv. comb. of 3 vectors $\in \mathcal C$ too.]
	After reading the definition of a convex set above, it seemed intuitive that any convex combination of points $\in \mathcal C$ should also be in it as well (i.e. generalizing the pairwise definition). Let $x, y, z \in \mathcal C$. How can we prove that $\theta_1 x + \theta_2 y + \theta_3 z$ (where $\theta_i$ satisfy the constraints of a convex comb.) is also in $\mathcal C$? Here is how I ended up doing it:
	
	\begin{compactitem}
		\item If we can prove that $\theta_1 x + \theta_2 y = (1 - \theta_3) v$ for some $v \in \mathcal{C}$, then our work is done. This is pretty easy to show via simple arithmetic. 
		\item Case 1: assume $\theta_3 < 1$, so that we can divide both sides by $1 - \theta_3$:
		$$
			v = \frac{\theta_1}{1 - \theta_3} x	+ \frac{\theta_2}{1 - \theta_3}
		$$
		Clearly, the two coefficients here sum 1 and satisfy the constraints of a convex combination, and therefore we know that $v \in \mathcal C$, and this case is done. 
		
		\item Case 2: assume $\theta_3 = 1$. Well, that means $\theta_1 = \theta_2 = 0$. Trivially, $z \in \mathcal C$ and this case is done. 
	\end{compactitem}
\end{example}

\textbf{Convex Function}
\begin{definition}[-1em]
	A continuously differentiable function $f : \R^p \mapsto \R$ is a \green{convex function} if $\forall \vec x, \vec y \in \R^p$, we have that
	\begin{align}
		f(\vec y) \ge f(\vec x) + \langle \nabla f(\vec x), \vec y - \vec x \rangle 
	\end{align}
\end{definition}
While thinking about how to gain intuition for the above, I came across chapter 3 of ``Convex Optimization'' which describes this in much more detail. It's crucial to recognize that the RHS of the inequality is the 1st-order Taylor expansion of the function $f$ about $\vec x$, evaluated at $\vec y$. In other words, \textit{the first-order Taylor approximation is a \textbf{global underestimator} of any convex function $f$}\footnote{Consider what this implies about all the 1st-order gradient-based optimizers we use.}. \\

\textbf{Strongly Convex/Smooth Function}. Informally, strong convexity ensures a convex function doesn't grow too \textit{slow}, while strong smoothness ensures a \sout{convex}\footnote{Strong smoothness alone does not imply convexity.} function doesn't grow too \textit{fast}. Formally,
\begin{definition}[-1em]
	A continuously differentiable function is considered \green{$\alpha$-strongly convex} (SC) and \green{$\beta$-strongly smooth} (SS) if $\forall \vec x, \vec y \in \R^p$ we have
	\begin{align}
		\frac{\alpha}{2} || \vec x - \vec y ||_2^2
		\le
		f(\vec y) - f(\vec x) -  \langle \nabla f(\vec x), \vec y - \vec x \rangle 
		\le 
		\frac{\beta}{2} || \vec x - \vec y ||_2^2
	\end{align}
\end{definition}
Considering the aforementioned 1st-order Taylor approximation interpretation, we see that $\alpha$ determines just how much larger $f(\vec y)$ must be compared to its linear approximation. Conversely, $\beta$ determines the upper bound for how large this discrepancy is allowed to be\footnote{Notice that SC and SS are \textit{quadratic} lower and upper bounds, respectively. This means that the allowed deltas grow as a function of the distance between $\vec x$ and $\vec y$, whereas things like Lipschitzness grow linearly.}. 

\begin{example}[Exercise 2.1: SS does not imply convexity]
	\textit{Construct a \textbf{non-convex} function $f : \R^p \mapsto \R$ that is $1$-SS.}
	\tcblower 
	
	We need to find a function whose linear approximation is always more than $\onehalf$ times the magnitude of the difference in inputs \textbf{squared}, compared to the true value. Intuitively, I'd expect any \textit{concave} function to satisfy this, since its linear approximation is a global \textit{overestimator} of the true value. So, for example, $f(\vec y) = - || \vec y ||_2^2$ would satisfy $1-SS$ while being non-convex.
\end{example}

\textbf{Lipschitz Function}
\begin{definition}[-1em]
	A function $f$ is \green{B-Lipschitz} if $\forall \vec x, \vec y \in \R^p$,
	\begin{align}
		|f(\vec x) - f(\vec y)| \le B \cdot ||\vec x - \vec y ||_2
	\end{align}
\end{definition}

\textbf{Jensen's Inequality}. Generalizes behavior of convex functions on convex combinations\footnote{It should be obvious that expectations are convex combinations.}.
\begin{definition}[-1em]
	If $X$ is a R.V. taking values in the domain of a convex function $f$, then
	\begin{align}
		\E{f(X)} \ge f(\E{X})
	\end{align}
\end{definition}


\myspace\Needspace{12\baselineskip}
\p \blue{Convex Projections} (2.2). Given any closed set $\mathcal C \in \R^p$, the projection operator $\Pi_{\mathcal C}(\cdot)$ is defined as
\begin{align}
	\Pi_{\mathcal C}(\vec z)
		&:= \argmin_{\vec x \in \mathcal C} || \vec x - \vec z ||_2
\end{align}
If $\mathcal C$ is a convex set, then the above reduces to a convex optimization problem. Projections onto convex sets have three particularly interesting properties. For each of them, the setup is: ``For any convex set $\mathcal C \subset \R^p$, and any $\vec z \in \R^p$, let $\hat{\vec z} := \Pi_{\mathcal C}(\vec z)$. Then $\forall \vec x \in \mathcal C$, \textellipsis''
\begin{compactitem}
	\item \textbf{Property-O}\footnote{In this case only, $\mathcal C$ need not be convex}: 
	$
		||\hat{\vec z} - \vec z||_2 \le ||\vec x - \vec z||_2
	$
	. Informally: ``the projection results in the point $\hat{\vec z}$ in $\mathcal C$ that is closest to the original $\vec z$''. This basically just restates the optimization problem. 
	
	\item \textbf{Property-I}. 
	$
		\langle \vec x - \hat{\vec z}, \vec z - \hat{\vec z} \rangle \le 0
	$
	. Informally: ``from the perspective of $\hat{\vec z}$, all points $\vec x \in \mathcal C$ are in the (informally) opposite direction of $\vec z$.''
	
	\item \textbf{Property-II}. $|| \hat{\vec z} - \vec x||_2 \le ||\vec z - \vec x||_2$. Informally: ``the projection brings the point closer to all points in $\mathcal C$ compared to its original location.''
\end{compactitem}


\begin{example}[Proving Property-I]
	A proof by contradiction.
	\begin{compactenum}
		\item Assume that $\exists \vec x \in \mathcal C$ s.t. $ 	\langle \vec x - \hat{\vec z}, \vec z - \hat{\vec z} \rangle > 0$.
		
		\item We know that $\hat{\vec z}$ is also in $\mathcal C$, and since $\mathcal C$ is convex, then for any $\lambda \in [0, 1]$, 
		\begin{align}
			\vec[\lambda]{x} := \lambda x + (1 - \lambda) \hat{\vec z}
		\end{align}
		must also be in $\mathcal C$. 
		
		\item If we can show that some value of $\lambda$ guarantees that $||\vec z - \vec[\lambda]{x}||_2 < ||\vec z - \hat{\vec z}||_2$, this would directly contradict property-O, implying $\hat{\vec z}$ is not the closest member of $\mathcal C$ to $\vec z$. I'm not sure how to actually derive the range of $\lambda$ values that satisfy this, though. 
	\end{compactenum}  
\end{example}



\myspace
\p \blue{(Convex) Projected Gradient Descent} (2.3). Our optimization problem is
\begin{align}
	\min_{\vec x \in \R^p} f(\vec x) \quad\text{s.t.}\quad \vec x \in \mathcal C
\end{align}
where $\mathcal C \subset \R^p$ is a convex constraint set, and $f: \R^p \mapsto \R$ is a convex objective function. Projected gradient descent iteratively updates the value of $\vec x$ that minimizes $f$ as usual, but additionally projects the current iterate (value of best $\vec x$) onto $\mathcal C$ at the end of each iteration. That's the only difference. 



% ---------------
\myspace
\subsub{Non-Convex Projected Gradient Descent (3)}
\myspace
% ----------------

\p \blue{Non-Convex Projections} (3.1). Here we look at a few special cases where projecting onto a non-convex set can still be carried out efficiently. 
\begin{compactitem}
	\item \textbf{Projecting into sparse vectors}. The set of $s$-sparse vectors (vectors with at most $s$ nonzero elements) is denoted as
	\begin{align}
		\mathcal{B}_0(s) 
			&\triangleq \{ \vec x  \in \R^p \mid ||\vec x ||_0 \le s   \}
	\end{align}
	It turns out that $\hat{\vec z} := \Pi_{\mathcal{B}_0(s)}(\vec z)$ is obtained by setting all except the top-s elements of $\vec z$ to zero. 
	
	\item \textbf{Projecting into low-rank matrices}. The set of $m \times n$ matrices with rank at most $r$ is denoted as
	\begin{align}
		\mathcal{B}_{rank}(r) 
			&\triangleq \{
				A \in \R^{m \times n} \mid
				\text{rank}(A) \le r
			\}
	\end{align}
	and we want to project some matrix $A$ onto this set,
	\begin{align}
		\Pi_{\mathcal{B}_{rank}(r)}(A)
			:= \argmin_{X \in \mathcal{B}_{rank}(r)} ||A - X||_F
	\end{align}
	This can be done efficiently via SVD on $A$ and retaining the top $r$ singular values and vectors.
\end{compactitem}


\myspace
\p \blue{Restricted Strong Convexity and Smoothness} (3.2).

\textbf{Restricted Convexity}
\begin{definition}[-1em]
	A continuously differentiable function $f : \R^p \mapsto \R$ is said to satisfy restricted convexity over a (possibly non-convex) region $\mathcal C \subseteq \R^p$ if $\forall \vec x, \vec y \in \mathcal C$, we have that
	\begin{align}
	f(\vec y) \ge f(\vec x) + \langle \nabla f(\vec x), \vec y - \vec x \rangle 
	\end{align}
\end{definition}
and a similar rephrasing for restricted strong convexity (RSC) and restricted strong smoothness (RSS). 
























% ===========================================================================================
\lecture{Papers and Tutorials}{Improving Language Understanding by Generative Pre-Training}{August 24, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Radford et al., ``Improving Language Understanding by Generative Pre-Training,'' (2018).}

\p \blue{Unsupervised Pre-Training} (3.1). Given unsupervised corpus of tokens $\mathcal{U} = \{u_1, \ldots, u_n \}$, train with a standard LM objective:
\begin{align}
	L_1(\mathcal U)
		&= \sum_i^n \log P(u_i \mid u_{i-k}, \ldots, u_{i-1}; \Theta)
\end{align}
The authors use a \green{Transformer decoder}, i.e. literally just the decoder part of the Transformer in ``Attention is all you need.'' 

\myspace
\p \blue{Supervised Fine-Tuning} (3.2). Now we have a labeled corpus $\mathcal C$, where each instance consists of a sequence of input tokens $x^1, \ldots, x^m$, along with a label $y$. They just feed the inputs through the transformer until they obtain the final transformer block's activation $h_l^m$, and linearly project it to output space:
\begin{align}
	P(y \mid x^1, \ldots, x^m )	
		&= \text{softmax}(h_l^m W_y) \\
	L_2(\mathcal C)
		&= \sum_{(x,y)} \log P(y \mid x^1, \ldots, x^m)
\end{align}
They also found that including a language modeling auxiliary objective helped learning,
\begin{align}
	L_3(\mathcal C)
		&= L_2(\mathcal C) + \lambda L_1(\mathcal C)
\end{align}
\textellipsis that's it. Extremely simple, yet somehow effective.




% ===========================================================================================
\lecture{Papers and Tutorials}{Deep Contextualized Word Representations}{August 30, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Peters et al., ``Deep Contextualized Word Representations,'' (2018).}

\p \blue{Bidirectional Language Models} (3.1). Given a sequence of $N$ tokens, a forward LM computes the probability of the sequence via
\begin{align}
	p(t_1, \ldots, t_N)
		&= \prod_{k=1}^{N} p(t_k \mid t_1, \ldots, t_{k-1})
\end{align}
A common approach is learning context-independent token representations $\vec[k]{x}$ and passing these through $L$ layers of forward LSTMs. The top layer LSTM output at step $k$, $\vec[k,L]{\overrightarrow{h}}$, is used to predict $t_{k+1}$ with a softmax layer. The authors' biLM combines a forward and backward LM to jointly maximize 
\begin{align}
\begin{split}
	\sum_{k=1}^N \bigg[
		&\log p(t_k \mid t_1, \ldots, t_{k-1}; 
			\Theta_x, \mpurple{\overrightarrow{\Theta}_{LSTM}}, \Theta_s) \\
	+	&\log p(t_k \mid t_{k+1}, \ldots, t_{N}; 
					\Theta_x, \mpurple{\overleftarrow{\Theta}_{LSTM}}, \Theta_s) 
	\bigg]
\end{split}
\end{align}
and it's important to note the shared parameters $\Theta_x$ (token representation) and $\Theta_s$ (output softmax). 

\myspace
\p \blue{ELMo} (3.2). A task-specific linear combination of the intermediate representations. 
\graybox{
	\bm{ELMo}_k^{task}
		&= \gamma^{task} \sum_{j = 0}^L s_j^{task} \vec[k,j]{h}^{LM}	
}
where $\vec{s}^{task}$ are softmax-normalized weights (so the combination is convex). The authors also mention that, in some cases, it helped to apply \green{layer normalization} to each biLM layer before weighting. 

\myspace
\p \blue{Using biLMs for Supervised NLP} (3.3). Given a pretrained biLM and a supervised architecture, we can learn the ELMo representations (jointly with the given supervised task) as follows. 
\begin{compactenum}
	\item Freeze the weights of the [pretrained] biLM. 
	\item Concatenate the token representations (e.g. GloVe) with the ELMo representation. 
	\item Pass the concatenated representation into the supervised architecture. 
\end{compactenum}
The authors found it beneficial to some dropout to ELMo, and in some cases add L2-regularization on the ELMo weights.

\myspace
\p \blue{Pretrained biLM Architecture} (3.4). In addition to the biLM we introduced earlier, the authors make the following changes/specifications for their pretrained biLMs:
\begin{compactitem}
	\item  \green{residual connections} between LSTM layers\footnote{So the output of some layer, instead of being LSTM(x), becomes (x + LSTM(x))}.
	
	\item Halved all embedding and hidden dimensions from the CNN-BIG-LSTM model in \href{https://arxiv.org/abs/1602.02410}{\textit{Exploring the Limits of Language Modeling}}. 
	
	\item The $\vec[k]{x}$ token representations use 2048 character n-gram convolutional filters followed by two \green{highway layers}. 
\end{compactitem}










% ===========================================================================================
\lecture{Papers and Tutorials}{Exploring the Limits of Language Modeling}{August 30, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Josefina et al., ``Exploring the Limits of Language Modeling,'' (2016).}


\p \blue{NCE and Importance Sampling} (3.1). In this section, assume any $p(w)$ is shorthand for $p(w \mid \{w_{prev}  \} )$. 
\begin{compactitem}
	\item \green{Noise Contrastive Estimation} (NCE). Train a classifier to discriminate between true data (from distribution $p_d$) or samples coming from some arbitrary noise distribution $p_n$. If these distributions were known, we could compute
	\begin{align}
		p(Y {=} true \mid w)
			&= \frac{ p(w \mid \text{true} ) p(\text{true}) }{ p(w)  } \\
			&= \frac{ p_d(w)  p(\text{true}) }{ p(w, \text{true}) + p(w, \text{false})  } \\
			&= \frac{ p_d(w)  p(\text{true}) }{ p_d(w)p(\text{true}) + p_n(w)p(\text{false})  } \\
			&= \frac{ p_d(w) }{ p_d(w) + k p_n(w) } 
	\end{align}
	where $k$ is the number of negative samples per positive word. The idea is to train a logistic classifier $p(Y{=}true \mid w) = \sigma(\log p_{model} - \log k p_n(w))$, then $\text{softmax}(\log p_{model})$ is a good approx of $p_d(\vec w)$. 
	
	
	\item \green{Importance Sampling}. Estimates the partition function. Consider that now we have a set of $k+1$ words $W = \{w_1, \ldots, w_{k+1} \}$, where $w_1$ is the word coming from the true data, and the rest are from the noise distribution. We train a multinomial logistic regression over $k + 1$ classes,
	\begin{align}
		p(Y{=}i \mid W) 
			&= \frac{ p_d(w_i) }{  p_n(w_i)  } \inv{ \sum_{i'=1}^{k+1} p_d(w_{i'})  /   p_n(w_{i'})} \\
			&\propto_Y  \frac{ p_d(w_i) }{  p_n(w_i)  }
	\end{align} 
	and we end up seeing that IS is the same as NCE, except in the multiclass setting and with cross entropy loss instead of logistic loss. 
\end{compactitem}


\myspace
\p \blue{CNN Softmax} (3.2). Typically the logit for word $w$ is given by $z_w = h^T e_w$, where $h$ is often the output state of an LSTM, and $e_w$ is a vector of parameters that could be interpreted as the word embedding for $w$. Instead of this, the authors propose what they call \textit{CNN Softmax}, where we compute $e_w = CNN(chars_w)$. Although this makes the function mapping from $w$ to $e_w$ much smoother (due to the tied weights), it ends up having a hard time distinguishing between similarly spelled words that may have entirely different meanings. The authors use a correction factor, learned for each word, such that
\begin{align}
	z_w 
		&= h^T CNN(chars_w) + h^T M corr_w
\end{align}
where $M$ projects low-dimensional $corr_w$ back up to the dimensionality of the LSTM state $h$. 

\myspace
\p \blue{Char LSTM Predictions} (3.3). To reduce the computational burden of the partition function, the authors feed the word-level LSTM state $h$ through a character-level LSTM that predicts the target word one character at a time.













% ===========================================================================================
\lecture{Papers and Tutorials}{Connectionist Temporal Classification}{October 28, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Graves et al., ``Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,'' (2006).}

\p \blue{Temporal Classification}.

\begin{compactitem}
	\item \textbf{Input space}: Let $\mathcal X = (\R^m)^*$ be the set of all sequences of $m$ dimensional real-valued vectors. 
	
	\item \textbf{Output space}: Let $\mathcal{Y} = L^*$ be the set of all sequences of a finite vocabulary of $L$ labels. 
	
	\item \textbf{Data distribution}: Denote by $\mathcal{D}_{\mathcal X \times \mathcal Y}$ the probability distribution over samples $(\vec x, \vec y)$. Let $S$ denote a set of training examples drawn from this distribution. 
\end{compactitem}

\myspace
\p \blue{From Network Outputs to Labellings} (3.1). Let $L' = L \cup \{ \epsilon \}$ denote the set of unique labels combined with the blank token $\epsilon$. We refer to the alignment sequences of length $T$ (same length as $\vec x$), i.e. elements of the set $(L')^T$, as \green{paths} and denote them $\pi$, where
\begin{align}
	p(\pi \mid \vec x)
		&= \prod_{t=1}^{T} y_{\pi_t}^t \qquad (\forall \pi \in (L')^T)
\end{align}
and $y_k^t$ denoting the probability of observing label $k$ at time $t$. Now that we have paths $\pi$, we need to convert them to label sequences by (1) merging repeated contiguous labels, and then (2) removing blank tokens. We denote this procedure as a many-to-one map $\mathcal{B} : L'^T \mapsto L^{\le T}$. In other words, $\mathcal B (\pi) \rightarrow \vec\ell$. We can then write the conditional posterior over possible output sequences $\vec\ell$:
\graybox{
	p(\vec\ell \mid \vec x)
		&= \sum_{\pi \in \mathcal{B}^{-1}(\vec\ell)} p(\pi \mid \vec x)
}


\myspace
\p \blue{Constructing the Classifier} (3.2). There is no tractable algorithm for exact decoding, i.e. computing
\begin{align}
	h(\vec x) 
		&:= \argmax_{\vec\ell \in L^{\le T}} p(\vec\ell \mid \vec x)
\end{align}
However, the following two approximate methods work well in practice:
\begin{compactenum}
	\item \textbf{Best Path Decoding}. $ h(\vec x) \approx \mathcal{B}(\pi^*) \quad \text{where} \quad \pi^* = \argmax_{\pi \in L'^T} p(\pi \mid \vec x)$. 
	
	\item \textbf{Prefix Search Decoding}.
\end{compactenum}

The authors end up using neither of the above, but rather a heuristic approach:
\vspace{-0.5em}
\begin{quote}
	{\footnotesize\itshape
	We divide the output sequence into sections that are very likely to begin and end with a blank. We do this by choosing boundary points where the probability of observing a blank label is above a certain threshold. We then calculate the most probable labelling for each section individually and concatenate these to get the final classification.}
\end{quote}

\myspace
\p \blue{The CTC Forward-Backward Algorithm} (4.1). Define the total probability of the first $s$ output labels, $\slice[s]{\ell}$, given the first $t$ inputs as 
\graybox{
	\alpha_t(s) 
		&\triangleq  p(\slice[s]{\ell} \mid \slice[t]{x}) \\
		&= \sum_{ \substack{ \pi \in L'^T \\  \mathcal{B}(  \slice[t]{\pi}  ){=}\slice[s]{\ell} } }
			\prod_{ t' = 1 }^{t} y^{t'}_{\pi_{t'}}
}
Note that the summation here could contain duplicate $\slice[t]{\pi}$ that differ only in their elements beyond $t$. \\

\p We insert a blank token at the beginning and end of $\vec\ell$ and between each pair of labels, and call this augmented sequence $\vec{\ell}'$. We have the following rules for initializing $\alpha$ at the first intput step $t{=}1$, followed by the recursion rule:
\begin{align}
	\alpha_1(s) &= \begin{cases}
			y_{\epsilon}^1 & s{=}1 \\
			y_{ \vec[1]{\ell} }^1 & s{=}2 \\
			0 & s > 2
	\end{cases}\\
	\alpha_t(s)
		&= \begin{cases}
			\bar{\alpha}_t(s) y_{\vec[s]{\ell}' }^t 	& \vec[s]{\ell}' {=} b ~ \text{or} ~ \vec[s-2]{\ell}'  \\
			( \bar{\alpha}_t(s) + \alpha_{t-1}(s -2) ) y_{\vec[s]{\ell}' }^t  							& \text{otherwise}
		\end{cases}\\
	\bar{\alpha}_t(s)
		&\triangleq \alpha_{t-1}(s) + \alpha_{t-1}(s - 1)
\end{align}
\textbf{Interpretation}:
\begin{compactitem}
	\item \textit{Initialization}: Given the first input $x_1$, the only valid paths\footnote{which remember are always the same length as x} that could potentially result in the final labeling $\vec\ell$ are $\{\epsilon \}$ and $\{\ell_1\}$.  These respectively correspond to the augmented labelings $\{\epsilon\}$ and $\{\epsilon, \ell_1\}$. This is what the authors are referring to when they say ``We allow all prefixes to start with either a blank or the first symbol in $\vec\ell$.''
	
	\item \textit{Case 1}. $\bar{\alpha}_t(s) $ corresponds to a right arrow ([s, t-1] $\rightarrow$ [s, t]) and a right-down-one arrow ([s-1, t-1] $\rightarrow$ [s, t]) (see explanations below). 
	
	\item \textit{Case 2}. $\bar{\alpha}_t(s) + \alpha_{t-1}(s -2) )$ corresponds to the 2 arrows from case 1 and additional a right-down-two arrow ([s-2, t-1] $\rightarrow$ [s, t]). 
\end{compactitem}

\textbf{Reading the lattice arrow diagrams}. The usage of the augmented label sequence $\vec{\ell}'$ can make reading these very confusing. Here's how to read them:
\begin{compactitem}
	\item \textit{Rows}: row $s$ corresponds to $\ell'_s$. 
	\item \textit{Columns}: column $t$ corresponds to $\vec[t]{x}$. 
	\item \textit{Arrows/Traversal}: this was the confusing part for me. The arrows correspond to a transition in a given \textit{path}. Each arrow direction has a different interpretation:
	\begin{compactitem}
		\item right: the path either had a repeated character (which would've been merged into a single output label) or a repeated blank token (if the row is a blank token) which would've been removed entirely in the final output label sequence. 
		\item right-down-one: transition from either label-to-blank or blank-to-label. 
		\item right-down-two: direct transition between two unique characters. 
	\end{compactitem}
\end{compactitem}

Ok, I finally get it now. In my opinion, introducing $\vec{\ell}'$ as the ``augmented label sequence'' is confusing/misleading/unnecessary/stupid/etc. It is literally just introduced so we can meaningfully talk about transitions between elements of a given path $\pi$. The traversal being done in the $\alpha$ formulas is referencing the valid \textit{paths}, NOT the final labels (at least directly). Especially confusing is when the distill article says crap like ``can't jump over $z_{s-1}$'' which is just total nonsense -- no one is jumping over anything! Literally all they mean is ``transition between unique characters in a path''. Lost so many hours confused over the wording in the distill article, which only served to confuse me further (just read the paper).  
	 

\begin{comment}
It's worth emphasizing how to interpret these, given we've imposed this weird augmented label sequence. In as-verbose-as-possible terms, 
\begin{quote}
{\bfseries
$\alpha_t(s)$ is the probability, after running our RNN for $t$ time steps to produce the path $\slice[t]{\pi}$, that $\mathcal{B}(\slice[t]{\pi}){==}\slice[ \frac{s - 1}{2} ]{\ell}$  for which, after inserting $\epsilon$ between all elements of $\slice[ \frac{s-1}{2} ]{\ell}$, we obtain the augmented labeling $\slice[s]{\ell}'$.
}
\end{quote}

\p The way you should think about the different possible cases here is that, at time step $t$, in order for there to be nonzero probability that we can merge the sequence of $t$ RNN outputs into the augmented label sequence $\slice[s]{\ell}'$, it must be true that:
\begin{compactitem}
	\item We emit the token $\ell'_s$ at time $t$ from the RNN. 
	
	\item At the previous timestep, $t  - 1$, we emitted a token consistent with our rules for merging combined with the fact that we've inserted $\epsilon$ between every pair of tokens in the final output labeling $\vec\ell$, in order to produce $\vec{\ell}'$. 
\end{compactitem}
The weird case (in my opinion) to consider is realizing that we can emit, for example, the label $a$ at time $t - 1$, then the label $b$ at time $t$, and this would eventually get mapped to a portion of the augmented label sequence, $[a, \epsilon, b]$. 
\end{comment}

The final probability of the label sequence $\vec\ell$ given input sequence $\vec x$ is thus:
\graybox{
	p(\vec\ell \mid \vec x) &= \alpha_T(|\vec{\ell}'|) + \alpha_T(|\vec{\ell}'|-1) 
}
which  corresponds to ``total probability of all paths ending in a blank token plus total probability of all paths ending in the final label of $\vec\ell$.''




\myspace
\subsub{Sequence Modeling With CTC}
\myspace

Notes on \href{https://distill.pub/2017/ctc/}{this distill article} (note that I do NOT recommending reading this article -- the wording is horrendously confusing compared to what's actually going on). 

\bluesec{Introduction}. CTC is an approach for mapping input sequences $X = \{x_1, \ldots, x_T\}$ to label sequences $Y = \{y_1, \ldots, y_U \}$, where the lengths may vary ($T \ne U$). 


\bluesec{Alignment}. An \green{alignment} between input sequence $X$ and label sequence $Y$ is a function $f: X \mapsto Y$. Take for example the label sequence $Y = \{ h, e, l, l, o\}$ and some input sequence (e.g. raw audio) $X = \{x_1, \ldots, x_{12}\}$. CTC places the following constraints:
\begin{compactenum}
	\item It must be the same length as the input sequence $X$. 
	
	\item It has the same vocabulary as $Y$, plus an additional token $\epsilon$ to denote blanks. 
	
	\item At position $i$, it either (a) repeats the aligned token at $i-1$, (b) assigns the empty token $\epsilon$, or (c) assigns the next letter of the label sequence. 
\end{compactenum} 
For our example, we could have an aligned sequence $A = \{h, h, e, \epsilon, \epsilon, l, l, l, \epsilon, l, l, o\}$. Then we apply the following two steps (can interpret as functions) to map from $A$ to $Y$:
\begin{compactenum}
	\item Merge any repeated [contiguous] characters. 
	
	\item Remove any $\epsilon$ tokens. 
\end{compactenum}

\begin{example}[Number of Valid Alignments]
	{\itshape\small Given $X$ of length $T$ and $Y$ of length $U \le T$ (and no repeating letters), how many valid alignments exist?}
	\tcblower
	
	The differences between alignments fall under two categories:
	\begin{compactenum}
		\item Indices where we transition from one label to the next. 
		\item Indices where we insert the blank token, $\epsilon$. 
	\end{compactenum}
	\vspace{1em}
	
	Stated even simpler, the alignments differ first and foremost by \textit{which elements of $X$ are ``extra'' tokens}, where I'm using ``extra'' to mean either blank or repeat token. Given a set of $T$ tokens, there are $\tbinom{T}{T - U}$ different ways to assign $T - U$ of them as ``extra.'' The tricky part is that we can't just randomly decide to repeat or insert a blank, since a sequence of one or more blanks is \textit{always} followed by a transition to next letter, by definition. And remember, we have defined $Y$ to have no repeated [contiguous] labels. \\
	
	Apparently, the answer is $\tbinom{T + U}{ T - U}$ total valid alignments. 
\end{example}


\bluesec{Loss Function}. When you hear someone say ``the CTC loss,'' they usually mean ``MLE using a CTC posterior.'' In other words, there is no ``CTC loss'' function, but rather there is the standard maximum likelihood objective, but we use a particular form for the posterior $p(Y \mid X)$ over possible output labels $Y$ given raw input sequence $X$:
\graybox{
	p(Y \mid X)
	&= \sum_{\mathcal A \in \mathcal{A}_{X, Y}} \prod_{t=1}^T p_t(a_t \mid X)
}
where $\mathcal A$ is one of the valid alignments from $\mathcal{A}_{X, Y}$. The value of $a_t$ obeys the set of three constraints listed above.

How can we compute the loss efficiently? Let $\vec z \triangleq [\epsilon, y_1, \epsilon, y_2, \ldots, \epsilon, y_U, \epsilon]$, and let $\alpha$ denote the \red{score of the merged alignments} at a given node [in the \red{CTC lattice}]. We compute the forward probabilities $\alpha_t(s)$, defined as the probability of arriving at [prefix of] augmented label sequence $\slice[s]{z}$ given unmerged alignments up to input step $t$:
$$
	\alpha_t(s) 
		\triangleq p(\slice[s]{z} \mid \slice[t]{x})
$$


\myfig[0.8\textwidth]{figs/ctc_distill.png}

The key insight is that we can compute $\alpha_t$ as long as we know $\alpha_{t-1}$. There are two cases to consider.
\begin{compactenum}
	\item  (1.1) \textbf{$z_s$ is the blank token $\epsilon$.} At the previous RNN output (time $t - 1$), we could've emitted either a blank token $\epsilon$ or the previous token in the augmented label sequence, $z_{s-1}$. In other words, 
	\begin{align}
		\alpha_t(s) 
			&= p(z_s \eq \epsilon \mid  x_t) \cdot
			\left(   \alpha_{t - 1}(s) + \alpha_{t - 1}(s - 1) \right) 
	\end{align}
	
	\item (1.2) \textbf{$z_s$ is the same label as at step $s - 2$}. This occurs when $Y$ has repeated labels next to each other. 
	\begin{align}
		\alpha_t(s) 
			&= p(z_s \eq z_{s-2} \mid x_t) 
			\cdot \left(   \alpha_{t - 1}(s) + \alpha_{t - 1}(s - 1)   \right)
	\end{align}
	In this situation, $\alpha_{t-1}(s)$ corresponds to us just emitting the same token as we did at $t - 1$ or emitting a blank token $\epsilon$, and $\alpha_{t-1}(s - 1)$ corresponds to a transition to/from $\epsilon$ and a label. 
	
	\item (2) \textbf{$z_{s-1}$ is the blank token $\epsilon$ between unique characters.} In addition to the two $\alpha_{t-1}$ terms from before, we now also must consider the possibility that our RNN emitted $z_{s-2}$ at the previous time ($t-1$) and then emitted $z_s$ immediately after at time $t$.  
	\begin{align}
		\alpha_t(s)
			&= p(z_s \mid x_t) \cdot \left(
				\alpha_{t-1}(s-2) + \alpha_{t-1}(s-1) + \alpha_{t-1}(s)
			\right)
	\end{align}
\end{compactenum}














% ===========================================================================================
\lecture{Papers and Tutorials}{BERT}{November 03, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Devlin et al., ``BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,''  Google AI Language (Oct 2018).}


\p \blue{TL;DR}. Bidirectional Encoder Representations from Transformers. Pretrained by jointly conditioning on left and right context, and can be fine-tuned with one additional non-task-specific output layer. Authors claim the following contributions:
\begin{compactitem}
	\item \textit{Demonstrate importance of bidirectional pre-training for language representations}. Ok, congrats. 
	
	\item \textit{Show that pre-trained representations eliminate needs of task-specific architectures}. We already knew this. Seriously, how is this news?
	
	\item \textit{Advances SOTA for eleven NLP tasks}. 
\end{compactitem}

\myspace 
\p \blue{BERT}. Instead of using the unidirectional transformer \textit{decoder}, they use the bidirectional \textit{encoder} architecture for their pre-trained model\footnote{Is this seriously paper-worthy?? I'm taking notes so I can easily refer back on popular approaches, but I don't see what's so special here.}. 
\myfig[0.7\textwidth]{figs/bert_input.png}

The input representation is shown in the figure above. The input is a sentence pair, as commonly seen in tasks like QA/NLI. 

\myspace
\p \blue{Pre-training Tasks} (3.3). 
\begin{compactenum}
	\item \textbf{Masked LM}: Mask 15\% of all tokens, and try to predict only those masked tokens\footnote{This is the only ``novel'' idea I've seen in the paper. Seems hacky-ish but also reasonable.} Furthermore, at training time, the mask tokens are either fed through as (a) the special \texttt{[MASK]} token 80\% of the time, (b) a random word 10\% of the time, and (c) the original word unchanged 10\% of the time. Now \textit{this} is just hackery. 
	
	\item \textbf{Next Sentence Prediction}: Given two input sentences A and B, train a binary classifier to predict whether sentence B came directly after sentence A. 
\end{compactenum}
They do the pretraining jointly, using a loss function that's the sum of the mean masked LM likelihood and mean next sentence prediction likelihood. 



% ===========================================================================================
\lecture{Papers and Tutorials}{Wasserstein is all you need}{November 25, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Singh et al., ``Wasserstein is all you need,''  EPFL Switzerland (August 2018).}

\p \blue{TL;DR}. Unsupervised representations of objects/entities via distributional + point estimate. Made possible by \green{optimal transport}. 

\myspace
\p \blue{Optimal Transport} (3). First, notation. Let\textellipsis
\begin{compactitem}
	\item $\Omega$ denote a space of possible outcomes. 
	
	\item $\mu$ denote an \textbf{empirical} probability measure, defined as some convex combination $\mu(\vec x) = \insum a_i \delta(x_i)$, where $x_i \in \Omega$. 
	
	\item $\nu$ denote a similar measure, also a convex combination, $\nu(\vec y) = \sum_j^m b_j \delta(y_j)$. 
	
	\item $M_{ij}$ denote the ground cost of moving from point $x_i$ to $y_j$. 
\end{compactitem}
Intuition break: recognize that $\mu$ and $\nu$ are just a formal description of probability densities via normalized ``counts'' $a_i$ and/or $b_j$. Those weights are basically probability mass The \green{Optimal Transport} distance between $\mu$ and $\nu$ is the following \purple{linear program}:
\graybox{
	\text{OT}(\mu, \nu; \matr M)
		&= \min_{ \matr T \in \R^{n \times m}_+  }   \sum_{ij} T_{ij} M_{ij} 
	\quad \text{s.t.} \quad
	(\forall i) \sum_j T_{ij} = a_i,
	\quad
	(\forall j) \sum_i T_{ij} = b_j
}
where $\matr T$ is called the \green{transportation matrix}. Informally, the constraints are simply enforcing bijection to/from $\mu$ and $\nu$, in that ``all the mass sent from element $i$ must be exactly $a_i$, and all mass sent to element $j$ must be exactly $b_j$''. A particular case called the \green{p-Wasserstein distance}, where $\Omega = \R^d$ and $M_{ij}$ is a distance metric over $\R^d$, is defined as
\graybox{
	W_p(\mu, \nu)
		&\triangleq \text{OT}(\mu, \nu; D_{\Omega}^p)^{1/p}
}
where $D$ is just a distance metric, e.g. for $p=2$ it could be euclidean distance. 

\myspace
\p \blue{Distributional Estimate} (4.1). Let $\mathcal C \triangleq \{ c  \}_i$ be the set of possible contexts, where each context $c_i$ can be a word/phrase/sentence/etc. For a given word $w$ and our set of observed contexts for that word, we essentially want to embed its context histogram into a space $\Omega$ (where typically $\Omega = \R^d$).  Let $\matr V$ denote a matrix of context embeddings, such that $V_{i,:} = \vec[i]{c} \in \R^d$, the embedding for context $c_i$ in what the authors call the \textit{ground space}. Combining the histogram $H^w$ containing observed context counts for word $w$ with $\matr V$, the \green{distributional estimate} of the word $w$ is defined as
\begin{align}
	P_{\matr V}^w
		&\triangleq \sum_{c \in \mathcal C} (H^w)_c \delta(\vec[c]{v})
\end{align}
Also, the \textit{point estimate} is just $\vec[w]{v}$, i.e. the embedding of the word $w$ when viewed as a context. 

\myspace
\p \blue{Distance} (4.2). Given some distance metric $D_{\Omega}$ in ground space $\Omega = \R^d$, the distance between words $w_i$ and $w_j$ is the solution to the following OT problem\footnote{I'm not sure whether the rightmost exponent of $p$ is a typo in the paper, but that is how it is written.}:
\begin{align}
	\text{OT}(P_{\matr V}^{w_i},  P_{\matr V}^{w_j}; D_{\Omega}^p)
		&:= W_p^{\lambda}( P_{\matr V}^{w_i},  P_{\matr V}^{w_j}  )^p
\end{align}

\myspace
\p \blue{Concrete Framework} (5). The authors make use of the \purple{shifted positive pointwise mutual information} (SPPMI), $\matr[s]{S}^{\alpha}$, for computing the word histograms:
\begin{align}
	(H^w)_c
		&:= \frac{
			\matr[s]{S}^{\alpha}(w, c)
		}{  
			\sum_{c' \in \mathcal C} \matr[s]{S}^{\alpha}(w, c')
		}\\
	\matr[s]{S}^{\alpha}(w, c)
		&\triangleq \max \left[ 
			\log\left(  \frac{ \text{Count}(w, c) \sum_{c'} \text{Count}(c')^{\alpha} }{  \text{Count}(w)\text{Count}(c)^{\alpha}  }   \right) - \log(s),
			~0
		\right]
\end{align}




% ===========================================================================================
\lecture{Papers and Tutorials}{Noise Contrastive Estimation}{December 9, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize M. Gutman and A. Hyv\"{a}rinen, ``Noise contrastive estimation: A new estimation principle for unnormalized statistical models,'' University of Helsinki  (2010).}

\p \blue{TL;DR}: A few ways of thinking about NCE:
\begin{compactitem}
	\item Instead of directly modeling a normalized word distribution $p_d(w)$, we can just model the unnormalized $\widetilde{p}_d$ distribution (and an additional parameter $c = - \ln Z$) by training our model to distinguish between true samples from $p_d$ and noise samples from some distribution $p_n$ that we choose.
	
	\item Instead of modeling $p(w \mid c)$, we model $p(D \mid w, c)$, where $D$ is binary RV indicating whether $w, c$ are from the true data distribution $p_d$ or the noise distribution $p_n$. 
\end{compactitem}

\myspace
\p \blue{Introduction}. Setup \& notation:
\begin{compactitem}
	\item We observe $\vec x \sim p_d(\cdot)$ but $p_d(\cdot)$ itself is unknown. 
	\item We model $p_d$ by some model $p_m(\cdot ; \alpha)$ parameterized by $\alpha$\footnote{The implicit assumption here is that $\exists \alpha^*$ such that $p_d(\cdot) = p_m(\cdot; \alpha^*)$.}.  
\end{compactitem}
So, can we get away with modeling the \textit{unnormalized} density $\widetilde{p}_m$ instead of requiring the normalization constraint to be baked in to our  optimization problem? Similar to approaches like \purple{contrastive divergence} and \purple{score matching}, \green{noise-contrastive estimation} (NCE) aims to address this question.

\myspace
\p \blue{Noise-contrastive estimation} (2.1). Let $c$ be an estimator for $-ln Z(\alpha)$, and let $\theta = \{\alpha, c\}$ denote all of our parameters. Given observed data $X = \vecseq[T]{x}$, and noise $Y = \vecseq[T]{y}$, we seek parameters $\hat{\theta}_T$ that maximize $J_T(\theta)$\footnote{This all assumes of course that $p_n$ is fully defined.}:
\graybox{
	J_T(\theta)
		&= \inv{2T} \sum_t \ln \left[ h(\vec[t]{x}) \right]
			+ \ln\left[         1 - h(\vec[t]{y}) \right] \\
	h(\vec u) 
		&= \sigma \left(    G\left(  \vec u  \right)  \right) \\
	G(\vec u)
		&= \ln p_m(\vec u) - \ln p_n(\vec u) \\
	\ln p_m(\cdot; \theta)
		&:= \ln \widetilde{p}_m(\cdot; \alpha) + c
}
where for compactness reasons I've removed the explicit dependence of all functions above (except $p_n$) on $\theta$. Notice how this fixes the issue of the model just setting $c$ arbitrarily high to obtain a high likelihood\footnote{The primary reason why MLE is traditionally unable to parameterize the partition function.}.

\myspace
\p \blue{Connection to supervised learning} (2.2). The NCE objective can be interpreted as binary logistic regression that discriminates whether a point belongs to the data ($p_d$) or to the noise ($p_n$).\marginnote{We model with a uniform prior: $P(C{=}1)=P(C{=}0)=1/2$   }[2em]
\begin{align}
	P(C{=}1 \mid \vec u \in X \cup Y)
		&= \frac{ P(C{=1}) p(\vec u \mid C{=}1) }{  p(\vec u) } \\
		&= \frac{  p_m(\vec u) }{   p_m(\vec u) + p_n(\vec u)  } \\
		&\equiv h(\vec u; \theta)
\end{align}
where we're now using the union of $X$ and $Y$, $U := \vecseq[2T]{u}$. The log-likelihood of the data under the parameters $\theta$ is
\begin{align}
	\ell(\theta)
		&= \sum_t^{2T} \left[
			C_t \ln P(C_t{=}1 \mid \vec[t]{u};\theta) 
			+ (1 - C_t) \ln P(C_t{=}0 \mid \vec[t]{u} ; \theta) \right] \\
		&= \sum_t^{2T} \left[
			C_t \ln h(\vec[t]{u}; \theta)
			+ (1 - C_t) \ln \left[ 1 - h(\vec[t]{u}; \theta)  \right] \right] \\
		&= \sum_t^T \left[
				\ln h(\vec[t]{x}; \theta)
			+ \ln \left[ 1 - h(\vec[t]{y}; \theta)  \right]
		\right]
\end{align}
which is (up to a constant factor) the same as our NCE objective.


\myspace
\p \blue{Properties of the estimator} (2.3). As $T \rightarrow \infty$, $J_T(\theta) \rightarrow J(\theta)$, where
\begin{align}
	J(\theta) 
		&\triangleq \lim_{T \rightarrow \infty} J_T(\theta)
		= \onehalf \E{\ln h(\vec x; \theta)     +    \ln\left[1 -  h(\vec y; \theta) \right]  } \\
	\widetilde{J}(f)
		&\triangleq \onehalf \E{ 
			   \ln\left[   \sigma \mgreen{ \bigg( }  f\left(\vec x\right) - \ln p_n\left( \vec x \right)  \mgreen{ \bigg) }   \right] 
			+ \ln\left[  1 -  \sigma \mgreen{ \bigg( }  f\left(\vec y\right) - \ln p_n\left( \vec y \right)   \mgreen{ \bigg) }  \right]    }
\end{align}
\begin{definition}[-1em][Theorem 1]
	$\widetilde J$ attains exactly one maximum, located at $f(\cdot) = \ln p_d(\cdot)$, provided $p_d(\cdot){\ne}0 \implies p_n(\cdot){\ne}0$. 
\end{definition}




\Needspace{15\baselineskip}
\subsub{Self-Normalized NCE}

Notes from A. Mnih and Y. Teh, ``A fast and simple algorithm for training neural probabilistic language models'' (2012). 

\bluesec{Maximum likelihood learning}. Let $P_{\theta}^h(w)$ denote the probability of observing word $w$ given context $h$. For neural LMs, we assume this is the softmax of a scoring function $s_{\theta}(w, h)$ (logits). In what follows, I'll drop the explicit $\theta$ and $h$ subscript/superscript notation for brevity.
\begin{align}
	\pderiv{\log P(w)}{\theta}
		&= \pderiv{}{\theta} s(w, h) - \pderiv{}{\theta} \log \left[ \sum_{w'} e^{s(w', h)} \right] \\
		&= \pderiv{}{\theta} s(w, h) - \sum_{w'} P(w') \pderiv{}{\theta} s(w', h) \\
		&=  \pderiv{}{\theta} s(w, h) - \mred{ \E[w \sim P_{\theta}^{h}]{\pderiv{}{\theta} s(w, h)} }
\end{align}
where the expectation (in red) is expensive due to requiring $s(w, h)$ evaluated for all words in the vocabulary. One approach is \green{importance sampling} where we sample a subset of $k$ words from the vocab and compute the probabilities from that approximation:

\begin{align}
	\pderiv{\log P(w)}{\theta}
		&= \pderiv{}{\theta} s(w, h) - \sum_{w'} P(w') \pderiv{}{\theta} s(w', h) \\
		&\approx  \pderiv{}{\theta} s(w, h) - \mgreen{ \inv{V} \sum_{j=1}^{k} v(x_j) }  \pderiv{}{\theta} s(w', h) \\
		\text{where} 
		\quad 
		v(x)
			&= \frac{ e^{s_{\theta}(x, h)}    }{Q^h(w \eq x)} 
\end{align} 
and we refer to $v$ as the \green{importance weights}. In NLP, we typically set $Q$ to the \green{Zipfian distribution}\footnote{\href{https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler}{TensorFlow} seems to define this as
	\begin{align}
		P_{Zipf}(w) &= \frac{  \log(w + 2) - \log(w + 1) }{   \log(V + 1)   }
	\end{align}
	where $V$ is the vocabulary size. I can't seem to find this definition anywhere else though. A more common form seems to be
	\begin{align}
		P(w) &= \frac{ \inv{w} }{  \sum_{w'} \inv{ w'^{s} } }
	\end{align}
	I plotted both on WolframAlpha (\href{https://www.wolframalpha.com/input/?i=plot+\%281\%2Fx+\%2F+5.187377517639620260805117675658253\%29+and+\%28log\%28x\%2B2\%29+-+log\%28x\%2B1\%29\%29+\%2F+log\%28100+\%2B+1\%29+for+x\%3D1+to+100}{link here}) and they do indeed look basically the same, especially for any reasonably large $V$. 
}

\bluesec{NCE}. In NCE, we introduce [unigram] noise distribution $P_n(w)$ and impose a prior that noise samples are $k$ times more frequent than data samples from $P_d^h(w)$, resulting in the joint distribution,
\begin{align}
	P^h(D, w) 
		&= P(D \eq 1) P_d^h(w) + P(D \eq 0) P_n(w) \\
		&= \inv{k+1} P_d^h(w) + \frac{k}{k+1} P_n(w) 
\end{align}
Our goal is to learn the posterior distribution $P^h(D \eq 1 \mid w)$ (so we replace $P_d$ with $P_{\theta}$):\marginnote{NCE posterior}[2em]
\graybox{
	P^h(D \eq 1 \mid w, \theta)
		&= \frac{  P_{\theta}^h(w)  }{  P_{\theta}^h(w) + kP_n(w) } \label{eq:nce-posterior} 
}
In NCE, we re-parameterize $P_{\theta}$ by treating $-\log Z$ as a parameter itself, $c^h$\footnote{Reminder that the $h$ is a reminder that $Z$ is a function of the context $h$.}. 
\begin{align}
	P_{\theta}^h(w) := P_{\theta^0}^h \exp(c^h)
\end{align}
where $P_{\theta^0}^h$ denotes the unnormalized distribution. It turns out that, in practice, we can impose that $\exp(c^h) \eq 1$ and use the unnormalized $P^h_{\theta^0}$ in place of the true probabilities in all that follows. \textit{Critically, note that this means we rewrite equation \ref{eq:nce-posterior} using the unnormalized probabilities in place of $P_{\theta}^h$}. The NCE objective is to find\footnote{I'll drop off $\theta$ dependence wherever obvious for the sake of compactness.} as follows, where I've shown each step of the derivation:
\begin{small}
\begin{align}
	\theta^* 
		&= \argmax_{\theta} J^h(\theta) \\
	J^h(\theta)
		&= \E[(D,w) \sim P^h]{\log P(D \mid w, \theta)} \\
		&= \sum_{D=0}^{1} \sum_w P^h(D, w) \log P^h(D \mid w, \theta) \\
		&= \sum_w  P^h(0, w) \log P^h(0 \mid w)   + \sum_w  P^h(1, w) \log P^h(1 \mid w) \\
		&= \inv{k+1} \sum_w \left[
			k P_n(w) \log P^h(0 \mid w) 
			+ P_d^h(w) \log P^h(1 \mid w)
		\right] \\
		&= \inv{k+1} \left[
			k \E[P_n]{\log P^h(0 \mid w)}
			+ \E[P^h_d]{\log P^h(1 \mid w)}
		\right] \\
		&\propto k \E[P_n]{\log P^h(0 \mid w)}
		+ \E[P^h_d]{\log P^h(1 \mid w)}
\end{align}
\end{small}
The gradient of the NCE objective is thus
\graybox{
	\pderiv{}{\theta} J^h(\theta)
		&= \sum_w \frac{ k P_n(w)  }{  P_{\theta}^h(w) + kP_n(w)   } \left( 
			P_d^h\left( w \right) - P_{\theta}^h\left( w \right) 
		\right)
		\pderiv{}{\theta} \log P_{\theta}^h(w)
}
\red{TODO}: incorporate more info from \href{http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf}{Chris Dyer's excellent notes}.
















% ===========================================================================================
\lecture{Papers and Tutorials}{Neural Ordinary Differential Equations}{December 16, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize R. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, ``Neural Ordinary Differential Equations,'' University of Toronto  (Oct 2018).}

\p \blue{Introduction} (1). Let $T$ denote the number of layers of our network. In the limit of $T \rightarrow \infty$ and small $\delta \vec{h}(t)$ between each ``layer'', we can parameterize the dynamics via an ODE:
\begin{align}
	\deriv{\vec{h}(t)}{t}
		&= f\left(
			\vec{h}\left(t \right), t, \theta	 
		\right) \label{eq:neural-ode-1}
\end{align}
Benefits of defining models using ODE solvers:
\begin{compactitem}
	\item \textbf{Memory}. Constant as a function of depth, since we don't need to store intermediate values from the forward pass. 
	
	\item \textbf{Adaptive computation}. Modern ODE solvers adapt evaluation strategy on the fly. 
	
	\item \textbf{Parameter efficiency}. Nearby ``layers'' have shared parameters. 
\end{compactitem}

\begin{example}[Review: ODE]
	Remember the basic idea with ODEs like the one shown above. Our goal is to solve for $\vec{h}(t)$. 
	\begin{align}
		d\vec{h}(t) 
			&= f\left(\vec{h}\left(t \right), t, \theta	 \right) \mathrm{d}t \\
		\int d\vec{h}(t) 
			&= \int f\left(\vec{h}\left(t \right), t, \theta	 \right) \mathrm{d}t \\
		\vec{h}(t) + c_1
			&= \int f\left(\vec{h}\left(t \right), t, \theta	 \right) \mathrm{d}t \\
	\end{align}
	and so the solution of an ODE is often represented as an integral. 
\end{example}


\p \blue{Reverse-mode automatic differentiation of ODE solutions} (2). Our goal is to optimize 
\begin{align}
	L(\vec{z}(t_1))
		&= L \left(
			\int_{t0}^{t_1} f(\vec{z}(t), t, \theta) \mathrm{d}t 
		\right)
\end{align}

Given our starting definition (eq \ref{eq:neural-ode-1}), we can say
\begin{align}
	\vec z (t + \epsilon) 
		&= z(t) + \int_t^{t + \epsilon} f\left(\vec{z}\left(t \right), t, \theta	 \right) \mathrm{d}t 
		:= T_\epsilon (\vec z (t), t)  
\end{align}
which we can use to define the \green{adjoint} $a(t)$:
\begin{align}
	\mgreen{a(t)}
		&\triangleq - \pderiv{L}{\vec z (t)} = -  \pderiv{L}{\vec z (t + \epsilon)} \deriv{\vec z (t + \epsilon)}{\vec z (t)} \\
		&= \mgreen{a(t + \epsilon)} \pderiv{T_\epsilon (\vec z (t), t)}{\vec z (t)} \\
	\deriv{\mgreen{a(t)}}{t}
		&= - \mgreen{a(t)}^T \pderiv{ f(\vec z (t), t, \theta) }{\vec z}
\end{align}
where $\deriv{a(t)}{t}$ can be derived using the limit definition of a derivative. We'll now outline the algorithm for computing gradients. We use a black box ODE solver as a subroutine that solves a first-order ODE initial value problem. As such, it accepts an initial state, its first derivative, the start time, the stop time, and parameters $\theta$ as arguments. 

\begin{algorithm}[Reverse-mode derivative]
	{\small\itshape 
		Given start time $t_0$, stop time $t_1$, final state $\vec{z}(t_1)$, parameters $\theta$, and gradient $\pderiv{L}{\vec{z}(t_1)}$ compute all gradients of $L$. 
}
	
	\tcblower 
	
	\begin{compactenum}
		\item Compute $t_1$ gradient:
		\begin{align}
			\pderiv{L}{t_1}
				&= \pderiv{L}{\vec{z}(t_1)}^T \pderiv{\vec{z}(t_1)}{t_1}
				=  \pderiv{L}{\vec{z}(t_1)}^T f(\vec z (t_1), t_1, \theta)
		\end{align}
		
		\item Initialize the augmented state:
		\begin{align}
			s_0
				&:= \left[
					\vec z (t_1),~
					\vec a (t_1),~
					\vec 0,~
					- \pderiv{L}{t_1}
				\right]
		\end{align}
		
		
		\item Define augmented sate dynamics:
		\begin{align}
			\deriv{ s }{ t }
				&\triangleq \left[
					f(\vec z (t), t, \theta),~
					-\vec a (t)^T \pderiv{f}{\vec z},~
					-\vec a (t)^T \pderiv{f}{\theta},~
					-\vec a (t)^T \pderiv{f}{t}
				\right]
		\end{align}
		
		\item Solve \textbf{reverse-time}\footnote{Notice how our ``initial state'' actually corresponds to $t_1$, and we pass in $t_1$ and $t_0$ in the opposite order we typically do.} ODE:
		\begin{align}
			\left[
				\vec z (t_0),~
				\pderiv{L}{\vec z (t_0)},~
				\pderiv{L}{\theta},~
				\pderiv{L}{t_0}
			\right]
				&= \text{ODESolve}\left(
					s_0, \deriv{s}{t}, t_1, t_0, \theta
				\right)
		\end{align}
		
		\item Return $	\pderiv{L}{\vec z (t_0)},~\pderiv{L}{\theta},~\pderiv{L}{t_0}, \pderiv{L}{t_1} $
	\end{compactenum}
\end{algorithm}






% ===========================================================================================
\lecture{Papers and Tutorials}{On the Dimensionality of Word Embedding}{December 16, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Z. Yin and Y. Shen, ``On the Dimensionality of Word Embedding,'' Stanford University (Dec 2018).}

\p \blue{Unitary Invariance of Word Embeddings} (2.1). Authors interpret result any unitary transformation (e.g. a rotation) on embedding matrix as equivalent to the original. 

\myspace
\p \blue{Word Embeddings from Explicit Matrix Factorization} (2.2). Let $M$ be the $V \times V$ co-occurrence counts matrix. One way of getting embeddings is doing a truncated SVD on $M = UDV^T$. If we want $k$-dimensional embedding vectors, we can do
\begin{align}
	\matr E
		&= \matr[1:k]{U} \matr[1:k, 1:k]{D}^{\alpha}
\end{align}
for some $\alpha \in [0, 1]$. 

\myspace
\p \blue{PIP Loss} (3). Given embedding matrix $E \in \R^{V \times d}$, define its \green{Pairwise Inner Product} (PIP) matrix to be 
\graybox{
	\text{PIP}(\matr E)
		&\triangleq \matr E \matr{E}^T 
}
Notice that $\text{PIP}(\matr E)_{i,j} = \langle \vec[i]{w}, \vec[j]{w} \rangle$. Define the \green{PIP loss} for comparing two embeddings $\matr[1]{E}$ and $\matr[2]{E}$ for a common vocab of $V$ words:
\graybox{
	|| \text{PIP}(\matr[1]{E}) - \text{PIP}(\matr[2]{E}) ||_F
		&= \sqrt{\sum_{i,j} \left(
			\langle  \vec[i]{w}^{(1)}, \vec[j]{w}^{(1)}    \rangle - 
			\langle  \vec[i]{w}^{(2)}, \vec[j]{w}^{(2)}    \rangle
			\right)^2}
}





% ===========================================================================================
\lecture{Papers and Tutorials}{Generative Adversarial Nets}{December 26, 2018}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Goodfellow et al., ``Generative Adversarial Nets,'' (June 2014)}

\p \blue{TL;DR}. The abstract is actually quite good:
\begin{quote}
	\vspace{-1em}
	\small\itshape
	\textellipsis we simultaneously train two models: a generative model \green{G} that captures the data distribution, and a discriminative model \purple{D} that estimates the probability that a sample came from the training data rather than \green{G}. The training procedure for \green{G} is to maximize the probability of \blue{D} making a mistake. This framework corresponds to a minimax two-player game.
\end{quote}

\myspace
\p \blue{Adversarial Nets} (3). As usual, first go over notation:
\begin{compactitem}
	\item Generator produces data samples\footnote{Note that G outputs samples $\vec x$, not probabilities. By doing this, it \textit{implicitly} defines a probability distribution $p_g(\vec x)$. This is what the authors say.}, $\vec x := \mgreen{G}(\vec z; \theta_g)$, where $\vec z \sim p_n$ (noise distribution prior). 
	
	\item Discriminator, $\mpurple{D}(\vec x; \theta_d)$, outputs probability that $\vec x$ came from (true) $p_{data}$ instead of $G$.  
\end{compactitem}
Our two-player minimax optimization problem can be written as:
\newcommand{\md}{\mpurple{D}}
\newcommand{\mg}{\mgreen{G}}
\graybox{
	\mgreen{\min_G} \mpurple{\max_D} V(\md, \mg) 
		&= \mpurple{ \E[\vec x \sim p_{data}]{\log D(\vec x)} } 
		  + \E[\vec z \sim p_n]{\log \left( 1 - \md ( \mg (\vec z) )  \right) } \label{eq:gans-1}
}

\myspace
\p \blue{Theoretical Results} (4). Below is the training algorithm.
\begin{algorithm}[SGD with GANs]
	Repeat the following for each training iteration.
	
	\begin{compactenum}
		\item Train $\md$. For $k$ steps, repeat:
		\begin{compactenum}
			\item Sample $m$ noise samples $\vecseq[m]{z}$ from noise prior $p_n$. 
			
			\item Sample $m$ data samples $\vecseq[m]{x}$ from data distribution $p_{data}$. 
			
			\item Update discriminator by \textit{ascending} $\nabla_{\theta_d} V(\md, \mg)$.
		\end{compactenum}
	
		\item Train $\mg$: Sample another $m$ noise samples $\vecseq[m]{z}$ and \textit{descend} on $\nabla_{\theta_g} V(\md, \mg)$. 
	\end{compactenum}
\end{algorithm}

\myspace
\p \blue{Global Optimality of $p_g \equiv p_{data}$} (4.1). 

\begin{definition}[-1em][Proposition 1]
	For fixed $\mg$, the optimal $\md$ is 
	\begin{align}
		D_G^*(\vec x) = \frac{ p_{data}(\vec x) }{ p_{data}(\vec x) + p_g(\vec x) } \label{eq:gans-2}
	\end{align}
\end{definition}

\begin{example}[Derivation of $D_G^*(\vec x)$.]

\textbf{Aside: Law of the unconscious statistician} (LotUS). 
	The distribution $p_g(\vec x)$ should be read as ``the probability that the output of $G$ yields the value $\vec x$.'' Take a step back and recognize that $G$ is simply a function of a random variable $\vec z$. As such, we can apply familiar rules like
	\begin{align}
		\E{G(\vec z)}
			&= \E[\vec z \sim p_n]{G(\vec z)} \\
			&= \int_{\vec z} p_n(\vec z) G(\vec z) \mathrm{d}\vec z
	\end{align}
	
	However, recall that functions of random variables can themselves be interpreted as random variables. In other words, we can also use the interpretation that $G$ evaluates to some output $\vec x$ with probability $p_g(\vec x)$. 
	\begin{align}
		\E{G} 
			&= \E[\vec x \sim p_g]{\vec x} \\
			&= \int_{\vec x} p_g(\vec x) \vec x \mathrm{d}\vec x
	\end{align}
	As \href{https://srome.github.io/An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes/}{this blog post} details, this equivalence is NOT due to a change of variables, but rather by the \textbf{Law of the unconscious statistician}. \\
	
	
\textbf{The Proof}: We can directly use LotUS to rewrite $V(\mg, \md)$:
	\begin{align}
		V(\mg, \md)
			&= \mpurple{ \E[\vec x \sim p_{data}]{\log D(\vec x)} } 
			+ \E[\vec z \sim p_n]{\log \left( 1 - \md ( \mg (\vec z) )  \right) } \\
			&=  \mpurple{ \E[\vec x \sim p_{data}]{\log D(\vec x)} } 
			+ \E[\mgreen{ \vec x \sim p_g } ]{  \log \left(1 -  \md (\mgreen{ \vec x } )   \right) } \\
			&= \int_{\vec x} \left[
				p_{data}(\vec x) \log \left(\md (\vec x)\right) +
				\mgreen{p_g(\vec x)} \log \left(1 - \md(\vec x)\right)
			\right] \mathrm{d} \vec x
	\end{align}
	LotUS allowed us to express $V(\mg, \md)$ as a continuous function over $\vec x$. More importantly, it means we can evaluate $\pderiv{V}{D}$ and take the derivative inside the integral\footnote{Also remember that $D(\cdot) \in [0, 1]$ since it is a probability distribution.}. Setting the derivative to zero and solving for $D$ yields $D*_G$, the form that maximizes $V$. 
\end{example}

\Needspace{15\baselineskip}
The authors use this proposition to define the virtual training criterion $C(G) \triangleq V(G, D^*_G)$:
\begin{align}
	C(G)
		&= \E[\vec x \sim p_{data}]{\log    \frac{ p_{data}(\vec x) }{ p_{data}(\vec x) + p_g(\vec x) }      } 
		+  \E[\vec x \sim p_{g}]{\log    \frac{ p_{g}(\vec x) }{ p_{data}(\vec x) + p_g(\vec x) }      } \label{eq:gans-3}
\end{align}

\begin{definition}[-1em][Theorem 1.]
	The global minimum of $C(G)$ is achieved IFF $p_g = p_{data}$. At that point $C(G) = - \log4$. 
\end{definition}

\begin{example}[Proof: Theorem 1]
	The authors subtract $V(D^*_G, G; p_g{=}p_{data})$ from both sides of \ref{eq:gans-3}, do some substitions, and find that
	\begin{align}
		C(G) = 2 \cdot JSD(p_{data} || p_g) - \log 4
	\end{align}
	where $JSD$ is the \purple{Jensen-Shannon divergence}\footnote{Recall that the JSD represents the divergence of each distribution from the mean of the two}. Since $0 \le JSD(p || q)$ always, with equivalence only if $p \equiv q$, this proves Theorem 1 above.
\end{example}
















% ===========================================================================================
\lecture{Papers and Tutorials}{A Framework for Intelligence and Cortical Function}{January 01, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize J. Hawkins et al., ``A Framework for Intelligence and Cortical Function Based on Grid Cells in the NeoCortex,'' Numata Inc. (Oct 2018).}


\p \blue{Introduction}. Authors propose new framework based on location processing that provides supporting evidence to the theory that \textbf{all regions of the neocortex are fundamentally the same}. We've known that \green{grid cells} exist in the hippocampal complex of mammals, but only recently have seen evidence that they may be present in the neocortex.

\myspace
\p \blue{How Grid Cells Represent Location}. Grid cells in the \purple{entorhinal cortex}\footnote{The entorhinal cortex is located in the medial temporal lobe and functions as a hub in a widespread network for memory, navigation and the perception of time.} represent space and location. The main concepts, in order such that they build on one another, are as follows:
\begin{compactitem}
	\item A \textbf{single grid cell} is a neuron that fires [when the agent is] at [one of many] multiple locations in a physical environment\footnote{For example, there may be a grid cell in my brain that fires when I'm at certain locations inside my room. Those locations tend to form a lattice of sorts.}.
	
	\item A \textbf{grid cell module} is a set of grid cells that activate with the \textit{same lattice spacing and orientation} but at shifted locations within an environment. 
	
	\item \textbf{Multiple grid cell modules} that differ in tile spacing and/or orientation can provide \textit{unique location} information\footnote{A single module alone cannot, because it repeats periodically. In other words, it can only provide relative location information.}.
\end{compactitem}
Crucially, the number of unique locations that can be represented by a set of grid cell modules \textbf{scales exponentially} with the number of modules. Every learned environment is associated with a set of unique locations (firing patterns of the grid cells).

\myspace\Needspace{10\baselineskip}
\p \blue{Grid Cells in the Neocortex}. The authors propose that we learn the structures of objects (like pencils, cups, etc) via grid cells in the \textit{neocortex}. Specifically, they propose:
\begin{compactenum}
	\item Every cortical column has neurons that perform a function similar to grid cells.
	
	\item Cortical columns learn models of \textit{objects} similarly to how grid/place cells learn models of \textit{environments}.
\end{compactenum}



% ===========================================================================================
\lecture{Papers and Tutorials}{Large-Scale Study of Curiosity Driven Learning}{January 05, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Burda et al., ``Large-Scale Study of Curiosity Driven Learning,'' OpenAI and UC Berkeley (Aug 2018).}

An agent sees observation $x_t$, takes action $a_t$, and transitions to the next state with observation $x_{t + 1}$. Goal: incentivize agent with reward $r_t$ relating to how informative the transition was. The main components in what follows are:
\begin{compactitem}
	\item Observation embedding $\phi(x)$. 
	\item Forward dynamics network for prediction $P(\phi(x_{t+1} \mid x_t, a_t)$. 
	\item Exploration reward (\textit{surprisal}):
	\graybox{
		r_t 
			&= -\log p\left( \phi\left(x_{t+1}\right) \mid x_t, a_t \right)	
	}
\end{compactitem}
The authors choose to model the next state embedding with a Gaussian,
\begin{align}
	\phi(x_{t+1}) \mid x_t, a_t
		&\sim \mathcal{N}(f(x_t, a_t), \epsilon ) \\
	r_t 
		&= ||f(x_t, a_t) - \phi(x_{t+1})||_2^2
\end{align}
where $f$ is the learned dynamics model. 

\myspace 
\p \blue{Feature spaces} (2.1). Some possible ways to define $\phi$:
\begin{compactitem}
	\item \textbf{Pixels}: $\phi(x) = x$.
	
	\item \textbf{Random Features}: Literally feeding $\phi(x) = ConvNet(x)$ where ConvNet is \textit{randomly initialized} and fixed. 
	
	\item \textbf{VAE}: Use the mapping to the mean [of the approximated distribution] as the embedding network $\phi$. 
\end{compactitem}


\myspace
\p \blue{Interpretation}. It seems that this works because after awhile, it is boring and predictable to take actions that result in losing a game. The most surprising actions seem to be those that advance us forward, to new and uncharted territory. However, these experiments are all on games that have a very "linear" uni-directional-like sequence of success. I wonder how successful this would be in a game like rocket league, where there is no tight coupling of direction with success and novelties (e.g. moving forward in mario bros). 







% ===========================================================================================
\lecture{Papers and Tutorials}{Universal Language Model Fine-Tuning for Text Classification}{March 02, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize J. Howard and S. Ruder, ``Universal Language Model Fine-Tuning for Text Classification,'' (May 2018).}

\p \blue{TL;DR}. ULMFiT is a transfer learning method. They introduce techniques for fine-tuning language models.

\myspace
\p \blue{Universal Language Model Fine-tuning} (3). Define the general \green{inductive transfer learning} setting for NLP:
\begin{definition}[-1em]
	Given a source task $\mathcal{T}_s$ and target task $\mathcal{T}_T \ne \mathcal{T}_S$, improve performance on $\mathcal{T}_T$. 
\end{definition}
ULMFiT is defined by the following three steps:
\begin{compactenum}
	\item General-domain LM pretraining.
	\item Target task LM fine-tuning.
	\item Target task classifier fine-tuning.
\end{compactenum}

\myspace 
\p \blue{Target Task LM Fine-tuning} (3.2). For step 2, the authors propose what they call \green{discriminative fine-tuning} and \green{slanted triangular learning rates}.
\begin{itemize}
	\item \textbf{Discriminative fine-tuning}. Tune each layer with different learning rates:
	\graybox{
		\theta_t^{\ell} &= \theta_{t - 1}^{\ell} - \eta^{\ell} \cdot \nabla_{\theta^{\ell}} J(\theta)	
	}
	The authors suggest setting $\eta^{\ell - 1} = \eta^{\ell} / 2.6$. 
	
	\item \textbf{Slanted triangular learning rates}. A type of learning rate schedule that looks like the picture below. 
	\myfig[0.4\textwidth]{figs/ulmfit.png}
	
	\Needspace{10\baselineskip}
	First, we define the following hyperparameters:
	\begin{compactitem}
		\item $T$: \textit{total} number of training iterations\footnote{Steps-per-epoch times number of epochs.}
		\item $cfrac$: fraction of $T$ (in num iterations) where we're \textit{increasing} the learning rate. 
		\item $cut$: $\lfloor T \cdot cfrac  \rfloor$. Iteration where we switch from increasing the LR to decreasing it.
		\item $ratio$: $\eta_{max} / \eta_{min}$. We of course must also define $\eta_{max}$. 
	\end{compactitem}
	We can now compute the learning rate for a given iteration $t$:\marginnote{Suggested:\\ $cfrac{=}0.1$ \\ $ratio{=}32$\\ $\eta_{\max}{=}0.01$}[2em]
	\graybox{
		\eta_t &= \eta_{max} \cdot \frac{ 1 + p \cdot (ratio - 1)  }{ ratio } \\
		p &= \begin{cases}
			\frac{t}{cut} & \text{if } t < cut \\
			1 - \frac{t - cut}{ cut \cdot (1 / cfrac - 1) } & \text{otherwise} 
		\end{cases}
	}
\end{itemize}

\myspace
\p \blue{Target Task Classifier Fine-tuning} (3.3). Augment the LM with two fully-connected layers. The first with ReLU activation and the second with softmax. Each uses batch normalization and dropout. The first is fed the output hidden state of the LM concatenated with the max- and mean-pooled hidden states over all timesteps\footnote{Or just as much as we can fit into GPU memory.}:
\graybox{
	\vec[c]{h} &= \left[ 
		\vec[t]{h}, \text{maxpool}(\matr H), \text{meanpool}(\matr H)
	\right]
}
In addition to DF-T and STLR from above, they also employ the following techniques:
\begin{compactitem}
	\item  \green{Gradual Unfreezing}: first unfreeze the last layer and fine-tune it alone with all other layers frozen \textit{for one epoch}. Then, unfreeze the next layer and fine-tune the last-two layers only \textit{for the next epoch}. Continue until the entire network is being trained, at which time we just train until convergence.
	\item \green{BPTT for Text Classification}. Divide documents into fixed-length ``batches''\footnote{Not a fan of how they overloaded this term here.} of size $b$. They initialize the $i$th section with the final state of the model run on section $i -1$. 
\end{compactitem}














% ===========================================================================================
\lecture{Papers and Tutorials}{The Marginal Value of Adaptive Gradient Methods in Machine Learning}{March 10, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Wilson et al., ``The Marginal Value of Adaptive Gradient Methods in Machine Learning,'' (May 2018).}


\p \blue{TL;DR}. For simple overparameterized problems, adaptive methods (a) often find drastically different solutions than SGD, and (b) tend to give undue influence to spurious features that have no effect on out-of-sample generalization. They also found that tuning the initial learning and decay scheme for Adam yields significant improvements over its default settings in all cases.

\myspace
\p \blue{Background} (2). The gradient updates for general stochastic gradient, stochastic momentum, and adaptive gradient methods, respectively, can be formalized as follows\footnote{I'm defining $\Delta w_{k + 1} \triangleq w_{k + 1} - w_k$.}
\begin{align}
	\mtgreen{[regular]} 	\quad	\Delta w_{k + 1} 
		&= - \alpha_k \widetilde{\nabla} f(w_k) \\
	\mtgreen{[momentum]} \quad	\Delta w_{k + 1} 
		&= - \alpha_k \widetilde{\nabla} f\left( w_k + \gamma_k   \Delta w_k    \right)
			+ \beta_k \Delta w_k \\
	\mtgreen{[adaptive]} \quad \Delta w_{k + 1}
		&= - \alpha_k \matr[k]{H}^{-1} \widetilde{\nabla} f\left( w_k + \gamma_k  \Delta w_k    \right)
			+ \beta_k \matr[k]{H}^{-1}  \matr[k - 1]{H} \Delta w_k
\end{align}
where $H_k$ is a p.d. matrix involving the entire sequence of iterates $(w_1, \ldots w_k$). For example, regular momentum would be $\gamma_k{=}0$, and Nesterov momentum would be $\gamma_k{=}\beta_k$. In practice, we basically always define $H_k$ as the diagonal matrix:
\begin{align}
	H_k := \text{diag}  \left[
		\left(
			\sum_{i=1}^k \eta_i \vec[i]{g} \odot  \vec[i]{g}
		\right)^{1/2}
	\right]
\end{align}

\myspace
\p \blue{The Potential Perils of Adaptivity} (3). Consider the binary least-squares classification problem, where we aim to minimize
\begin{align}
	R_s[w] &:= \onehalf ||Xw - y||_2^2
\end{align}
where $X \in \R^{n \times d}$ and $y \in \{-1, 1\}^{n}$. 

\begin{definition}[-1em][Lemma 3.1]
	If there exists a scalar $c$ s.t. $X \text{sign}(X^T y) = cy$, then (assuming $w_0 := 0$) AdaGrad, Adam, and RMSProp all converge to the unique solution $w \propto \text{sign}(X^T y)$.
\end{definition}





% ===========================================================================================
\lecture{Papers and Tutorials}{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}{March 17, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Y. Gal and Z. Ghahramani, ``A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,'' \textit{University of Cambridge} (Oct 2016).}

\p \blue{Background} (3). In Bayesian regression, we want to infer the parameters $\vec \omega$ of some function $\vec y = f^{\vec \omega}(\vec x)$. We define a prior, $p(\vec \omega)$, and a likelihood,
\begin{align}
	p(y{=}d \mid \vec x , \vec \omega)
		&= \text{Cat}_d\left(
			\text{softmax}\left(
				f^{\vec \omega}(\vec x)
			\right)
		\right)
\end{align}
for a classification setting. Given a dataset $\matr X, \matr Y$, and some new point $\vec{x}^*$, we can predict its output via
\begin{align}
	p(\vec{y}^* \mid \vec{x}^*, \matr X, \matr Y)
		&= \int p(\vec{y}^* \mid \vec{x}^*, \vec \omega) p(\vec \omega \mid \matr X, \matr Y) \mathrm{d}\vec\omega 
\end{align}
In a \green{Bayesian neural network}, we place the prior over the NNs weights (typically Gaussians). The posterior $p(\vec \omega \mid \matr X, \matr Y)$ is usually intractable, so we resort to \green{variational inference} to approximate it. We define our approximating distribution $\vec q (\vec \omega)$ and aim to minimize the KLD:
\graybox{
	\text{KL}\left(
		q(\vec \omega) || p(\vec \omega \mid \matr X, \matr Y)
	\right)
		&\propto 
			- \int q(\vec \omega) \log p(\matr Y \mid \matr X, \vec \omega) \mathrm{d}\vec \omega
			+ \text{KL}(q(\vec \omega) || p(\vec \omega)) \\
		&= \sum_{i=1}^{N} \int q(\vec \omega) \log p(\vec[i]{y} \mid f^{\vec \omega}(\vec[i]{x})) \mathrm{d}\vec\omega
			+ \text{KL}(q(\vec\omega) || p (\vec\omega))
}


\myspace
\p \blue{Variational Inference in RNNs} (4). The authors use MC integration to approximate the integral. The use only a single sample $\hat{\vec \omega} \sim q(\vec \omega)$ for each of the $N$ summations, resulting in an unbiased estimator. Plugging this in, we obtain our objective:
\graybox{
	\mathcal L &\approx 
		- \sum_{i=1}^{N} \log p \left(
			\vec[i]{y} \mid f_{\vec y}^{ \hat{\vec[i]{\omega}} } \left(
				f_{\vec h}^{  \hat{\vec[i]{\omega}} }  \left(      \vec[i, T]{x}, \vec[T - 1]{h}  \right)
			\right)
		\right) + \text{KL}(q( \vec\omega )    || p(\vec \omega))
}
The crucial observations here are:
\begin{compactitem}
	\item For each sequence $\vec[i]{x}$, we sample a new realization $\hat{\vec[i]{\omega}}$. 
	\item For each of the $T$ symbols in $\vec[i]{x}$, we use that \textit{same} realization. 
\end{compactitem}
We define our approximating distribution to factorize over the weight matrices and their rows in $\vec\omega$. For each weight matrix row $\vec[k]{w}$, we have
\graybox{
	q(\vec[k]{w})
		&\triangleq p \mathcal{N}(\vec[k]{w}; \vec{0}, \sigma^2 I)
			+ (1 - p) \mathcal{N}(\vec[k]{w}; \vec[k]{m}, \sigma^2 I)
}
with $\vec[k]{m}$ \green{variational parameter} (row vector). 







% ===========================================================================================
\lecture{Papers and Tutorials}{Improving Neural Language Models with a Continuous Cache}{March 24, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize E. Grave, A. Joulin, and N. Usunier, ``Improving Neural Language Models with a Continuous Cache,'' \textit{Facebook AI Research} (Dec 2016).}

The cache stores pairs $(h_t, x_{t+1})$ of the final hidden-state representation at time t, along with the word which was \textit{generated}\footnote{They say this, but everything else in the paper strongly suggests they mean the next gold-standard input instead.} based on this representation. 
\graybox{
	p_{vocab}(w \mid \slice{x})
		&\propto \exp\left( h_t^T o_w \right) \\
	p_{cache}(w \mid \slice{h}, \slice{x}) 
		&\propto \sum_{i=1}^{t - 1} \ind_{x_{i+1}{=}w} \exp\left(\theta h_t^T h_i \right) \\
		&= \sum_{ \substack{ (x, h) \in cache \\ s.t. ~ x{=}w } }  \exp\left(\theta h_t^T h \right) \\
	p( w \mid \slice{h}, \slice{x} )
		&= (1 - \lambda) p_{vocab}(w \mid h_t) + \lambda p_{cache}(w \mid \slice{h}, \slice{x})
}
where $\theta$ is a scalar parameter that controls the flatness of the cache distribution. 











% ===========================================================================================
\lecture{Papers and Tutorials}{Protection Against Reconstruction and Its Applications in Private Federated Learning}{April 26, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize A. Bhowmick et al., ``Protection Against Reconstruction and Its Applications in Private Federated Learning,'' \textit{Apple, Inc.} (Dec 2018).}


\p \blue{Introduction} (1). In many scenarios, it is possible to reconstruct model inputs $x$ given just $\nabla_{\theta} \ell (\theta; x, y)$. \green{Differential privacy} is one approach for obscuring the gradients such that guarantees can be made regarding risk of compromising user data $x$. \green{Locally private} algorithms, however, are preferred to DP when the user wants to keep their data private even from the data collector. The authors want to find a way to perform SGD while providing both local privacy to individual data $X_i$ and stronger guarantees on the global privacy of the output $\hat \theta_n$ of their procedure. \\

Formally, say we have two users' data $x$ and $x'$ (both in $\mathcal X$) and some randomized mechanism $M : \mathcal X \mapsto \mathcal Z$. We say that $M$ is \green{$\varepsilon$-local differentially private} if $\forall x, x' \in \mathcal X$ and sets $S \subset \mathcal Z$:
\graybox{
	\dfrac{
		\Prob{ M(x) \in S }
	}{
		\Prob{ M(x') \in S }
} \le e^{\varepsilon}
}
Clearly, the RHS will need to be pretty big for this to be achievable. The authors claim that allowing $\varepsilon >> 1$ ``may [still] provide meaningful privacy protections.''

\bluesec{Privacy Protections} (2). The focus here is on the \green{curious onlooker}: an individual (e.g. Apple PFL engineer) who can observe all updates to a model and communication from individual devices. Let $X$ denote some user data. Let $\Delta W$ denote the weights difference after some model update using the data $X$. Let $Z$ be the result of the randomized mapping $\Delta W \mapsto Z$. Our setting can be described with the Markov chain $X \rightarrow \Delta W \rightarrow Z$. The onlooker observes $Z$ and wants to estimate some function $f(X)$ on the private data.\\

\textbf{Separated private mechanisms} (2.2). The authors propose, instead of a simple mapping $\Delta W \rightarrow Z$, to split it up into two parts: $Z_1 = M_1(U)$ and $Z_2 = M_2(R)$, where 
\begin{align}
	U &= \frac{\Delta W}{||\Delta W||_2} \\
	R &= ||\Delta W||_2
\end{align}

\begin{definition}[-1em][Separated Differential Privacy]
	A pair of mechanisms $M_1, M_2$ mapping from $\mathcal U \times \mathcal R$ to $\mathcal Z_1 \times \mathcal Z_2$ is \green{$(\varepsilon_1, \varepsilon_2)$-separated differentially private} if  $M_1$ is $\varepsilon_1$-locally differentially private and $M_2$ is $\varepsilon_2$-locally differentially private.
\end{definition}



\bluesec{Privatizing Unit $\ell_2$ Vectors with High Accuracy} (4.1). Given some vector $u \in \mathbb{S}^{d-1}$\footnote{Here, this denotes an n-sphere: $$ \mathbb{S}^n \triangleq \{ x \in \R^{n+1} : ||x|| = r  \} $$}, we want to generate an $\varepsilon$-differentially private vector $Z$ such that
\begin{align}
	\E{Z \mid u} = u \qquad \forall u \in \mathbb{S}^{d-1}
\end{align}

\begin{algorithm}[Privatized Unit Vector: \texttt{PrivUnit}$_2$]
	Sample random vector $V$:
	\begin{align}
		V \sim \begin{cases}
			U\left( 
				\{  v \in \mathbb{S}^{d-1} \mid \langle v, u \rangle \mred{ \geq } \gamma  \} 
				\right) & \text{with probability } p \\
						U\left( 
			\{  v \in \mathbb{S}^{d-1} \mid \langle v, u \rangle \mred{ < } \gamma  \}
			\right) & \text{otherwise}
		\end{cases}
	\end{align}
	where $\gamma \in [0, 1]$ and p $\geq \onehalf$ together control \textit{accuracy}  and \textit{privacy}.\\ 
	
	Let $\alpha = \tfrac{d-1}{2}$, $\tau = \tfrac{1+\gamma}{2}$, and
	\begin{align}
		m = \frac{  (1-\gamma^2)\alpha  }{  2^{d-2} (d-1)  } \left[
			\frac{p}{B(\alpha, \alpha) - B(\tau; \alpha, \alpha)}
			- \frac{1 - p}{B(\tau; \alpha, \alpha)}
		\right]
	\end{align}
	where $B(x, \alpha, \beta)$ is the incomplete beta function (see paper pg 17 for details). \\
	
	Return $Z = \inv m \cdot V$
\end{algorithm}


\bluesec{Privatizing the Magnitude} (4.3). We also need to privatize the weight delta norms. We want to return values $r \in [0, r_{max}]$ for some $r_{max} < \infty$. 






% ===========================================================================================
\lecture{Papers and Tutorials}{Context Dependent RNN Language Model}{June 02, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize T. Mikolov and G. Zweig, ``Context Dependent Recurrent Neural Network Language Model,'' \textit{BRNO and Microsoft} (2012).}

\bluesec{Model Structure (2)}. Given one-hot input vector $\vec[t]{x}$, output a probability distribution $\vec[t]{y}$ for the next word. Incorporate a \textit{feature vector} $\vec[t]{f}$ that will contain topic information.
\begin{align}
	\vec[t]{y}
		&= \text{Softmax}\left( \matr V \vec[t]{h} + \matr G \vec[t]{f} \right) \\
	\vec[t]{h}
		&= \sigma\left( \matr U \vec[t]{x} + \matr W \vec[t-1]{h} + \matr F \vec[t]{f}  \right)
\end{align}

\bluesec{LDA for Context Modeling (3)}. ``Documents'' fed to LDA here will be individual sentences. The generative process assumed by LDA is compactly defined by the following sequence of operations\footnote{$\alpha$ is a vector with number of elements equal to number of topics.}:\marginnote{N: number of words\\$\Theta_i \equiv p(\text{topic}[i])$\\$z_n$: topic of word $n$}[2em]
\begin{align}
	N &\sim \text{Poisson}(\xi) \\
	\Theta &\sim \text{Dir}(\alpha) \\
	z_n &\sim \text{Multinomal}(\Theta) \\
	w_n &\sim \Prob{w_n \mid z_n, \beta}
\end{align}
where $\Prob{w_n \eq a \mid z_n \eq b} = \beta_{b, a}$, so we are really just sampling from row $z_n$ of $\beta$, where $\beta \in [0, 1]^{Z \times V}$ (where Z is number of topics). The result of LDA is a learned value for $\alpha$, and the topic distributions $\beta$.

\graybox{
	\vec[t]{f}
		&= \inv{Z} \prod_{i=0}^{K} \vec[ x_{t-i} ]{t} \\
	\vec[t]{f}
		&= \inv{Z} \vec[t-1]{f}^{\gamma} \vec[x_t]{t}^(1 - \gamma)
}








% ===========================================================================================
\lecture{Papers and Tutorials}{Strategies for Training Large Vocabulary Neural Language Models}{July 27, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Chen et al., ``Strategies for Training Large Vocabulary Neural Language Models,'' \textit{FAIR} (Dec 2018). arXiv:1512.04906}


\bluesec{Setup/Notation}. Note that in everything below, the authors are using a rather primitive feed-forward network as their language model. To predict $w_t$ it just concatenates the embeddings of the previous $n$ words and feeds it through a $k$-layer FF network. Then, layer $k+1$ is the dense projection and softmax:
\begin{align}
h^{k+1} &= W^{k+1} h^k + b^{k+1} \in \R^V \\
y &= \inv{Z} \exp\left\{ h^{k+1}  \right\}
\end{align}

Using cross-entropy loss, the derivative of $\log p(w_t\eq i)$ wrt the $j$th element of the logits is:
\begin{align}
\pderiv{ \log y_i }{ h_j^{k+1} }
&= \pderiv{}{h_j^{k+1}} \left[  
h^{k+1}_i - \log Z 
\right] \\
&= \delta_{ij} - y_j
\end{align}
When computing gradients of the cross-entropy loss, $y_i$ here is the ground truth. Therefore, to increase the probability of the correct token, we need to increase the logits element for that index, and decrease the elements for the others. Note how this implies we must compute the final activations for \textit{all words in the vocabulary}. 

\bluesec{Hierarchical Softmax} (2.2). Group words into one of two clusters $\{c_1, c_2\}$, based on unigram frequency\footnote{For example, you could just put the top 50\% in $c_1$ and the rest in $c_2$.}. Then model $p(w_t \mid x) = p(c_t \mid x) p(w_t \mid c_t)$ where $c_t$ is the class that word $w_t$ was assigned to.

\bluesec{NCE} (2.5). Define $P_{noise}(w)$ by the unigram frequency distribution. For each real token $w_t$ in the training set, sample $K$ noise tokens $\{ n_k \}_{k \eq 1}^{K}$. NCE aims to minimize
\begin{align}
L_{NCE}(   \vecseq[N]{w}   ) &= \sum_{i=1}^{N} \sum_{t=1}^{len(\vec w^{(i)})  } \left[ 
\log h(w_t^{(i)}) + \sum_{k=1}^{K} \log (  1 - h( n_k^{(i)}  )   ) 
\right] \\
h(w_t) 
&= \frac{ P_{model}(w) }{   P_{model}(w) + k P_{noise}(w)   } \\
&\approx \frac{ \widetilde P_{model}(w) }{  \widetilde  P_{model}(w) + k P_{noise}(w)   } 
\end{align}
where the final approximation is what makes NCE less computationally expensive in practice than standard softmax. This would seem to imply that NCE should approach standard softmax (in terms of correctness) as $k$ increases. 


\bluesec{Takeaways}.
\begin{compactitem}
	\item Hierarchical softmax is the fastest.
	\item NCE performs well on large-volume large-vocab datasets.
	\item Similar NCE values can result in very different validation perplexities. 
	\item Sampled softmax shows good results if the number of negative samples is at 30\% of the vocab size or larger. 
	\item Sampled softmax has a lower ppl reduction per step than others. 
\end{compactitem}



% ===========================================================================================
\lecture{Papers and Tutorials}{Product quantization for nearest neighbor search}{August 03, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize J\'{e}gou, ``Product quantization for nearest neighbor search,'' (2011)}


\bluesec{Vector Quantization}. Denote the \textit{index set} $\mathcal I = [0..k-1]$ and the set of reproduction values (a.k.a. \green{centroids}) $c_i$ as $\mathcal C = \{ \vec[i]{c} \in \R^D : i \in \mathcal I \}$. We refer to $\mathcal C$ as the \green{codebook} of size $k$.  A \green{quantizer} is a function $q: \vec x \mapsto \vec q(\vec x)$, where $x \in \R^D$ and $\vec q(\vec x) \in \mathcal C$. We typically evaluate the quality of a quantizer with mean squared error of the reconstruction:
\begin{align}
	MSE(q) &= \E[\vec x \sim p(\vec x)]{|| \vec x - \vec q (\vec x) ||_2^2 }
\end{align}
\vspace{1em}

In order for an optimizer to be optimal, it must satisfy the \green{LLoyd optimality conditions}:
\begin{align}
	\mtgreen{(1)}&
		\quad
		\vec q (\vec x) = \argmin_{c_i \in mathcal C} ||\vec x - \vec[i]{c}||_2 \\
	\mtgreen{(2)}&
		\quad
		\vec[i]{c} = \E[\vec x]{\vec x \mid i} \triangleq \int_{\mathcal V_i} \vec x p(\vec x) \mathrm{d} \vec x
\end{align}
a.k.a. literally just K-means. 

\bluesec{Product Quantization}. Input vector $\vec x \in \mathcal R^D$ is split into $m$ distinct subvectors $\vec[j]{u} \in \R^{D/m}$, where $j \in [1..m]$. Note that $D$ must be an integer multiple of $m$ (i.e. $D = a m$ for some $a \in \mathbb Z$).

\graybox{
	\vec x = 
		\underbrace{\slice[D^*]{x}}_{\vec[1]{u}(\vec x)}, \ldots, 
		\underbrace{\slice[D][D-D^*+1]{x}}_{\vec[m]{u}(\vec x)}
	\quad \rightarrow \quad
	\vec[1]{q} ( \vec[1]{u}(\vec x) ), \ldots, \vec[m]{q} ( \vec[m]{u}(\vec x) )
}
Note that each subquantizer $q_j$ has its own index set $\mathcal{I}_j$ and codebook $\mathcal C_j$. Therefore, the final reproduction value of a product quantizer is identified by an element of the product set $\mathcal I = \mathcal I_1 \times \cdots \times \mathcal I_m$. The associated final codebook is $\mathcal C = \mathcal C_1 \times \cdots \times \mathcal C_m$. 


% ===========================================================================================
\lecture{Papers and Tutorials}{Large Memory Layers with Product Keys}{August 03, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Lample et al., ``Large Memory Layers with Product Keys,'' \textit{FAIR} (July 2019). arXiv:1907.05242v1}


\bluesec{Memory Design} (3.1). The high-level structure (sequence of ops) is as follows:
\begin{compactenum}
	\item \green{Query network} computes some query vector $\vec q$. 
	
	\item Compare $\vec q$ with each \green{product key} via some scoring function.
	
	\item Select the $k$ highest scoring product keys. 
	
	\item Compute output $\vec m (\vec x)$ as weighted sum over the values associated with each of the top $k$ keys from the previous step.
\end{compactenum}
The \textbf{query network} is usually just a dense layer\footnote{They choose $d_q = 512$ as the output dimensionality of their query network.}. Since they want it to output query vectors with good coverage over the key space, the put a batch normalization layer before the query network\footnote{Recall that batch norm just normalizes the batch inputs to have 0 mean and unit standard deviation, followed by a scaling and bias factor.}. \\

The \textit{standard} way of doing key assignment/weighting is as follows:
\graybox{
	\mtgreen{[KNN]}& 
		\quad 
		\mathcal I \triangleq \text{TopK}(\vec q (\vec x)^T \vec[i]{k}) 
		\qquad 
		1 \le i \le \mathcal K \label{eq:standard-top-k} \\
	\mtgreen{[Normalize]}&
		\quad
		\vec w = \text{Softmax}\left( \mathcal I   \right) \\
	\mtgreen{[Aggregate]}&
		\quad
		\vec m(\vec x) = \sum_{i \in \mathcal I} w_i \vec[i]{v}
}
where equation \ref{eq:standard-top-k} is inefficient for large memory (key-value) stores. The authors propose instead a structured set of keys that they call \green{product keys}. Spoiler alert: it's just product quantization with $m \eq 2$ subvectors. Instead of using the flat key set $\mathcal K \triangleq \vecseq[|\mathcal K|]{k}$ with each $\vec[i]{k} \in \R^{d_q}$ from earlier, we redefine it as
\graybox{
	\mathcal K \triangleq \{  (\vec c , \vec c' ) \mid c \in \mathcal C, ~ c' \in \mathcal C'   \}
}
where both $\mathcal C$ and $\mathcal C'$ are sets of \textit{sub-keys} $\vec[i]{k} \in \R^{d_q/2}$. 

\Needspace{5\baselineskip}
Then\textellipsis
\begin{compactenum}
	\item Just run each of subvectors $q_1$ and $q_2$ through the standard TopK. You'll have $k$ sub-keys for both, defined by their index into their respective codebook. 
	
	\item Let $\mathcal K := \{ (\vec[i]{c}, \vec[j]{c}') \mid i \in \mathcal I_{\mathcal C}, ~ \mathcal I_{\mathcal C'}  \}$. This new reduced-size key set $\mathcal K$ has only $k \times k$ entries.
	
	\item Run the standard algorithm using the new reduced key set $\mathcal K$. 
\end{compactenum}

\red{TODO}: finish this note






% ===========================================================================================
\lecture{Papers and Tutorials}{Show, Ask, Attend, and Answer}{August 10, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize V. Kazemi and A. Elqursh, ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' \textit{Google Research} (April 2017).  arXiv:1704.03162v2}

\bluesec{TL;DR:} Good for a high-level overview of the VQA task, but extremely vague with so many details omitted it renders the paper fairly useless. 

\bluesec{Method} (3). Given a training set of image-question-answer triplets $(I, q, a)$, learn to estimate the most likely answer $\hat a$ out of the set of most frequent answers\footnote{Same approach as how we define vocabulary in language modeling tasks.} in the training data:
\begin{align}
	\hat a = \argmax_{a} \Prob{a \mid I, q}
\end{align}

The method utilizes the following architectural components:
\begin{compactitem}
	\item \textbf{Image Embedding} (3.1). Extracts features $\phi \eq \text{CNN}(I)$. 
	
	\item \textbf{Question Embedding} (3.2). Encode question $q$ as the final state of LSTM: $\vec s \eq \text{LSTM}(\text{Embed}(q))$. 
	
	\item \textbf{Stacked Attention} (3.3)\footnote{Authors do a laughably poor job at describing this part in any detail, so I'm taking the liberty of filling in the blanks. Blows my mind that papers this sloppy can even be published.} Seems like they feed $\text{Concat}[\vec s, \phi]$ through two layers of convolution to produce an output $F_c$ for $c \in [1..C]$ (meaning they do $C$ such convolutions separately an in parallel, like multi-head attention). This represents the scores for the attention function. The attention output, as usual, is computed as 
	\begin{align}
		\vec[c]{x} &= \sum_{\ell} \alpha_{c, \ell} \phi_{\ell} \\
		\alpha_{c, \ell} &\propto \exp F_c(\vec s, \phi_{\ell})
	\end{align}
	where $\ell$ appears to be over all [flattened] spatial indices of $\phi$. 
	
	\item \textbf{Classifier} (3.4). Concat the \green{image glimpses} $\vec[c]{x}$ with the LSTM output $\vec s$ and feed through a couple FC layers to eventually obtain softmax probabilities over each possible answer $a_i$, $i \in [1..M]$. 
\end{compactitem}

\bluesec{Dataset}. Although, again, the authors are horribly vague/sloppy here, it \textit{seems} like the data they use actually provides $K$ ``correct'' answers for each image-question pair. The model loss is therefore an average NLL over the $K$ true classes. 









% ===========================================================================================
\lecture{Papers and Tutorials}{Did the Model Understand the Question?}{August 10, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Mudrakarta et al., ``Did the Model Understand the Question?'' \textit{Univ. Chicago \& Google Brain} (May 2018).  arXiv:1805.05492v1}

\bluesec{TL;DR}. Basically all QA-related networks are dumb and don't learn what we think they learn. 
\begin{compactitem}
	\item Networks tend to make predictions based a tiny subset of the input words. Due to this, altering the non-important words in ways that may drastically change the meaning of the question can have virtually no impact on the network's prediction. 
	
	\item Networks assign high-importance to words like ``there'', ``what'', ``how'', etc. These are actually low-information words that the network should not heavily rely on.
	
	\item Networks rely on the image far more than the question.  
\end{compactitem}

\bluesec{Integrated Gradients (IG)} (3). Purpose: ``isolate question words that a DL system uses to produce an answer.''
\begin{align}
	F(\vec x \eq \seq[n]{x}) &\in [0, 1] \\
	A_F(\vec x, \vec x')
		&= \seq[n]{a} \in \R^n
\end{align}
where $\vec x'$ is some baseline input we use to compute the relative attribution of input $x$. The authors set $\vec x'$ as the ``empty question'' (sequence of padding values)\footnote{The use the same context though (e.g. the associated image for VQA). Only the question is changed.}.

\begin{definition}
	Given an input $x$ and baseline $x'$, the \green{integrated gradient} along the $i$th dimension is as follows.
	\graybox{
		IG_i(x, x')
			&\triangleq (x_i - x_i') \times \int_{\alpha \eq 0}^{1} \pderiv{F(x' + \alpha \times (x - x') ) }{ x_i } \mathrm{d}\alpha 
	}
\end{definition}
Interpretation: seems like IG gives us a better idea of the \textit{total} ``attribution'' of each input dimension $x_i$ relative to baseline $x_i'$ along the line connecting $x_i$ and $x_i'$, instead of just the immediate derivative around $x_i$. Although, the fact that infinitesimal contributions could cancel each other out seems problematic (positive and negative gradients along the interpolation). 







% ===========================================================================================
\lecture{Papers and Tutorials}{XLNet}{August 17, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Yang et al., ``XLNet: Generalized Autoregressive Pretraining for Language Understanding'' \textit{CMU \& Google Brain} (May 2018).}

\bluesec{TL;DR}: Instead of minimizing the NLL using $p(w_1, \ldots, w_T)$, minimize over NLL's using every possible order of the given word sequence. 

\bluesec{Background}. Recall that BERT does denoising auto-encoding. Given text sequence $\vec x = \seq{x}$, BERT constructs a corrupted version $\hat x$ by randomly masking out some tokens. Let $\bar{\vec x}$ denote the tokens that were masked. The BERT training objective is then
\begin{align}
\mtgreen{[BERT]} ~ \max_{\theta} ~~
\log p_{\theta} (\bar{\vec x} \mid \hat{\vec x})
&\approx 
\sum_{\bar x \in \bar{\vec x}} \log p_{\theta} (\bar x \mid \hat{\vec x}) \\
p(\bar x \mid \hat{\vec x})
&=  \text{Softmax}\left(  H_{\theta}(\hat{\vec x})_t^T e(\bar x) \right)
\end{align}

\bluesec{Objective \& Architecture}. Their proposed \green{permutation language modeling objective} is:
\graybox{
	\max_{\theta} ~~  \E[\vec z \sim \mathcal{Z}_T]{  \sum_{t=1}^{T} \log p_{\theta}(x_{z_t}    \mid  \slice[z_{t-1}][z_1]{x}   )   }
}
where $\mathcal Z_T$ is the set of all possible permutations of the length-$T$ index sequence $[1..T]$. To implement this, the authors had to re-parameterize the next-token distribution to be \textbf{target position aware}:
\graybox{
	p_{\theta}(X_{z_t} \eq x \mid \slice[z_{t-1}][z_1]{x})
		&= \text{Softmax}\left(   
			g_{\theta}\left( \slice[z_{t-1}][z_1]{x} , \mred{ z_t } \right)^T 
			e\left( x\right)      
		\right)
}
They accomplish this via \green{two-stream self-attention}, a technique that utilizes two sets of hidden representations (instead of one):
\begin{compactitem}
	\item \green{Content representation}: $h_{z_t} \triangleq h_{\theta}(\slice[z_{t}][z_1]{x})$.
	
	\item \green{Query representation}: $g_{z_t} \triangleq g_{\theta}(\slice[z_{t-1}][z_1]{x}, z_t)$.
\end{compactitem}

\Needspace{10\baselineskip}
The query stream is initialized with some vector $g_i^{(0)} \eq w$, and the content stream is initialized with word embedding $h_i^{(0)} \eq e(x_i)$. For the subsequent attention layers $1 \le m \le M$, they are computed respectively as follows: 
\begin{align}
	g_{z_t}^{(m)}
		&\leftarrow \text{Attention}(  Q \eq g_{z_t}^{(m-1)},  K \eq  V \eq \vec[z_{<t}]{h}^{(m-1)}      )
	\\
	h_{z_t}^{(m)}
		&\leftarrow \text{Attention}(  Q \eq h_{z_t}^{(m-1)},  K \eq  V \eq \vec[z_{ \leq t}]{h}^{(m-1)}      )
\end{align}


In practice, in order to speed up optimization, the authors do \green{partial prediction}: only train to predict over $\vec[z_{>c}]{x}$ targets rather than all of them.

\bluesec{Incorporating Ideas from Transformer-XL}. Often times, sequences are too long to feed all at once. The authors adopt relative positional encoding and segment-level recurrence from Transformer-XL. To compute the attention update with memory on a given segment, we use the content representations from the \textit{previous} segment, $\widetilde{\vec h}$, along with the current segment, $\vec[z_{\leq t}]{h}$ as follows:
\begin{align}
	h_{z_t}^{(m)}
		&\leftarrow \text{Attention}\left(
			Q \eq h_{z_t}^{(m-1)}, 
			K \eq V \eq \left[
				\widetilde{\vec h}^{(m-1)};
				\vec[z_{\leq t}]{h}^{(m-1)}
			\right]
		\right)
\end{align}





% ===========================================================================================
\lecture{Papers and Tutorials}{Transformer-XL}{August 24, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Dai et al., ``Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context'' \textit{CMU \& Google Brain} (Jan 2019).}

\DeclareDocumentCommand{\xlh}{ m m }{ \vec[#1]{h}^{#2} }
\DeclareDocumentCommand{\xlhtilde}{ m m }{ \mgreen{ \vec[#1]{\widetilde{h}}^{#2} } }
\DeclareDocumentCommand{\xls}{ m m }{ \vec[#1]{s}^{#2} }
\DeclareDocumentCommand{\xlq}{ m m }{ \vec[#1]{q}^{#2} }
\DeclareDocumentCommand{\xlk}{ m m }{ \vec[#1]{k}^{#2} }
\DeclareDocumentCommand{\xlv}{ m m }{ \vec[#1]{v}^{#2} }

\bluesec{Segment-Level Recurrence with State Reuse}. Denote two consecutive segments of length $L$ as $\vec[\tau]{s} = [x_{\tau, 1}, \ldots, x_{\tau, L}]$ and $\vec[\tau + 1]{s} = [x_{\tau +1, 1}, \ldots, x_{\tau + 1, L}]$. Denote output of layer $n$ given input segment $\vec[\tau]{s}$ as $\vec[\tau]{h}^n \in \R^{L \times d}$, where $d$ is the hidden dimension. To obtain the output of layer $n$ given the next segment, $\vec[\tau + 1]{s}$, do:
\graybox{
	\xlh{\tau + 1}{n}
		&= \text{TransformerLayer}( \xlq{\tau + 1}{n}, \xlk{\tau + 1}{n}, \xlv{\tau + 1}{n}  ) \\
		&= \text{TransformerLayer}( 
			\mpurple{ \xlh{\tau + 1}{n - 1} } \matr[q]{W}^T, ~
			 \xlhtilde{\tau + 1}{n - 1} \matr[k]{W}^T, ~
			 \xlhtilde{\tau + 1}{n - 1} \matr[v]{W}^T) \\
	\xlhtilde{\tau + 1}{n-1}
		&= \left[
			\text{SG}\left(   \xlh{\tau}{n -1}   \right)
			;
			\mpurple{ \xlh{\tau + 1}{n - 1} }
		\right] \label{eq:transformer-xl-concat}
}
where the concat in \ref{eq:transformer-xl-concat} is along the length (time) dimension. In other words, $\matr Q$ remains the same, but $\matr K$ and $\matr V$ get the previous segment prepended. Ultimately this only changes the inner dot products in the attention mechanism to attend over both segments. The $L$ output attention vectors are therefore each weighted sums over the previous $2L$ timesteps instead of just $L$. 


\bluesec{Relative Positional Encodings}. Instead of absolute positional encodings (as regular transformers do), only encode the \textit{relative} positional information in the hidden states. Ignoring the scale factor of $1/\sqrt{d_{k}}$, we can write the score for query vector $q_i = W_q (e_{x_i} + u_i)$ and key vector $k_j = W_k (e_{x_j} + u_j)$, for input embeddings $e$ and positional encodings $u$ as follows. Below it we show the authors proposed re-parameterized relative encoding version.
\graybox{
	A_{i,j}^{abs}
		&= \mgreen{ e_{x_i}^T } W_q^T W_k \mgreen{ e_{x_j} }
		+ \mgreen { e_{x_i}^T } W_q^T W_k \mblue{ u_j } 
		+ \mblue{ u_i^T } W_q^T W_k \mgreen{ e_{x_j} }
		+\mblue{ u_i^T } W_q^T W_k \mblue{ u_j } \\
	A_{i,j}^{res}
		&= 
		\underbrace{ \mgreen{ e_{x_i}^T } W_q^T W_{k, \mred{E}} \mgreen{ e_{x_j} } }_{     \mscript{cont-based addr}  }
		+ \underbrace{ \mgreen { e_{x_i}^T } W_q^T W_{k, \mred{R}} \mred{ r_{i - j} } }_{ \mscript{cont-dep pos bias} }
		+ \underbrace{ \mred{ \vec{u}^T }  W_{k, \mred{E}} \mgreen{ e_{x_j} } }_{  \mscript{global cont bias} }
		+\underbrace{ \mred{ \vec{v}^T } W_{k, \mred{R}} \mred{ r_{i-j} } }_{\mscript{global pos bias}}
}
where content is abbreviated as ``cont'' and positional is abbrev as ``pos''. I've shown all differences introduced by the second version in \red{red} font. It appears that $r_{i - j}$ is literally just $u_{i - j}$ but I guess using new letters is cool. Note that they separate $W_k$ into \green{content-based} $W_{k, E}$ and \green{location-based} $W_{k, R}$. 


% ===========================================================================================
\lecture{Papers and Tutorials}{Efficient Softmax Approximation for GPUs}{August 31, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Grave et al., ``Efficient Softmax Approximation for GPUs'' \textit{FAIR} (June 2017).}

\begin{algorithm}[Adaptive Softmax: Two-Level]
	Partition the vocabulary $\mathcal V$ into two clusters $\mathcal{V}_h$ and $\mathcal V_t$, where
	\begin{compactitem}
		\item $\mathcal V_h$ denotes the \textit{head}, consisting of the most frequent words.
		
		\item $\mathcal V_t$ denotes the \textit{tail}, associated with a \textbf{large number} of rare words.
		
		\item $|\mathcal V_h | << |\mathcal V_t|$ and $P(\mathcal V_h) >> P(\mathcal V_t)$.  
	\end{compactitem}

	To compute the probability of some word $w$ given context $h$, do:
	\graybox{
		\Prob{w \mid h}
			&= \begin{cases}
				P_{\mathcal V_h} (w \mid h) & \text{if } w \in \mathcal V_h \\
				P_{\mathcal V_t}(w \mid h) P_{\mathcal V_h} (\text{tail} \mid h) & \text{otherwise}
			\end{cases}
	}
	
	where both $P_{\mathcal V_h}$ and $P_{\mathcal V_t}$ are modeled with a softmax over the words in their respective clusters ($P_{\mathcal V_h}$ also includes the special ``tail'' token). 

\end{algorithm}

More generally, we can extend the above algorithm to N clusters (instead of 2). We can also adapt the \textit{capacity} of each cluster (varying their embedding size). The authors recommend, for each successive tail cluster, reducing the output size by a factor of 4. Of course, this then has to be followed by projecting back up to the number of words associated with the given cluster.

\red{TODO}: detail out how cross entropy loss is computed under this setup.






% ===========================================================================================
\lecture{Papers and Tutorials}{Adaptive Input Representations for Neural Language Modeling}{September 02, 2019}

% ===========================================================================================
\vspace{-1em}
{\footnotesize A. Baevski and M. Auli, ``Adaptive Input Representations for Neural Language Modeling'' \textit{FAIR} (Feb 2019).}

\bluesec{TL;DR}: Literally just adaptive softmax but for the input embeddings. Official implementation can be found \href{https://github.com/pytorch/fairseq/blob/master/fairseq/modules/adaptive_input.py}{here}. 


\bluesec{Adaptive Input Representations} (3). Same as Grave et al., they partition the vocabulary $\mathcal V$ into
\begin{align}
	\mathcal V &= \mathcal V_1 \cup \mathcal V_2 \cup \ldots \cup  \mathcal V_n
\end{align}
where $\mathcal V_1$ is the head and the rest are the tails (ordered by decreasing frequency). They reduce the capacity of each cluster by a factor of $k \eq 4$ (also same as Grave et al.). Finally, they add linear projections for each cluster's embeddings in order to ensure they all result in $d$-dimensional output embeddings (even $\mathcal V_1$). 




% ===========================================================================================
\lecture{Papers and Tutorials}{Neural Module Networks}{September 14, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Andreas et al., ``Deep Compositional Question Answering with Neural Module Networks'' \textit{UC Berkeley} (Nov 2015).}


\bluesec{NMNs for Visual QA} (4). Model and task overview:
\begin{compactitem}
	\item \textbf{Data}: 3-tuples $(w, x, y)$ containing the question, image, and answer, respectively.
	
	\item \textbf{Model}: fully specified by a collection of \green{modules} $\{m\}$. Each module $m$ has parameters $\theta_m$ and a \green{network layout predictor} $P(w)$ that maps from strings to networks. The high-level procedure is, for each $(w, x, y)$, do:
	\begin{compactenum}
		\item Instantiate a network based on $P(w)$.
		\item Pass the image $x$ (and possibly $w$ again) as inputs to the network. 
		\item Obtain network outputs encoding $p(y \mid w, x; \theta)$. 
	\end{compactenum}

	\item \textbf{Modules}:
\end{compactitem}
\myfig[0.6\textwidth]{figs/nmn_modules.png}

\bluesec{From strings to networks} (4.2).
\begin{compactenum}
	\item Parse question $w$ with the Stanford Parser to obtain universal dependency representation.
	
	\item Filter dependencies to those connected to the wh-word in the question. Some examples:
	\begin{compactitem}
		\item \textit{what is standing in the field} $\mapsto$ \texttt{what(stand)}
		\item \textit{what color is the truck} $\mapsto$ \texttt{color(truck)}
		\item \textit{is there a circle next to a square} $\mapsto$ \texttt{is(circle, next-to(square))}
	\end{compactitem} 

	\item Assign identities of modules (already have full network structure). 
	\begin{compactitem}
		\item Leaves become \texttt{attend} modules.
		\item Internal nodes become \texttt{re-attend} or \texttt{combine} modules.
		\item Root nodes become \texttt{measure} (y/n questions) or \texttt{classify} (everything else) modules.
	\end{compactitem}
\end{compactenum}

\bluesec{Answering natural language questions} (4.3). They combine the results from the module network with an LSTM, which is fed the question as input and outputs a predictive distribution over the set of answers\footnote{This is the same distribution that the root module is trying to predict}. The final prediction is a geometric average of the LSTM output probabilities and the root module output probabilities. 




% ===========================================================================================
\lecture{Papers and Tutorials}{Learning to Compose Neural Networks for QA}{September 14, 2019}
% ===========================================================================================
\vspace{-1em}
{\footnotesize Andreas et al., ``Learning to Compose Neural Networks for Question Answering'' \textit{UC Berkeley} (June 2016).}


\bluesec{TL;DR}. Improve initial NMN work (previous note) by (1) learning network predictor ($P(w)$ in previous paper) instead of manually specifying it, and (2) extending visual primitives from previous work to reason over structured world representations.

\myfig[0.5\textwidth]{figs/nmn2_fig1.png}

\bluesec{Model} (4). Training data consists of (world, question, answer) triplets $(w, x, y)$. The model is built around two distributions:
\begin{compactitem}
	\item \green{layout model} $p(z \mid x; \theta_{\ell})$ which predicts a layout $z$ for sentence $x$. 
	\item \green{execution model} $p_z(y \mid w; \theta_e)$ which applies the network specified by $z$ to the world representation $w$. 
\end{compactitem}

\Needspace{18\baselineskip}
\bluesec{Evaluating Modules} (4.1). The execution model is defined as
\begin{align}
	p_z(y \mid w) &= \left( \llbracket z \rrbracket_w  \right)_y
\end{align}
where $\llbracket z \rrbracket_w$ denotes the output of network with layout $z$ on input world $w$. The defining equations for all modules is as follows ($\sigma \equiv $ ReLU, $sm \equiv$ softmax):\marginnote{$\bar{w}(h) \triangleq \sum_k h_k w^{(k)}$}[3em]
\newcommand\nmn[1]{\llbracket \texttt{\small #1} \rrbracket}
\graybox{
	\nmn{lookup[i]} 
		&= e_{f(i)} \\
	\nmn{find[i]}
		&= \text{sm}(a \odot \sigma(B v^i \oplus CW \oplus d) ) \\
	\nmn{relate[i](h)}
		&= \text{sm}(
				a \odot 
				\sigma( 
					Bv^i \oplus CW \oplus D \bar{w}(h) \oplus e				
				)) \\
	\llbracket \texttt{\small and} (h^1, h^2, \ldots) \rrbracket
		&= h^1 \odot h^2 \odot \cdots \\
	\nmn{describe[i](h)}
		&= sm(A \sigma (B \bar w(h)) + v^i) \\
	\nmn{exists(h)}
		&= sm\left(  \left(\max_k h_k \right)a + b  \right)
}
To train, maximize 
\begin{align}
	\sum_{(w, y, z)} \log p_z (y \mid w; \theta_e)
\end{align}

\bluesec{Assembling Networks} (4.2). \red{TODO}: finish note



% ===========================================================================================
\lecture{Papers and Tutorials}{End-to-End Module Networks for VQA}{September 14, 2019}

% ===========================================================================================
\vspace{-1em}
{\footnotesize R. Hu, J. Andreas, et al., ``Learning to Reason: End-to-End Module Networks for Visual Question Answering'' \textit{UC Berkeley, FAIR, BU} (Sep 2017).}

\bluesec{End-to-End Module Networks} (3). High-level sequence of operations, given some input question and image:
\begin{compactenum}
	\item Layout policy predicts a coarse functional expression that describes the structure of the computation.
	
	\item Some subset of function applications within the expression receive parameter vectors predicted from the text. 
	
	\item Network is assembled with the modules according to layout expression to output an answer. 
\end{compactenum}


\bluesec{Attentional Neural Modules} (3.1). A neural module $m$ is a parameterized function $y = f_m(a_1, a_2, \ldots; x_{vis}, x_{txt}, \theta_m)$, where the $a_i$ are image attention maps and the output $y$ is either an image attention map or a probability distribution over answers. The full set of modules used by the authors, along with their inputs/outputs, is tabulated below. 
\myfig[0.7\textwidth]{figs/n2nmn_modules.png}

Note that, whereas the original NMN paper (see previous note) instantiated module types based on words (e.g. describe[shape] vs describe[where]) and gave different instantiations different parameters, this paper has a single module for each module type (no``instances'' anymore). To distinguish between cases where e.g. \texttt{describe} should describe a shape vs describing a location, the module incorporates a text feature $x_{txt}^{(m)}$ computed separately/identically for each module $m$:
\begin{align}
	x_{txt}^{(m)} &= \sum_{i=1}^T \alpha_i^{(m)} w_i
\end{align}

\bluesec{Layout Policy with Seq2Seq RNN} (3.2). \red{TODO} finish note



% ===========================================================================================
\lecture{Papers and Tutorials}{Fast Multi-language LSTM-based Online Handwriting Recognition}{October 01, 2019}

% ===========================================================================================
\vspace{-1em}
{\footnotesize Carbune et al., ``Fast Multi-language LSTM-based Online Handwriting Recognition'' \textit{Google AI Perception} (Feb 2019).}


\bluesec{Introduction} (1). Task: given input strokes, i.e. sequences of points $(x, y, t)$, output it in the form of text.

\bluesec{Model Architecture} (2). The high-level sequences of operations is:
\begin{compactenum}
	\item Input time series $(v_1, \ldots, v_T)$ encoding user input. The authors experiment with two representations:
	\begin{compactenum}
		\item \textit{Raw touch points}: sequence of 5-dimensional points $(x_i, y_i, t_i, p_i, n_i)$, where $t_i$ is seconds since first touch point in current observation, $p_i$ is binary-valued equal to 0 if pen-up, else 1 if pen-down, and $n_i$ is binary on start-of-new-stroke (1 if True). 
		\item \textit{B\'{e}zier curves}: TL;DR is that they model x, y, t each as a cubic polynomial over a new variable $s \in [0, 1]$. Ultimately this means solving for some coefficients $\Omega$ of a linear system of equations: $V^T Z = V^T V \Omega$. 
	\end{compactenum}
	
	\item Several BiLSTM layers for contextual character embedding.
	
	\item Softmax layer providing character probabilities at each time step. 
	
	\item CTC decoding with beam search. They also incorporate \green{feature functions} into the output logits to help with decoding. They use the following 3 feature functions:
	\begin{compactenum}
		\item Character language models. A 7-gram LM over Unicode codepoints using Stupid back-off.
		
		\item Word language models. 3-grams pruned to between 1.25M and 1.5M entries.
		
		\item Character classes. Scoring heuristic which boosts the score of characters from the LM's alphabet.
	\end{compactenum}
\end{compactenum}


\bluesec{Training} (3). Training happens in two stages, each on a different dataset:
\begin{compactenum}
	\item Training neural network model with CTC loss on large dataset.
	
	\item Decoder tuning using \green{Bayesian optimization} through \green{Gaussian Processes} in Vizier\footnote{Vizier is a  program made by Google for black-box tuning}.
\end{compactenum}




% ======================================================================
\lecture{Papers and Tutorials}{Multi-Language Online Handwriting Recognition}{October 02, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Keysers et al., ``Multi-Language Online Handwriting Recognition'' \textit{Google} (June 2017).}


\bluesec{System Architecture} (3). Segment-and-decode approach consisting of the following steps:
\begin{compactitem}
	\item Preprocessing (4).
	\begin{compactenum}
		\item Resampling. 
		\item Slope correction.
	\end{compactenum}
	
	\item \green{Segmentation}\footnote{\textbf{Segmentation/cut point}: a point at which another character my start. \textbf{Segment}: the (partial) strokes between 2 consecutive segmentation points.} and search lattice creation (5).
	\begin{compactenum}
		\item Segmentation goal: obtain high \textit{recall} of all actual character boundaries. Accomplished via a heuristic which creates a set of potential cut points and then a neural net which assigns a score to each.
		
		\item Segmentation lattice: a graph $(V, E)$ of ink segments. Each segment is identified by a unique integer index. 
		\begin{compactitem}
			\item Nodes (in $V$) define the path of ink segments up to that point (e.g. $\{1, 0, 2\}$) (i.e. a character hypothesis)
			\item Edges (in $E$) from a given node $v$ indicate the ink segments which are grouped in a character hypothesis. For example, if $v\eq\{i, j\}$ has some edge $k$, then that edge will have node $\{i, j, k\}$ on the other end, and $\{i, j, k\}$ is a valid character hypothesis. 
		\end{compactitem}
		It appears that each node (assign from the empty start node) is passed to the next stage as a character hypothesis to be scored/classified.
	\end{compactenum}
	
	\item Generation \& scoring of \green{character hypotheses}\footnote{\textbf{Character hypothesis}: a set of one or more segments (not necessarily consecutive).} (5.3). Goal: cetermine the characters most likely to have been written.
	\begin{compactenum}
		\item Feature extraction: they make a fixed-length dense feature vector containing \textit{pointwise} and \textit{character-global} features.
		
		\item Classification: single hidden layer NN with tanh activation followed by softmax.
		
		\item Create a labeled latice which will later be decoded to find the final recognition result.
	\end{compactenum}
	
	\item Best path search in the resulting lattice using additional knowledge sources (6). 
\end{compactitem}


\bluesec{Language Models} (6.1). They utilize two types of language models:
\begin{compactitem}
	\item Stupid-backoff entropy-pruned 9-gram character LM. This is their ``main'' LM. Depending on the language, they use about 10M to 100M n-grams.
	\item Word-based probabilistic finite automaton.  Creating using 100K most frequent words of a language.
\end{compactitem}


\bluesec{Search} (6.2). Goal: obtain a recognition result by finding the best path from the source node (no ink recognized) to the target node (all ink recognized). Algorithm: ink-aligned beam search that starts at the start node and proceeds through the lattice in topological order. 










% ======================================================================
\lecture{Papers and Tutorials}{Modular Generative Adversarial Networks}{October 13, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Zhao et al., ``Modular Generative Adversarial Networks'' \textit{UBC, Tencent AI Lab} (April 2018).}

\bluesec{TL;DR}. Task(s): multi-domain image \purple{generation} and image-to-image \purple{translation}.

\bluesec{Network Construction} (3.2). Let $x$ and $y$ denote the input and target image, respectively, wherever applicable. Let $\matr A = \{A_1, A_2, \cdots, A_n\}$ denote an \green{attribute set}. Four types of modules are used:
\begin{compactenum}
	\item Initial module is task-dependent (below). Output is feature map in $\R^{C \times H \times W}$. 
	\begin{compactitem}
		\item \purple{[translation]} \green{encoder $\matr E$}: $x \mapsto \matr E(x)$
		\item \purple{[generation]} \green{generator $\matr G$}: $(z, a_0) \mapsto \matr G(z, a_0) $ where $z$ is random noise and $a_0$ is a condition vector representing auxiliary information. 
	\end{compactitem}

	\item \underline{\green{transformer(s) $\matr[i]{T}$}} : $E(x) \mapsto \matr[i]{T}(\matr E(x), a_i)$. Modifies repr of attrib $a_i$ in the FM.
	\item \green{reconstructor $\matr R$} : $(\matr[i]{T}, \matr[j]{T}, \ldots) \mapsto y$. Reconstructs image from an intermediate FM. 
	\item \green{discriminator $\matr[i]D$}: $\matr R \mapsto \{0, 1\} \times \text{Val}(a_i)$. Predicts probability that $R$ came from $p_{true}$, and the [transformed] value of $a_i$. 
\end{compactenum}
The authors emphasize that the transformer module is their core module. It's architecture is illustrated below. 

\myfig[0.6\textwidth]{figs/mgan_transformer_fig3.png}


\bluesec{Loss Function} (3.4). 
\graybox{
	\mathcal{L}_{D}(\matr D)
		&= - \mgreen{ \insum \mathcal{L}_{adv_i} } 
			+ \lambda_{cls} \mpurple{ \insum \mathcal{L}_{cls_i}^r } \\
	\mathcal{L}_{G}(\matr E, \matr T, \matr R)
		&= \mgreen{ \insum \mathcal{L}_{adv_i} } 
			+ \lambda_{cls} \mblue{\insum \mathcal{L}_{cls_i}^{f} }
			+ \lambda_{cyc} \mred{ \lr{ \mathcal{L}_{cyc}^{\matr E \matr R} + \insum \mathcal{L}_{cyc}^{\matr[i]{T}}  } } \\
	\begin{split}
	\mgreen{ \mathcal{L}_{adv_i}(\matr E, \matr[i]T, \matr R, \matr[i]D) }
		&= \E[y \sim p_{data}(y)]{\log \matr[i]{D}(y)} + \\
		& \quad ~ \E[x \sim p_{data}(x)]{\log \lr{1 - \matr[i]{D} \lr{\matr R  \lr{\matr[i]{T} \lr{\matr E (x)}     }  }  }   } 
	\end{split}
	\\
	\begin{split}
		\mpurple{ \mathcal{L}_{cls_i}^r }
			&= - \E[x, c_i]{\log \matr[cls_i]{D}(c_i \mid x)}
	\end{split}
	\\
	\begin{split}
		\mblue{\mathcal{L}_{cls_i}^{f}}
			&= - \E[x, c_i]{ \log \matr[cls_i]{D}(c_i \mid  \matr R(\matr E(\matr[i]{T}(x))) ) }
	\end{split}
	\\
	\begin{split}
		\mred{ \mathcal{L}_{cyc}^{\bm{ER}} }
			&= \E[x]{|| \matr R(\matr E(x))   - x||_1  }
	\end{split}
	\\
	\begin{split}
		\mred{ \mathcal{L}_{cyc}^{\matr[i]{T}} }
		&= \E[x]{||   \matr[i]{T}(\matr E(x))     - \matr E(\matr R(\matr[i]{T}(\matr E(x))))       ||_1  }
	\end{split}
}
where $n$ is the total number of controllable attributes. 


% ======================================================================
\lecture{Papers and Tutorials}{Transfer Learning from Speaker Verification to TTS}{October 13, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Jia et al., ``Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis'' \textit{Google} (Jan 2019).}

\bluesec{TL;DR}: TTS that's able to generate speech in the voice of different speakers, including those unseen during training.


\bluesec{Multispeaker Speech Synthesis Model} (2). System is composed of three independently trained NNs:
\begin{compactenum}
	\item \green{Speaker Encoder}. Computes a fixed-dimensional vector from a speech signal.
	
	\item \green{Synthesizer}. Predicts a \purple{mel spectrogram} from a sequence of \purple{grapheme} or \purple{phoneme} inputs, conditioned on the speaker vector. Extension of \purple{Tacotron 2} to support multiple speakers.
	
	\item \green{Vocoder}. Autoregressive WaveNet, which converts the spectrogram into time domain waveforms.
\end{compactenum}








% ======================================================================
\lecture{Papers and Tutorials}{Tacotron 2}{November 10, 2019}
% ======================================================================
\vspace{-1em}
{\footnotesize Shen et al., ``NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET ON MEL SPECTROGRAM PREDICTIONS'' \textit{Google}, \textit{UCB} (Feb 2018).}

\bluesec{TL;DR}: Seq2seq network for mapping characters to mel-scale spectrograms\footnote{A \green{mel-frequency spectrogram} is obtained by applying a nonlinear transform to the frequency axis of the short-time Fourier transform (STFT).}, followed by a modified WaveNet vocoder. 


\bluesec{Spectrogram Prediction Network} (2.2). Illustrated below.
\myfig[0.5\textwidth]{figs/tacotron2.png}

\begin{compactitem}
	\item Encoder: character language model architecture. They use convolution layers in the middle under the hypothesis that these should learn ngram-like representations. 
	
	\item Attention Decoder: \green{location-sensitive} encoder-decoder attention\footnote{Basically additive attention with cumulative weights.}. At each timestep, instead of incorporating the previous decoder prediction, they first feed it through a \green{pre-net} (2 FF layers). Furthermore, they also have a \green{post-net} which predicts a residual to add to the prediction ``to improve the overall reconstruction''. \\
	
	Finally, they also project the decoder/attention outputs to a scalar (sigmoid activation) for predicting probability that output sequence has completed.

\end{compactitem}

\bluesec{WaveNet Vocoder} (2.3). Inverts the mel spectrogram feature representation into time-domain waveform samples. Specifically, I assume this means they take each time slice of the predicted spectrogram (a vector over frequencies?) and do \green{local conditioning} (as described in WaveNet paper) with $\vec[t]{h}$ the spectrogram at time slice $t$. 

\begin{example}[Digression: Fourier Transforms]
	We measure sound by pressure/amplitudes as a function of time. What we actually measure is the \textit{sum total} of all the individual sinusoidal waves each with their own frequency/amplitude. To reiterate: in the \textit{time domain} our x-axis is time (duh) and our y-axis is amplitude. The \green{Fourier transform} maps our time-domain representation into a \textit{frequency domain}. Now, the x-axis is frequency (duh) and the y-axis represents how much of our original signal consisted of waves with a given frequency. For example, if our original signal was literally a 3 Hz wave superimposed over some 5 Hz wave, our frequency-domain plot would show two spikes at x=3 and x=5, and be zero everywhere else. \\
	
	Formally, let $g(t)$ denote the amplitude of some sound wave at time $t$.
	\begin{align}
		g(t) e^{-2\pi i f t}
	\end{align}
	where 
	\begin{compactitem}
		\item The negative sign in the exponent is a convention that we should think about \textit{clockwise} rotations. 
		
		\item f denotes the ``winding'' frequency: the number of full cycles represented in our wound up graph. 
		
		\item Multiplying by $g(t)$ means that the distance-to-origin (magnitude) at time $t$ will always equal $g(t)$. 
	\end{compactitem}
	Similarly, the \green{inverse Fourier transform} maps from frequency domain to time domain.
	
	
		Remember that the trick for identifying the component frequencies is by computing the \green{center of mass} of this formula:
	\begin{align}
	\inv{t_2 - t_1} \int_{t_1}^{t_2} g(t) 	e^{-2\pi i f t} \mathrm{d} t
	\end{align}
	where, to do this for one full circle, we'd set $t_1 \eq 0$ and $t_2 \eq 1/f$. If our signal $g(t)$ does contain the frequency $f$, this integral will be relatively large in comparison with other frequencies. The final formula for the Fourier transform is just removing the scale factor (i.e. the FT is just the CoM scaled by the time interval of our signal):
	\graybox{
		\hat{g}(\omega)
		&\triangleq  \Re \left\{  \int_{t_1}^{t_2} g(t) 	e^{-2\pi i \omega t} \mathrm{d} t \right\}
	}
	More intuition regarding removal of the scale factor: if there is a component wave in $g(t)$ that only exists for a small portion of time $\delta t$, it would have a smaller value of $\hat{g}(\omega)$ than a component of some different frequency (but same amplitude) that persisted throughout the entirety of our time interval.
\end{example}

\begin{example}[Digression: Spectrograms]
	Now that we know about Fourier transforms (above example), we can define what a spectrogram is. Say that, instead of a single function $g(t)$, I split time into various windows $\{t_0, t_1, t_2, t_T\}$ and store a separate function, $g_{\tau}(t)$ with $1 \le \tau \le T$, for each window. Then, I apply the FT on each function, resulting in a set of $\hat{g}_{\tau}(\omega)$. If I plot a 2D heatmap with my x-axis denoting the window $\tau$, the y-axis denoting frequency $\omega$, and values (color) denoting $\hat{g}_{\tau}(\omega)$, I have a \green{spectrogram} (technically I need to transform my y-axis to $\log \omega$ and my color/value axis to Decibels). 
\end{example}




% ======================================================================
\lecture{Papers and Tutorials}{Glow}{November 16, 2019}
% ======================================================================

\vspace{-1em}
{\footnotesize Kingma et al., ``Glow: Generative Flow with Invertible 1x1 Convolutions'' \textit{OpenAI}, (July 2018).}

\newcommand\pt{p_{\theta}}

\bluesec{Flow-based Generative Models} (2). 
\begin{align}
	\vec z
		&\sim p_{\theta}(\vec z) \\
	\vec x 
		&= \vecfn[\theta]{g}{z}
\end{align}
where $\vec g$ is invertible. \textit{Inference} is done by $\vec z =\vecfn[\theta]{f}{x} = \vinv[\theta]{g}(\vec x)$. 
\begin{align}
	\log	\pt (\vec x)
		&= \log \pt (\vec z) + \log | \det \pderiv{\vec z}{\vec x} | \\
		&= \log \pt (\vec z) + \sum_{i=1}^K \log | \det \pderiv{\vec[i]{h}}{\vec[i-1]{h}} |
\end{align}
The log-determininant gives the change in log-density when going $\vec[i-1]{h} \rightarrow \vec[i]{h}$. We can see that maximum likelihood will encourage $\vecfn[\theta]{f}{x}$ to increase volume.


% ======================================================================
\lecture{Papers and Tutorials}{WaveGlow}{November 16, 2019}
% ======================================================================

\vspace{-1em}
{\footnotesize Prenger et al., ``WaveGlow: A Flow-Based Generative Network for Speech Synthesis''' \textit{NVIDIA}, (Oct 2018).}


\bluesec{TL;DR}: flow-based \green{vocoder}\footnote{Vocoder: network that transforms time-aligned features (e.g. spectrograms) into audio samples.}


\bluesec{WaveGlow} (2). Model the distribution of audio samples \textit{conditioned on} a mel-spectrogram. Note that the \textit{forward pass} is defined as going from $\vec x$ to $vec z$. Train by minimizing the negative log-likelihood:
\graybox{
	\vec z 
		&\sim \Gauss{\vec z; \vec 0, \matr I} \\
	\vec x
		&= \vec[0]{f} \circ \vec[1]{f} \circ \cdots \circ  \vec[k]{f}(\vec z) \\
	\log p_{\theta}(\vec x)
		&= \log p_{\theta}(\vec z) + \sum_{i=1}^{k} \log | \det \lr{   \matr J \lr{ \vinv[i]{f}(\vec x)  }  } | \\
	\vec z 
		&= \vinv[k]{f} \circ \vinv[k-1]{f} \circ \cdots \circ \vinv[0]{f}(\vec x)
}
and $\vec x$ denotes the output audio. The mel-spectrogram conditioning happens in the affine coupling layer(s). Training penalizes the norm of $\vec z$ and encourages each layer $f_i^{-1}$ to increase the log-density volume of the previous layer. 



\myfig[0.3\textwidth]{figs/waveglow.png}


\begin{align}
	\vec[a]{x}, \vec[b]{x}
		&= split(\vec x) \\
	\log \vec s , \vec t
		&= WN(\vec[a]{x}, mel-spectrogram) \\
	\vec[b]{x}'
		&= \vec s \odot \vec[b]{x} + \vec t \\
	\vinv[coupling]{f}(\vec x)
		&= concat(\vec[a]{x}, \vec[b]{x})
\end{align}




% ======================================================================
\lecture{Papers and Tutorials}{Solving Rubik's Cube with a Robot Hand}{January 18, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Akkaya et al., ``Solving Rubik's Cube with a Robot Hand''' \textit{OpenAI}, (Oct 2019).}

\bluesec{TL;DR}: \green{Automatic Domain Randomization} (ADR) + robot platform = solving rubik's cube from simulation alone. The two main structural components are (1) the hand and (2) a few cameras that observe the pose/state of the hand/cube.

\bluesec{ADR} (5). They denote an ``environment'' as $e_{\lambda}$, parameterized by $\lambda \in \R^d$. In other words, there are $d$ parameters/knobs they can fiddle with in simulation to change various characteristics of the given environment\footnote{I guess it comes down to semantics whether we interpret a $\Delta \lambda$ as a new environment or just the same environment with different characteristics.}.\marginnote{$$\lambda \in \R^d$$$$\phi \in \R^{d'}$$}. Here, the authors choose $d' = 2d$. Let $\phi^L, \phi^H \in \R^d$ be some partition of $\phi$.
\begin{align}
	P_{\phi}(\lambda)
		&= \prod_{i=1}^{d} P_{\phi}(\lambda_i) = \prod_{i=1}^{d} U(\phi_i^L, \phi_i^H) \\
	\mathcal H(P_{\phi})
		&= - \inv{d} \int P_{\phi}(\lambda) \ln P_{\phi}(\lambda) \mathrm{d} \lambda
		= \inv{d} \sum_{i=1}^d \ln \lr{ \phi_i^H - \phi_i^L }
\end{align}
where they define a normalized entropy $\mathcal H$ in nats/dimension. The pairs $(\phi_i^L, \phi_i^H)$ are referred to as \green{boundary values}. 

\begin{algorithm}[Automatic Domain Randomization]
	TL;DR: algorithm for updating parameter ranges $\phi_i$ in distribution $P_{\phi}$.\\
	 
	Init with \green{performance buffers} $\{ D_i^L, D_i^H \}_{i=1}^d$ (presumably empty at start), \green{thresholds} $m, t_L, t_H$ ($t_L < t_H$). Repeat the following until ``training is complete'':
	\begin{enumerate}
		\item Sample $\lambda \sim P_{\phi}$. 
		
		\item Randomly sample index $1 \le i \le d$, and number $x \sim U(0, 1)$. 
		
		\item If $x < 0.5$, set $D_i \eq D_i^L$, $\lambda_i \eq \phi_i^L$, else  $D_i \eq D_i^H$, $\lambda_i \eq \phi_i^H$. 
		
		\item Collect model performance $p$ on environment parameterized by $\lambda$. 
		
		\item $D_i \leftarrow D_i \cup \{ p  \}$. 
		
		\item If $\text{Length}(D_i) \ge m$:
		\begin{enumerate}
			\item $\bar p \leftarrow \textsc{Average}(D_i)$
			\item \textsc{CLEAR}($D_i$)
			\item if $\bar p \ge t_H$ increase $\phi_i$ by $\Delta$. If $\bar p \le t_L$, decrease $\phi_i$ by $\Delta$. (not sure if this means increase range scale, or shift range left/right (seems like the former))
		\end{enumerate}
	\end{enumerate}

	In English: we sample environment params $\lambda$ at each iteration, but then set a random dimension of $\lambda$ to a boundary value. We measure model performance and append to performance buffer index associated with the boundary value. After we have enough performance measurements for a given boundary value index $i$, we increase the associated pair $(\phi_i^L, \phi_i^H)$.
\end{algorithm}

To generate training data, they use the ADR algorithm above in conjunction with sampling from the [changing each time ADR is run] distribution $P_{\phi}$ and running the model (running the model results in new training data\footnote{\red{TODO}: how? how does running a model generate data? what is the data anyway?}). 
\myfig[0.6\textwidth]{figs/adr.png}


\red{TODO}: finish note









% ======================================================================
\lecture{Papers and Tutorials}{Fine-Tuning Language Models from Human Preferences}{January 18, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Ziegler et al., ``Fine-Tuning Language Models from Human Preferences'' \textit{OpenAI}, (Sep 2019).}


\bluesec{Methods} (2). Setup: vocab $\Sigma$, language model $\rho$. Want to model probability of response sequence $y$ given input sequence $x$.  They initialize a \green{policy} $\pi = \rho$, then fine-tune $\pi$ with RL. Instead of \textit{defining} a reward function $r: X \times Y \mapsto \R$, the \textit{learn} one using a dataset $S$ of human annotations. Each element of $S$ is a tuple $(x, y_0, y_1, y_2, y_3, b)$ -- each $y_i$ is a multiple-choice option presented to the human labeler, and the human selects option $0 \le b < 3$. They train the reward model $r$ with loss
\begin{align}
	\text{loss}(r)
		&= \E[x, \{y_i\}, b \sim S]{\log \frac{ e^{r(x, y_b)} }{ \sum_i e^{r(x, y_i)}   }}
\end{align}
They initialize $r$ as a random linear function of the final embedding output of $\rho$. They fine-tune $\pi$ to optimize $r$, performing RL on the modified reward:
\graybox{
	R(x, y) = r(x,  y) - \beta \log \frac{\pi(y \mid x)}{\rho(y \mid x)}
}

\red{how is this any different from just continuing training the LM on the human annotations?}






% ======================================================================
\lecture{Papers and Tutorials}{Deep Double Descent}{January 25, 2020}
% ======================================================================

\vspace{-1em}
{\footnotesize Nakkiran et al., ``Deep Double Descent: Where Bigger Models and More Data Hurt'' \textit{OpenAI}, (Dec 2019).}


\bluesec{TL;DR}: For a variety of DL tasks, increasing model size (or training epochs) first leads to \textit{worse} performance, and then gets better. Define a new complexity measure called \green{effective model complexity}. 
\vspace{-0.5em}
\begin{quote}
	{\small\itshape
		Informally, our intuition is that for model-sizes at the interpolation threshold, there is effectively only one model that fits the train data and this interpolating model is very sensitive to noise in the train set and/or model mis-specification. That is, since the model is just barely able to fit the train data, forcing it to fit even slightly-noisy or mis-specified labels will destroy its global structure, and result in high test error.
	}
\end{quote}

\bluesec{Introduction} (1). Bias-variance tradeoff suggests that increasing model complexity results in lower bias and higher variance. After a certain threshold [of increasing model complexity], conventional wisdom says that the model will ``overfit'' with the variance term dominating the test error. Therefore, increasing the complexity beyond this threshould \textit{should} merely result in a larger variance term and thus worse MSE (i.e. \textit{larger models are worse} [after a certain point]). Conventioanal wisdom also tells us that \textit{more data is always better} [for improving the test MSE]. 

\bluesec{Results} (2). Define a \green{training procedure} $\mathcal T$ to be any procedure that takes as input a training set $S = \{(x_i, y_i)\}_{i=1}^{n}$ and outputs a classifier $\mathcal T(S) : x \mapsto y$. 

\begin{definition}[-1em][Effective Model Complexity]
	The \green{EMC} of $\mathcal T$, wrt distribution $\mathcal D$ and $\epsilon > 0$, is defined as:
	\begin{align}
		\mathrm{EMC}_{\mathcal D, \epsilon}(\mathcal T) 
			&:= \max\{   n \mid \E[S \sim \mathcal{D}^n]{\mathrm{Error}_S(\mathcal T (S))  } \le \epsilon \}
	\end{align}
	where $\mathrm{Error}_S(M)$ is the mean error of model $M$ on train samples $S$.
\end{definition}
In other words, the EMC of $\mathcal T$ is the maximum number of training samples for which $\mathcal T$ gets [on average] zero \textit{training error}. The authors then hypothesize the 3 following regimes (assuming $n$ training examples):

\newcommand{\emc}{\mathrm{EMC}_{\mathcal D, \epsilon}(\mathcal T)}
\begin{compactitem}
	\item \textbf{Under-parameterized}: $\emc << n$. Any $\delta \mathcal T$ resulting in larger $\emc$ will \textit{decrease} test error.
	
	\item \textbf{Over-parameterized}: $\emc >> n$. (same as above).
	
	\item \textbf{Critically parameterized}: $\emc \approx n$. Any $\delta \mathcal T$ resulting in larger $\emc$ may \textit{decrease or increase} test error.
\end{compactitem}











% ========================= SECTIONSWITCH ========================= 

\end{document}








