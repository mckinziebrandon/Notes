% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% P  R  E  A  M  B  L  E
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
\documentclass[11pt]{article}
\usepackage{amsbsy, amsmath, amssymb, authblk}

%\usepackage{array} 
%\usepackage{algorithm2e}

\usepackage{booktabs, bm}
\usepackage[small,labelfont=bf,up,singlelinecheck=false]{caption}
\usepackage{cancel}
\usepackage{comment}
%\usepackage{fancyhdr}
%\usepackage[default]{lato}
\usepackage[T1]{fontenc}
\usepackage[bottom]{footmisc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage[utf8]{inputenc}
%	\inputencoding{latin1}
%	\inputencoding{utf8}
%\usepackage{lettrine}
%\usepackage[sc]{mathpazo}
\usepackage{lmodern} % Nice fonts?
%\usepackage{mathrsfs}
\usepackage{mathtools} 
%\usepackage{marvosym} % silly bullet-point symbols (misc symbols)
%\usepackage{microtype}
\usepackage{minitoc}         % left in case it is needed elsewhere
\setcounter{secttocdepth}{5} % idem
\usepackage{etoc} % for toc before each section.
%\usepackage{multicol}
\usepackage{needspace}
\usepackage{paralist}
\usepackage{pifont}
%\usepackage{polynom} 			% typesetting polynomial long division
%\usepackage{setspace}
%	\onehalfspacing 
\usepackage{tocloft}
\usepackage{xparse} % DeclareDocumentCommand
\usepackage[compact]{titlesec} 		% compact shrinks whitespace around section headings.
\usepackage{ulem} 				% for strikeout \sout command.
%\usepackage{verbatim}

% Muh packagez :)
\usepackage{../../Packages/MathCommands}
\usepackage{../../Packages/BrandonColors}
\usepackage{../../Packages/BrandonBoxes}
\usepackage{../../Packages/NoteTaker}
\usepackage{../../Packages/CS221}

%\usepackage{program}
% DL BOOK CONVENTIONS
\renewcommand\vec[2][]{\bm{#2}_{#1}}

\DeclareDocumentCommand{\slice}
	{ O{t} O{1} m }
	{\vec[\langle #2 \ldots #1 \rangle]{#3}}

\newcommand\myfig[2][0.3\textwidth]{\begin{figure}[h!]\centering\includegraphics[width=#1]{#2}\end{figure}}
\newcommand\myspace[1][]{\vspace{#1\bigskipamount}\Needspace{10\baselineskip}}
\newcommand\p{\Needspace{10\baselineskip} \noindent}
\newcommand\tlab[1]{\tag{#1}\label{#1}}
\newcommand\Var[1]{\mathrm{Var}\left[#1\right]}

\usepackage{layout} % Type \layout() anywhere to see values of layout frame.
%\usepackage{showframe} % Displays layout frame on all pages
\usepackage{marginnote}
\renewcommand*{\marginfont}{\scriptsize}

\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\usepackage{tikz}
\usetikzlibrary{arrows, automata, shapes, snakes, positioning}
% \usetikzlibrary{bayesnet}


\titleformat*{\subsubsection}{\small\scshape}
\newcommand\subsub[1]{\Needspace{15\baselineskip}\hrule\subsubsection{#1}\hrule}
\newcommand\matgrad[2]{\nabla_{\mathbf{#2}} #1}

\newcommand\dotseq[2]{#1^{(1)}, \ldots, #1^{(#2)}}
\newcommand\rdotseq[2]{#1^{(#2)}, \ldots, #1^{(1)}} % reversed

% O{T} means "optional with default value of `T`"
% m means mandatory argument
\DeclareDocumentCommand{\vecseq}
	{ O{T} m }
	{ \{  \vec[1]{#2}, \ldots, \vec[#1]{#2}   \}  }
\DeclareDocumentCommand{\seq}
	{ O{T} m }
	{ \{ #2_1, \ldots #2_#1 \} }
	
\newcommand\QA[2]{\item \red{Q}: #1
	\begin{compactitem}
		\item \green{A}: #2
	\end{compactitem}}

%\setlength{\parskip}{1pt}
%\setlength{\columnseprule}{0.1pt}
%\setlength{\columnsep}{0.6cm}
%\setlength\tabcolsep{0.1cm}
\renewcommand{\arraystretch}{1.2}


\makeatletter
\newcommand*\dotp{\mathpalette\dotp@{.5}}
\newcommand*\dotp@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

% --------------------------------------------------------------
% --------------------------------------------------------------

% Make all the ToC section/subsections small/condensed.
\renewcommand\cftsecfont{\small\bfseries}
\renewcommand\cftsubsecfont{\scriptsize}
\renewcommand\cftsubsubsecfont{\scriptsize}
\renewcommand\cftsecafterpnum{\vskip-5pt}
\renewcommand\cftsubsecafterpnum{\vskip-7pt}
\renewcommand\cftsubsubsecafterpnum{\vskip-7pt}

% bluesec
\newcommand\bluesec[1]{\myspace \p \blue{#1}}

\begin{document}
\dosecttoc
\tableofcontents




% ==================================================================================
% Lectures
% ==================================================================================
\mysection{Lectures}\label{Lectures}

% ------------------------------------------------------------------------------
\lecture{Lectures}{Machine Learning II}{April 09, 2019}

\p \blue{Roadmap}. 
\begin{compactitem}
	\item Features. 
	\item Neural Networks.
	\item Gradients without tears.
	\item Nearest Neighbors.
\end{compactitem}

\p \blue{Linear Classifiers}. Discussion of feature maps $\phi(x)$ that can be used for making a non-linear decision boundary \tstamp{52:00}.  

\myspace
\p \blue{Neural Networks}. 

\begin{example}[Predicting Car Collision \tstamp{58:00}]
Input: position of two oncoming cars $x = [x_1, x_2]$. 
 
Output: Whether safe ($y = +1$) or collide $y = -1$. 

We are told that the true function is
$$
y^* = \text{sign}(|x_1 - x_2| - 1)
$$
\tcblower

\begin{compactitem}

\item \textbf{Insight}: In $(x_1, x_2)$ space, there is a ``band'' (diagonal to the right centered about origin) representing ``safe'' \tstamp{58:30}. Recognize that these can be thought of as two linear decision boundaries.  

\item \textbf{Decompose} into subproblems\footnote{He is defining $\sign{0} \equiv -1$.}.
\begin{align}
	h_1 &= \ind{x_1 - x_2 \ge 1} \\
	h_2 &= \ind{x_2 - x_1 \ge 1} \\
	y &= \sign{h_1 + h_2}
\end{align}

\item \textbf{Learning strategy}. Define $\vec\phi(\vec x) \triangleq [1, x_1, x_2]$.
\begin{align}
	h_1
		&= \ind{\vec[1]{v} \dotp \vec\phi(\vec x) \ge 0  } 
		\qquad \vec[1]{v} \triangleq [ -1, +1, -1  ]   \\
	h_2	
		&= \ind{\vec[2]{v}  \dotp \vec\phi(\vec x) \ge 0 }
		\qquad
		\vec[2]{v} \triangleq [-1, -1, +1] \\
	f_{\matr V, \vec w}(\vec x)
		&= \sign{\vec w \dotp \vec h}
	\qquad
	\vec w \triangleq [1, 1]
\end{align}

\end{compactitem}

\red{Key idea: joint learning} \tstamp{1:03:42}. Learn both hidden subproblems $\matr V = (\vec[1]{v}, \vec[2]{v})$ and combination weights $\vec w = [w_1, w_2]$. \\
\end{example}

\myspace
\p \blue{Gradients}. (Continuing off example) Our parameters are $\{\vec[1]{v}, \vec[2]{v}, \vec w\}$. To learn these parameters, we may consider gradient descent on $y$ (and therefore on $h_1$ and $h_2$). Notice that
\begin{align}
\nabla_{\vec[1]{v}} h_1 = \vec 0 
\end{align}
Solution: Redefine $h_1 \triangleq \sigma\left( \vec[1]{v} \dotp \vec\phi(\vec x) \right)$. 

\myfig[0.3\textwidth]{figs/ml2_nn.png}


\myspace
\p \blue{Gradients without tears} \tstamp{1:14:00}. Gradients of common function building blocks:

\myfig[0.3\textwidth]{figs/ml2_building_blocks.png}


\myspace
\p \blue{Binary classification with hinge loss} \tstamp{1:17:01}. 
\begin{align}
L(x, y, w) 
&= \max\left\{
1 - \vec w \dotp \vec\phi(\vec x) y, 0
\right\}
\end{align}

\myfig[0.5\textwidth]{figs/ml2_backprop.png}



% ------------------------------------------------------------------------------
\lecture{Lectures}{Machine Learning III}{April 11, 2019}

\p \blue{Review}. Some feature extractor $\vec\phi(\vec x)$.
\begin{compactitem}
	\item Linear predictor score: $\vec w \dotp \vec\phi(\vec x)$. 
	\item Neural network score: $\sum_{j=1}^k w_j \sigma\left(  \vec[j]{v} \dotp \phi(x) \right)$. 
\end{compactitem}

\myspace
\p \blue{Generalization}. Goal is to minimize error on unseen future examples. 
\begin{compactitem}
	\item Suppose there is some space of all possible predictors $f$ and, within this space, there exists the optimal predictor $f^*$. 
	
	\item When we decide how we will do our feature extraction, we constrain the possible predictors we can get to be inside some \green{hypothesis class} $\mathcal F$ \tstamp{21:11}. Note that there exists a predictor $g$ that is optimal, constrained to being inside $\mathcal F$.  
	
	\item \green{Approximation Error}: How good is our hypothesis class $\mathcal F$? 
	
	\item \green{Estimation Error}: How good is our learned predictor $\hat f$ \textit{relative to} our hypothesis class $\mathcal F$?
	\graybox{
		\text{Err}(\hat f) - \text{Err}(f^*)
		&= \underbrace{ \text{Err}(\hat f) - \text{Err}(g) }_\text{Estimation} 
			+
			\underbrace{ \text{Err}(g) - \text{Err}(f^*) }_\text{Approximation}
	}
\end{compactitem}
\myfig[0.4\textwidth]{figs/ml3_err.png}

For linear predictors with weights $\vec w \in \R^d$, we can \textbf{control the size of $\mathcal F$} by \textellipsis
\begin{compactitem}
	\item Keeping dimensionality $d$ small.
	\begin{compactitem}
		\item Add/remove features.
		\item Forward selection.
		\item Boosting.
		\item $L_1$ regularization.
	\end{compactitem}
	\item Keeping the norm $|| \vec w ||$ small.
\end{compactitem}
Note that \green{SVMs} = hinge loss + regularization.\\

\p How do we choose \blue{hyperparameters} \tstamp{43:39}? One way is to use a \green{validation set}. 

\myspace 
\p \blue{Unsupervised Learning} \tstamp{1:06:00}. Data has rich \green{latent structures} that we'd like to learn automatically. Two types of unupervised learning: \green{clustering} (e.g. K-means) and \green{dimensionality reduction} (e.g. PCA). 

\begin{definition}{Clustering}
	\textbf{Input}: Training set of inputs $\mathcal{D}_{train} = \{ x_1, \ldots, x_n \}$.
	
	\textbf{Output}: Assignment of each point to a cluster $[ z_1 \ldots, z_n ]$ where $z_i \in \{1, \ldots, K\}$. 
\end{definition} 

\begin{algorithm}[K-Means Clustering \tstamp{1:10:35} ]
	\textbf{Setup}: Each cluster $k = 1, \ldots, K  $ is represented by a \green{centroid} $\vec[k]{\mu} \in \R^d$. Want each $\phi(x_i)$ close to its assigned centroid $\vec[z_i]{\mu}$. 
	
	\textbf{Objective}: MSE. 
	\begin{align}
		L(z, \mu)
			&= \insum || \phi(x_i) - \vec[z_i]{\mu} ||^2
	\end{align}
	
	\textbf{Algorithm}:
	\begin{compactenum}
		\item For each point $x_i$, assign:
		\begin{align}
			z_i := \argmin_{k=1,\ldots,K} ||\phi(x_i) - \mu_k||^2
		\end{align}
		
		\item For each cluster $k = 1, \ldots, K$, assign:
		\begin{align}
			\vec[k]{\mu} := \inv{| \{ i: z_i{=}k  \}   |}  \sum_{i: z_i{=}k} \phi(x_i)
		\end{align}
	\end{compactenum}
\end{algorithm}




% ------------------------------------------------------------------------------
\lecture{Lectures}{Search I}{April 16, 2019}

Running example: farmer has a boat. Needs to get a cabbage, goat, and wolf across river. Goat and cabbage cannot be alone. Wolf and goat cannot be alone. Boat can hold at most two things (including farmer). How he do it tho?

\p \blue{Beyond Reflex} \tstamp{9:00}. Classifiers (reflex) map $x$ to single action $y$. In \green{search}, we map $x$ to an \textit{action sequence} $(a_1, a_2, \ldots)$. \textbf{Key}: Need to consider consequences of future actions. 

\myspace
\p \blue{Tree search}. Root note is start state. Edges are actions. Nodes are states resulting from said action. Branches represent a sequence of actions. 
\begin{compactitem}
	\item $S_{start}$ is "FCGW|" (farmer, cabbage, goat, wolf all on left of river). 
	\item Actions: farmer can go to/from river. Actions enumerate what he takes with him and what direction he goes. 
	\item Cost(s, a): ?
	\item Succ(s, a): returns the state resulting from taking action $a$ from state $s$. 
	\item IsEnd(s): ?
\end{compactitem}

\begin{example}[Transportation example \tstamp{17:40}]
	\textit{Streets with blocks numbered $1$ to $n$. Walking from $s$ to $s+1$ takes 1 minute. Taking a magic tram from $s$ to $2s$ takes 2 minutes. How to travel from $1$ to $n$ in the least time?}
	
	\tcblower 
	
	You should read this and then try formalizing it as a search problem.
	\begin{compactitem}
		\item Actions: either take 1 step forward or take the tram.
		\item The costs are 1 minute and 2 minutes for the aforementioned actions.
	\end{compactitem}


	She starts coding it up at \tstamp{19:30}. Takeaways from coding:
	\begin{compactitem}
		\item Separate modeling from inference. 
		\item Modeling: She defines a class with methods startState, isEnd(state), succAndCost(state). 
	\end{compactitem}
\end{example}

\myspace
\p \blue{Backtracking search} \tstamp{23:10}. Also uses a search tree. Define branching factor $b$ and depth $D$ of tree\footnote{Distinguishing between uppercase $D$ and lowercase $d$ will be important in this lecture.}. BS goes down all the way leftmost to leaf. Sounds like \green{depth-first search} tbh\footnote{At \tstamp{34:00} she address this. Yes, it is basically DFS, but DFS will \textit{stop} as soon as it hits a leaf (a solution). Also, DFS implicitly defines all costs as zero.}
\begin{compactitem}
	\item Memory: $\mathcal O(D)$ (small). Why? because we only need to remember our past history, which for BS is basically a straight line from leaf back up to root. 
	
	\item Time: $\mathcal O(b^D)$ (huge). In other words, all nodes. Why? because worst-case is that it has to visit every single node. 
\end{compactitem}

\myspace\Needspace{10\baselineskip}
She starts coding for BS at \tstamp{27:28}. Note that she is able to reuse her code from the previous part, where she just defined the \textit{model}. Now, she implements BS which is the \textit{inference}. Below is the BS \textit{algorithm}\footnote{Which is slightly different in form than her code.}:
\begin{lstlisting}[language=Python]
def backtracking_search(s, path):
	if is_end(s):
		minimum_cost_path = min(minimum_cost_path, path)
	
	for a in actions(s):
		path.append([succ(s, a), cost(s, a)])
		backtracking_search(Succ(s, a), path)
	
	return minimum_cost_path
\end{lstlisting}

\myspace
\p \blue{Breadth-first Search} \tstamp{37:00}. Requires that cost for all actions is the same constant $c \ge 0$. Explores level-by-level. Like DFS, terminates as soon as it finds a solution\footnote{This is a consequence of the fixed global cost $c \ge 0$.}. Important: let $d$ (small d) denote the depth of the \textit{solution}. 
\begin{compactitem}
	\item Memory: $\mathcal O(b^d)$ (a lot worse!).
	\item Time: $\mathcal O(b^d)$ (better, depends on $d$, not $D$).
\end{compactitem}

Brief overview of \green{DFS with iterative deepening} at \tstamp{41:00}. Hybrid of DFS with BFS. Call DFS at max depths 1, 2, $\ldots$ etc. $\mathcal O(d)$ space and $\mathcal O(b^d)$ time. Nice summary slide of tree search algorithms at \tstamp{43:11}. 

\myspace 
\p \blue{Dynamic programming} \tstamp{43:30}. Observation: the cost of going from $s$ to end state can be decomposed into cost of $(s, a) \rightarrow s'$ plus the cost of going from $s'$ to end state.
\begin{compactitem}
	\item Denote ``future cost'' of state $s$ as $F(s)$. 
	\begin{align}
		F(s) \triangleq 
		\begin{cases}
			0 & IsEnd(s) \\
			\min_{a \in Actions(s)} C(s, a) + F(Succ(s, a)) & \text{otherwise} 
		\end{cases}
	\end{align}
	
	\item \textbf{Key observation}: \textit{we can cache intermediate results}. 
	
	\item \textbf{Assumption}: State graph defined by $Actions(s)$ and $Succ(s, a)$ is acyclic. 
	
	\item State should be sufficient to encode full history, so that futureCost(state) can be computed. 
\end{compactitem}

\myspace\Needspace{15\baselineskip}
\begin{example}[Route finding \tstamp{47:38}]
	\textit{Find minimum cost path from city $1$ to $n$, only moving forward. It costs $c_{ij}$ to go from $i$ to $j$.}
	\tcblower 
	Clarification: Yeah, you can go from any $i$ to $i + j$ for $0 \le j \le n - i$. 
	\textbf{Observation}: future cost only depends on the current city (since we can only move forward). Cache, cache, cache.\\
	
	What if we add the \textbf{constraint} that you can't visit three odd cities in a row. How does this change our definition of a state? Well, now we need to encode history somehow. 
	\begin{compactitem}
	\item Naive: redefine our state as (prev city, current city). This increases our state space size from $n$ to $n^2$. 
	
	\item More compact: redefine our state as (prevWasOdd, current city). Now our state space is $2n$. 
	\end{compactitem}
\end{example}

\myspace\Needspace{10\baselineskip}
Coding for dynamic programming:
\begin{lstlisting}[language=Python]
def dynamic_programming(problem):
	cache = {}  # state -> future_cost(state)
	def future_cost(state):
		if problem.is_end(state):
			return 0
		if state in cache:
			return cache[state]
		result = min(cost + future_cost(new_state) for _, new_state, cost in problem.succ_and_cost(state))
		cache[state] = result
		return result
	return future_cost(problem.start_state(), [])
\end{lstlisting}


\myspace
\p \blue{Uniform cost search} \tstamp{1:05:00}. Can handle cycles. 
\begin{compactitem}
	\item \textbf{Assumption}: $\forall (s, a): C(s, a) \ge 0$. 
	
	\item \textbf{Observation}: prefixes of optimal path are optimal. 
	
	\item \textbf{Key idea}: Enumerate states in order of increasing past cost. 
\end{compactitem}


\begin{center}
	\begin{tikzpicture}[font=\sffamily,node distance=1.cm,->,>=latex,auto,line width=0.4mm]
	
	
	\tikzset{node st/.style={state, draw=none,
			fill=gray!30!white,
			text=blue!60!white}}
	\tikzset{node obs/.style={state, draw=none,
			fill=gray!60!black,
			text=blue!20!white}}
	
	\node[node st] (A) {B};
	\node[below=3em of A] (dummy) {};
	\node[node st, left=5em of dummy] (D) {A};
	\node[node st, right=5em of dummy] (B) {D};  
	\node[node st, below=6em of A] (C) {C};
	
	\draw[every loop,auto=right,draw=blue!30!gray]
	(A) edge[-] node{100} (B)
	(A) edge[-] node{1} (D)
	(A) edge[-] node{1} (C)
	(B) edge[-] node{1} (C)
	(D) edge[-] node{100} (C);
	\end{tikzpicture}
\end{center}
Goal: Use UCS to get from A to D. 
\begin{compactitem}
	\item Maintain 3 sets of states: explored, frontier, and unexplored.
	\item We start with all states in the unexplored sets. 
	\item Pop A into the frontier. What states can we reach from A? 
	\item Pop B and C into frontier and the cost to get to them. A is now in the explored set. 
\end{compactitem}
It seems we don't explore states we've already been to? UCS is \textit{correct}: when any state $s$ is moved from frontier to explored, its priority of $PastCost(s)$ is the minimum cost to $s$ (discussion around \tstamp{1:18:00}). 

\myspace
Coding for UCS begins at \tstamp{1:15:00}. 

\begin{lstlisting}[language=Python]
def uniform_cost_search(problem):
	frontier = PriorityQueue()
	frontier.update(problem.start_state(), 0)
	
	while True:
	# Move from frontier to explored.
	state, past_cost = frontier.remove_min()
	if problem.is_end(state):
		return past_cost, []
	# Push out onto frontier.
	for a, new_state, cost in problem.succ_and_cost(state):
		frontier.update(new_state, past_cost + cost)
\end{lstlisting}

\myfig[0.4\textwidth]{figs/search_ucs.png}





% ------------------------------------------------------------------------------
\lecture{Lectures}{Search II}{April 18, 2019}

\p Discussion of \textbf{Final Project} up until \tstamp{15:00}. 

\myspace
\p \blue{Review}. \textbf{Key idea}: a \green{state} is a summary of all the past actions sufficient to choose future actions \textbf{optimally}. The class paradigm includes modeling, inference, and learning. Today will talk more about learning. 

\myspace
\p \blue{Learning costs} \tstamp{25:00}. What if we don't have access to the costs? Goal: develop a learning algorithm that outputs the costs of actions.  
\graybox{
	\text{\footnotesize Forward problem \textbf{(Search)}} \quad 	
		\mblue{Cost(s, a)} &\longrightarrow \mred{(a_1, \ldots, a_k)} \\
	\text{\footnotesize Inverse problem \textbf{(Learning)}} \quad
		\mred{(a_1, \ldots, a_k)} &\longrightarrow \mblue{Cost(s, a)}
}

Prediction (inference) problem:
\begin{compactitem}
	\item Inputs (x): search problem without costs.
	\item Outputs (y): sequence of optimal actions.
\end{compactitem}
So our training data is search problem inputs and the labeled solutions (action sequence) for each. We need to learn that mapping from search problem to optimal action sequence. The idea is that we'll learn the costs by doing this. A simple way of modeling this\footnote{Note that $w$ does not care about the state we are taking the action from.}:
\begin{align}
	Cost(s, a_i) &\triangleq w[a_i] \triangleq w_i 
\end{align}

\begin{algorithm}[Structured Perceptron (simplified) \tstamp{32:00}]
	\begin{compactenum}
		\item For each action $a$, initialize $w[a] \leftarrow 0$. 
		
		\item For each iteration $t = 1, \ldots, T$:
		\begin{compactenum}
			\item For each $(x, y) \in \mathcal D_{train}$:
			\begin{compactenum}
				\item Compute minimum cost path $y'$ given $\vec w$. 
				\item $\forall a \in y: w[a] \leftarrow w[a] - 1$. 
				\item $\forall a \in y': w[a] \leftarrow w[a] + 1$. 
			\end{compactenum}
		\end{compactenum}
	\end{compactenum}
\end{algorithm}
Coding for this starts at \tstamp{38:00}. It just uses a modified version of TransportationProblem (model) that incorporates weights, and uses the dynamicProgramming function (inference) unchanged. Then it updates weights as defined above. \\

\p \blue{Generalization to features} \tstamp{46:10}. Costs are parameterized by [incorporating] a feature vector:
\begin{align}
	Cost(s, a) &= \vec w \dotp \phi(s, a) \\
	Cost(y) &= \vec w \dotp \sum_{(s, a) \in y} \phi(s, a)
\end{align} 

\myspace
\p \blue{A* search} \tstamp{46:30}. Improving upon UCS:
\begin{compactitem}
	\item \textbf{Problem}: UCS orders states by cost from $s_{start}$ to $s$. 
	\item \textbf{Goal}: take into account cost from $s$ to $s_{end}$. 
\end{compactitem}
Since we obviously don't know $FutureCost(s)$, A* will explore in order $PastCost(s) + \mred{h(s)}$, using a \green{heuristic function} that estimates future cost. 
\graybox{
	\underbrace{Cost'(s, a)  }_{\text{A*}}
		&= \underbrace{ Cost(s, a) }_{\text{UCS}} 
			+ h\left(  Succ\left(   s, a  \right)  \right) - h(s)
}
\begin{compactitem}
	\item \textbf{Consistency of $h$}. A heuristic $h$ is \green{consistent} if both of the following hold:
	\begin{align}
		Cost'(s, a) &\ge 0 \\
		h(s_{end}) &= 0
	\end{align}
	\myfig[0.3\textwidth]{figs/search2_consistency.png}
	
	\begin{definition}[0em]{Proposition: Correctness of A* \tstamp{56:40}}
		If $h$ is consistent, A* returns the minimum cost path\footnote{Proof hint: show that A* ultimately ends up returning UCS + a constant after you sum everything up.}.
	\end{definition}

	
	\item \textbf{Admissibility of $h$} \tstamp{1:02:20}. A heuristic $h$ is admissible if
	\begin{align}
		h(s) \le FutureCost(s)
	\end{align} 
	Intuition: admissible heuristics are \textit{optimistic}. Theorem: $isConsistent(h) \Rightarrow isAdmissible(h)$. 
\end{compactitem}

\myspace 
\p \blue{Efficiency}.
\begin{compactitem}
	\item UCS efficiency: UCS explores all states $s$ satisfying $$PastCost(s) \le PastCost(s_{end})$$. 
	
	\item \red{A* efficiency: A* explores all states $s$ satisfying $$PastCost(s) \le PastCost(s_{end}) - h(s)$$.}\footnote{IDGI. Plz explain here when smarter.}
\end{compactitem}

\myfig[0.6\textwidth]{figs/search2_astar_explore.png}

\Needspace{10\baselineskip}
\begin{definition}{Key Idea: \green{distortion} \tstamp{1:02:00}}
	\red{A* distorts edge costs to favor end states.}
\end{definition} 



\myspace
\p \blue{Relaxation} \tstamp{1:04:00}. How we find heuristics. Basically, removing constraints. 













% ------------------------------------------------------------------------------
\lecture{Lectures}{Markov Decision Processes}{April 23, 2019}

\p \blue{Motivation}. (Building off search problems) There is \textbf{uncertainty} in the world. Volcano crossing example starts at \tstamp{7:20}. 

\begin{example}[Dice Game \tstamp{13:12}]
	For each round $r = 1, 2, \ldots$
	\begin{compactitem}
		\item You choose to \green{stay} or \red{quit}. 
		\item If \red{quit}, you get \$10 and end the game.
		\item If \green{stay}, get \$4 and then roll a 6-sided dice. 
		\begin{compactitem}
			\item If result is 1 or 2 $\rightarrow$ end game.
			\item Otherwise, continue to next round.
		\end{compactitem}
	\end{compactitem}

	\tcblower

	Train of thought should roughly go as follows:
	\begin{compactitem}
		\item If I follow policy \green{stay}, I can compute my expected reward as
		\begin{align}
			\inv{3} (4) + \frac{2}{3}\inv{3} (8) + \frac{2}{3} \frac{2}{3} \inv{3} (12) + \ldots = 12
		\end{align}
		where the $\inv{3}$ represent the probability that my roll ends the game. 
	\end{compactitem}
	\vspace{1em}
	
	Using MDP lingo, we revisit this at \tstamp{44:30}. We have $\text{States} = \{in , end\}$ and $\text{Actions} = \{stay, quit\}$. Our policy is the simple $\pi(in) := stay$. We can compute our expected reward (assuming $\gamma =1$ discount) as:
	\begin{align}
		V_\pi(in) &= \inv{3}(4 + \cancel{ V_\pi(end)  } )  + \frac{2}{3} (4 + V_\pi(in)) = 12
	\end{align}
	
\end{example}



\myspace
\blue{Markov Decision Processes}. We can represent an MDP as a graph containing both state nodes and \textit{chance} nodes. Edges from state nodes to chance nodes represent an action. Edges from chance nodes to state nodes represent the random outcomes of the given action.
\myfig[0.4\textwidth]{figs/mdp_def.png}

\begin{compactitem}
	\item $\forall (s, a): \sum_{s' \in \text{States}} T(s, a, s') = 1$
	\item Successors are defined as all $s'$ s.t. $T(s, a, s') > 0$. 
\end{compactitem}
Revisit transportation example (magic tram) at \tstamp{22:00}, with addition that tram fails with $p = 0.5$.  A \textit{solution} to an MDP is a \green{policy} $\pi$: a mapping from each state $s \in \text{States}$ to an action $a \in \text{Actions}(s)$. 

\myspace
\p \blue{Policy Evaluation} \tstamp{30:50}. Following a policy yields a \textit{random path}. Policy terminology:
\begin{compactitem}
	\item  \green{Utility} $U$ is discounted $\sum R(s, a, s')$ on the path (this is a random quantity).
	
	\item \green{Value} $V_{\pi}(s)$ of policy $\pi$ from state $s$ is the $\E{U}$ received by following policy $\pi$.  
	
	\item \green{Q-value} $Q_\pi(s, a)$  is $\E{U}$ of taking action $a$ from state $s$, and then following policy $\pi$. 
\end{compactitem}
We can compute these quantities via:
\graybox{
	V_{\pi}(s)
		&= \begin{cases}
			0 & \text{if IsEnd(s)} \\
			Q_{\pi}(s, \pi(s)) & \text{otherwise}
		\end{cases} \\
	Q_{\pi}(s, a)
		&= \sum_{s'} T(s, a, s')\left[
			R(s, a, s') + \gamma V_\pi(s')
		\right] 
}



\begin{algorithm}[Policy Evaluation \tstamp{45:00}]
	\textbf{Goal}: automatically compute value of any policy $\pi$. 
	
	\begin{compactenum}
		\item Initialize $\V[0]{s} \leftarrow 0$ for all states $s$. 
		
		\item For each iteration $t = 1, \ldots, t_{PE}$ and for each state $s$:
		\graybox{
			V_\pi^{(t)}(s)
				&\leftarrow  \underbrace{ \sum_{s'} T(s, \pi(s), s') \left[ 
					R(s, \pi(s), s') + \gamma V_\pi^{(t-1)}(s')
				\right] }_{Q^{(t-1)}(s, \pi(s)) }
		}
	\end{compactenum}
	where it's important to note that we use the values $\V[t - 1](s')$ from the previous iteration when computing the values $\V{s}$ for the current iteration. We are iteratively updating our estimates of $V$. 
	

	We want to choose $t_{PE}$ such that 
	\begin{align}
		\max_{s \in \text{States}} \left|
			\V{s} - \V[t -1]{s}
		\right| \le \epsilon 
	\end{align}
	
\end{algorithm}

\textbf{Complexity} of policy evaluation:
\begin{compactitem}
	\item Time: $\mathcal O (t_{PE} S S' )$
\end{compactitem}

\myspace 
\blue{Value iteration} \tstamp{51:05}. Given an MDP, how do we find a good policy $\pi$?


\begin{algorithm}[Value Iteration \tstamp{57:00}]
	\begin{compactenum}
		\item Initialize $\V[0]{s} \leftarrow 0$ for all states $s$. 
		
		\item For each iteration $t = 1, \ldots, t_{VI}$ and for each state $s$:
		\graybox{
			V_{opt}^{(t)}(s)
			&\leftarrow  
			\mred{\max_{a \in \text{Actions(s)}   } }
			\underbrace{ \sum_{s'} T(s, a, s') \left[ 
				R(s, a, s') + \gamma V_{opt}^{(t-1)}(s')
				\right] }_{Q_{opt}^{(t-1)}(s, a) }
		}
	\end{compactenum}
	
	We want to choose $t_{PE}$ such that 
	\begin{align}
	\max_{s \in \text{States}} \left|
	\V{s} - \V[t -1]{s}
	\right| \le \epsilon 
	\end{align}
\end{algorithm}

\textbf{Complexity} of value iteration:
\begin{compactitem}
	\item Time: $\mathcal O (t_{VI} S A S' )$
\end{compactitem}
Coding of VI starts around \tstamp{1:00:00}. Convergence discussion around \tstamp{1:08:00}. 







% ------------------------------------------------------------------------------
\lecture{Lectures}{Reinforcement Learning}{April 25, 2019}

\begin{table}{c | c}
		MDPs  & RL \\ \midrule \midrule 
	Mental model of world & Don't know how world works \\ \midrule
	Find policy to collect max R & Perform actions in world to find out and collect R \\
\end{table}


\myspace
\blue{Monte Carlo Methods} \tstamp{18:53}. Our data consists of paths of the form $s_0; a_1, r_1, s_1; a_2, r_2, s_2; \ldots$, which in RL we also call an \green{episode}\footnote{Defined as beginning with a start state, and ending with an end state.}.

\begin{definition}{Model-Based [Monte Carlo]  Learning}
	Estimate the MDP \underline{parameters} $T(s, a, s')$ and $R(s, a, s')$. 
	\begin{align}
		\hat T(s, a, s')
			&= \frac{ \text{Count}(s \rightarrow a \rightarrow s')   }{ \text{Count}(s \rightarrow a)    } \\
		\hat R (s, a, s')
			&= r \text{ in } (s, a, r, s')
	\end{align}
	We can use these parameter estimates to compute an estimate for $\hat Q_{opt}(s, a)$. 
\end{definition}
Problem: might not see all of the state/action space. Solution: Need $\pi$ to \textit{explore} explicitly. Next, consider that all we really need for prediction is (estimate of) $Q_{opt}(s, a)$:
\begin{align}
	\hat Q_{opt}(s, a) 
		&= \sum_{s'} \hat T(s, a, s')\left[
			\hat R(s, a, s') + \gamma \hat V_{opt}(s')
		\right]
\end{align}
\begin{definition}{Model-Free [Monte Carlo] Learning}
	Estimate $Q_{opt}(s, a)$ directly (without learning parameters). Recall that we define the utility at time $t$ as
	\begin{align}
		u_t \triangleq r_t + \gamma \cdot r_{t + 1} + \gamma^2 \cdot r_{t+2} + \cdots 
	\end{align}
	Collect all episodes in our data for which there exists transition\footnote{And $s, a$ doesn't occur in $s_0, \ldots, s_{t-2}$ (i.e. only care about first occurrence).} $s_{t-1}{=}s ~\longrightarrow ~ a_t{=}a$. Then\footnote{Assume for now that we've decided on using some policy $\pi$.} $\hat Q_{\pi}(s, a) := \text{average}(\{u_t\})$ for those episodes. 
\end{definition}

Model-Free Monte Carlo is an example of an \green{on-policy} algorithm\marginnote{On-Policy vs Off-Policy}: estimating the value of data-generating policy. There are also \green{off-policy} algorithms, ones that estimate the value of a different policy than seen in the data.

\myspace
\blue {Model-Free Equivalences} \tstamp{39:40}. 
\begin{compactitem}
	\item \textbf{Original}:$ \hat Q_{\pi}(s, a) := \text{average}(\{u_t\})$
	
	\item \textbf{Convex combination}: At each occurrence of $u_t$, we assign $\hat Q_{\pi}(s, a)$ to the convex combination of itself (our current estimate) and $u_t$:
	\begin{align}
		\hat Q_{\pi}(s, a)
			&\leftarrow (1 - \eta) \hat Q_{\pi}(s, a) + \eta u \\
		\text{where}\quad 
		\eta
			&:= \inv{1 + \text{NumUpdates}(s, a)} 
	\end{align}
	Where "NumUpdates" for a given $(s, a)$ is the number of times we've updated our estimate of $\hat Q_{\pi}(s, a)$ so far\footnote{So e.g. if our data only contains $u$ corresponding to the same $(s, a)$, then $\eta_i$ (for iteration $i$) would be $\inv{1 + i}$ (we start at $i=0$).}.
	
	\item \textbf{Stochastic gradient} \tstamp{42:23}:
	\begin{align}
		\hat Q_{\pi}(s, a) 
			&\leftarrow \hat Q_{\pi}(s, a) - \eta \left[
				\underbrace{ \hat Q_{\pi}(s, a) }_\text{prediction}  - 
				\underbrace{ u }_\text{target}
			\right]
	\end{align}
	where the implicit objective is least squares regression $(Q - u)^2$. 
\end{compactitem}

\begin{algorithm}[Bootstrapping methods: SARSA \tstamp{50:20}]
	Motivation: the values of $u$ we observe in the data often have high variance\footnote{This is because we are defining $u$ as the discounted sum of rewards from whenever $(s, a)$ first happened \textit{until the end of the episode}. A lot of random stuff can happen in that sequence of actions.}. 
	
	\tcblower 
	
	On each $(s, a, r, s', a')$:
	\begin{align}
		\hat Q_{\pi}(s, a)
			&\leftarrow (1 - \eta) \mred{ \hat Q_{\pi}(s, a) }
				+ \eta \mgreen{ \left[
					\underbrace{ r }_\text{data} + 
					\gamma \underbrace{ \hat Q_{\pi}(s', a') }_\text{estimate}
				\right] }
	\end{align}
	
	Comparison with model-free MC (colors are unrelated to colors in eq above):
	\begin{compactitem}
		\item \red{biased}\footnote{TODO: should probably know how to prove this.}
		\item \green{small variance}
		\item \green{immediate updates} (don't have to wait until end of episode). 
	\end{compactitem}
\end{algorithm}

\begin{algorithm}[Bootstrapping methods: Q-Learning \tstamp{59:50}]
	On each $(s, a, r, s')$:
	\begin{align}
		\hat Q_{opt}(s, a)
			&\leftarrow (1- \eta) \underbrace{ \mred{  \hat Q_{opt}(s, a)    }   }_\text{prediction}
			+ \eta \underbrace{   \mgreen{  \left(
				r + \gamma \hat V_{opt}(s')
			\right) } }_\text{target} \\
		\hat V_{opt}(s')
			&= \max_{a' \in Actions(s')} \hat Q_{opt}(s', a')
	\end{align}
\end{algorithm}

\myspace
\blue{Covering the unknown}. Note that we still have not talked about how to decide what actions to take. There are a few approaches:

\begin{align}
	\mtgreen{[Exploit Only]}& \qquad
	\pi_{exploit}(s) := \argmax_{a \in Actions(s)} \hat Q_{opt}(s, a)
	\\
	\mtgreen{[Explore Only]}& \qquad
	\pi_{explore}(s) := \text{GetRandom}(Actions(s))
	\\ 
	\mtgreen{[Epsilon-Greedy]}& \qquad
	\pi_{eps} := \begin{cases}
		\pi_{exploit} & \text{probability } 1 - \epsilon \\
		\pi_{explore} & \text{probability} \epsilon 
	\end{cases}
\end{align}
\begin{compactitem}
	\item Exploit Only: $\hat Q(s, a)$ inaccurate, too greedy. 
	\item Explore Only: average utility is low because exploration is not guided. Our Q-values are actually quite good, though.
\end{compactitem}

Another problem is \textbf{generalization} \tstamp{1:07:00}, which is particularly relevant when our state space is \textit{large}. Also note that we've not yet taken advantage of similarities between states, we've basically been treating states as individual black boxes. We can't generalize to unseen states/actions. 

\begin{algorithm}[Q-learning with function approximation]
	Define features $\vec \phi(s, a)$ and weights $\vec w$. 
	\begin{align}
		\hat Q_{opt}(s, a; \vec w) := \vec w \dotp \vec \phi(s, a)
	\end{align}
	
		On each $(s, a, r, s')$:
	\begin{align}
	\vec w
	&\leftarrow \vec w - \eta \left[ \underbrace{ \mred{  \hat Q_{opt}(s, a)    }   }_\text{prediction}
	- \underbrace{   \mgreen{  \left(
			r + \gamma \hat V_{opt}(s')
			\right) } }_\text{target} \right]  \vec \phi(s, a)
	\end{align}
	with implied objective $(\hat Q - (r + \gamma \hat V))^2$. 
	
\end{algorithm}












% ------------------------------------------------------------------------------
\lecture{Lectures}{Games I}{April 30, 2019}


Code for the $\lfloor \tfrac{N}{2} \rfloor$ game at \tstamp{13:00}. 

\begin{itemdefinition}{Two-Player Zero-Sum Game}
		\item $s_{start}$: starting state
		\item $\text{Actions}(s)$
		\item $\text{Succ}(s, a) \rightarrow s'$
		\item $\text{IsEnd}(s)$
		\item $\text{Utility}(s)$: agent's utility for \red{end state}\footnote{\red{TODO}: So all non-terminal states have zero utility? Answer: No, there is no utility for non-terminal states. It's just not a thing.} $s$
		\item $\text{Player}(s)$: player who controls state $s$
\end{itemdefinition}

\begin{itemdefinition}{Policies of Player $p$}
	\item \textbf{Deterministic}: $\pi_p(s) \in \text{Actions}(s)$
	\item \textbf{Stochastic}: $\pi_p(s, a) \in [0, 1]$
\end{itemdefinition}




\bluesec{Evaluation} \tstamp{22:00}.  Let $V_{eval}(s)$ denote the expected value of state $s$. We also say that it's a recurrence for expected utility.
\graybox{
	V_{eval}(s)
		&= \begin{cases}
			\text{Utility}(s) & \text{if IsEnd}(s) \\
			\sum_{a \in \text{Actions}(s)} \pi_{\text{Player}(s)}(s, a) V_{eval}(\text{Succ}(s, a)) & \text{otherwise}
		\end{cases}\label{eq:v-eval-1}
}
Around \tstamp{26:00}, she introduces the idea of $V_{expmax}$ (expectimax\footnote{It seems that we say "expectimax" when we assume our opponent plays \sout{randomly} deterministically, and "minimax" when we assume our opponent is trying to minimize our utility.}) that uses $\max_{a \in \text{Actions}(s)}$ in the above equation, which corresponds to a player that always maximizes their expected utility. Note, however, that there is no need to clutter our brains\footnote{We can, however, clutter this footer with them\textellipsis \tstamp{29:31}
	\begin{align}
		V_{minmax}(s)
	&= \begin{cases}
		\text{Utility}(s) & \text{if IsEnd}(s) \\
		\mred{ \max_{a \in \text{Actions}(s)} }  V_{minmax}(\text{Succ}(s, a)) & \text{Player(s)} {=} agent \\
		\mblue{ \min_{a \in \text{Actions}(s)} }  V_{minmax}(\text{Succ}(s, a)) & \text{Player(s)} {=} opp 
	\end{cases}
	\end{align}
} with new definitions, since this is still the same definition as above with a player that has $\pi(s, a)$ equal to 1 for the action that maximizes expected utility, and zero elsewhere. 



\begin{drawing}
	% Root.
	\node[node max] (Root) {};
	
	% Branches.
	\node[node min, below=2em of A, xshift=-6em] (L1Left) {};
	\node[node min, below=2em of A] (L1Center) {};
	\node[node min, below=2em of A, xshift=6em] (L1Right) {};
	
	% Leafs.
	\node[node leaf, below=2em of L1Left, xshift=-1em] (LeafLeftLeft) {-50};
	\node[node leaf, below=2em of L1Left, xshift=1em] (LeafLeftRight) {50};
	
	\node[node leaf, below=2em of L1Center, xshift=-1em] (LeafCenterLeft) {1};
	\node[node leaf, below=2em of L1Center, xshift=1em] (LeafCenterRight) {3};
	
	\node[node leaf, below=2em of L1Right, xshift=-1em] (LeafRightLeft) {-5};
	\node[node leaf, below=2em of L1Right, xshift=1em] (LeafRightRight) {15};
	
	
	\draw[draw=gray]
	(Root) edge[-] node{} (L1Left)
	(Root) edge[-] node{} (L1Center)
	(Root) edge[-] node{} (L1Right)
	(L1Left) edge[-] node{} (LeafLeftLeft)
	(L1Left) edge[-] node{} (LeafLeftRight)
	(L1Center) edge[-] node{} (LeafCenterLeft)
	(L1Center) edge[-] node{} (LeafCenterRight)
	(L1Right) edge[-] node{} (LeafRightLeft)
	(L1Right) edge[-] node{} (LeafRightRight)
	;
\end{drawing}


\textbf{Minimax Property 1} \tstamp{41:49}. If opponent is minimizing, agent should maximize.
\graybox{
	V\left( 
		\mred{\pi_{agent}},
		\mblue{\pi_{min}}
	 \right) 
		&\ge V\left(   
			\mred{\pi_{agent}},
			\mblue{\pi_{min}}
		\right)
		\quad
		\forall \mred{\pi_{agent}}
}
However, if e.g. your opponent plays randomly, then agent should should use expectimax (lesson: maximizing not \textit{always} optimal). 


\bluesec{Computation}. 
\begin{compactitem}
	\item \textbf{Tree Search}: $\mathcal O(d)$ space, $\mathcal O(b^{2d})$ time\footnote{She defines $d$ really confusingly (\tstamp{52:00}), but wikipedia says $d$ should be the number of moves a given player makes, while plies (2d here since 2 players) is the total number of moves made.}. 
\end{compactitem}



\bluesec{Evaluation Functions} \tstamp{53:25}. We can use \green{Depth-limited [tree] search}: stop at some maximum depth $d_{max}$. This can be defined by making a slight modification to eq. \ref{eq:v-eval-1}:
\graybox{
	V_{minmax}(s, \mred{d})
		&= \begin{cases}
			\text{Utility}(s) & \text{if IsEnd}(s) \\
			\mtred{Eval}\mred{(s)} & \mtred{d = 0} \\
			\max_{a \in \text{Actions}(s)}   V_{minmax}(\text{Succ}(s, a), \mred{d}  ) & \text{Player(s)} {=} agent \\
			\min_{a \in \text{Actions}(s)}  V_{minmax}(\text{Succ}(s, a), \mred{d - 1}) & \text{Player(s)} {=} opp 
		\end{cases}\label{eq:v-eval-2}
}
where $\text{Eval}(s)$ is an estimate\footnote{No guarantees on error of estimate, unlike A*.} of $V_{minimax}(s)$. For example, in chess we could define $\text{Eval}(s) := sum(\text{material, mobility, king-safety})$. Pacman code example at \tstamp{57:28}. 

\bluesec{Alpha-Beta Pruning} \tstamp{59:19}. \textbf{Key Idea}: branch and bound. Maintain lower and upper bounds on values. If intervals don't overlap, we can choose optimally without considering the non-optimal interval choices. Specifically, when exploring tree, we can keep track of\textellipsis
\begin{compactitem}
	\item $\mblue{a_s}$: lower bound on value of max node $s$ (we know value is \textit{at least} ($\geq$) $a_s$)
	
	\item $\mgreen{b_s}$: upper bound on value of min node $s$ (we know value is \textit{at most} ($\leq$) $b_s$). 
	
	% \preceq means "Precedes or Not Equal To"
	\item $\alpha_s \triangleq \max_{s' \preceq s} a_s$, where the max is over ancestors of $s$. 
	
	\item $\beta_s \triangleq \min_{s' \preceq s} b_s$. 
\end{compactitem}
which clearly only makes sense in a minimax context\footnote{\red{TODO}: think about situations where we aren't doing minimax and want to prune. This was an exam question in CS188.}. Detailed walkthrough example at \tstamp{1:05:00}. The values of $a_s$ and $b_s$ can really be thought of locally, in the sense that they are determined by either a max ($a_s$) or min ($b_s$) over the values presented to them by their immediate successors (values are propagated ``up'' the tree when we are computing these quantities). \\

Clearly, \textbf{ordering} (when traversing/considering states) makes a big difference with pruning. In practice, ordering by $\text{Eval}(s)$ can be a good strategy:
\begin{compactitem}
	\item Max nodes: order successors by decreasing $\text{Eval}(s)$.
	\item Min nodes: order successors by increasing $\text{Eval}(s)$. 
\end{compactitem}

\newpage
% -----------------
\subsub{Adversarial Search (AIMA Ch. 5)}
% -----------------

First, I think it's crucial to understand the game tree in depth. Below is a 2-ply game tree\footnote{This tree has depth 1, with 2 plys.}, which translates for game problems to 2-player game tree. The leaf nodes show the \textit{utility values as perceived by the root node}. \textbf{IMPORTANT}: the only reason it is valid for us to focus on the utility of the root is because this is a \textit{two-player zero-sum game} (the MIN utilities are defined as the negation of the MAX utilities). If this were a multiplayer game, we'd have to track a \textit{vector} of utilities for each player. 

\begin{drawing}
	% Root.
	\node[node max] (Root) {A};
	
	% Branches.
	\node[node min, below=of Root, xshift=-6em] (L1Left) {B};
	\node[node min, below=2em of Root] (L1Center) {C};
	\node[node min, below=2em of Root, xshift=6em] (L1Right) {D};
	
	% Leafs.
	\node[node leaf, below=of L1Left, xshift=-1em] (LeafLeftLeft) {-50};
	\node[node leaf, below=of L1Left, xshift=1em] (LeafLeftRight) {50};
	
	\node[node leaf, below=of L1Center, xshift=-1em] (LeafCenterLeft) {1};
	\node[node leaf, below=2em of L1Center, xshift=1em] (LeafCenterRight) {3};
	
	\node[node leaf, below=of L1Right, xshift=-1em] (LeafRightLeft) {-5};
	\node[node leaf, below=of L1Right, xshift=1em] (LeafRightRight) {15};
	
	\edge {Root} 			{L1Left, L1Center, L1Right};
	\edge {L1Left} 			{LeafLeftLeft, LeafLeftRight};
	\edge {L1Center} 	{LeafCenterLeft, LeafCenterRight};
	\edge {L1Right}		  {LeafRightLeft, LeafRightRight};
\end{drawing}


Define the \green{minimax value} $\mgreen{V_{minmax}(s)}$ of state $s$ to be the utility \footnote{as perceived by root} of being in state $s$, \textit{assuming both players play optimally} from there to the end of the game.

\begin{align}
	V_{minmax}(s)
	&= \begin{cases}
	\text{Utility}(s) & \text{if IsEnd}(s) \\
	\max_{a \in \text{Actions}(s)}   V_{minmax}(\text{Succ}(s, a)) & \text{Player(s)} {=} agent \\
	\min_{a \in \text{Actions}(s)}   V_{minmax}(\text{Succ}(s, a)) & \text{Player(s)} {=} opp 
	\end{cases}
\end{align}

\newpage
\begin{lstlisting}[language=Python]
def minimax_decision(root: State) -> Action:
	"""We assume the root is a MAX node."""
	best_action = None
	best_value = -float("inf")
	for action in root.actions():
		value = min_value(succ(root, action))
		if value > best_value:
			best_value = value
			best_action = action
	return best_action
	
def min_value(state: State) -> float:
	"""Returns V_minmax from perspective of a MIN node."""
	assert isMinNode(state)
	if isEnd(state):
		return state.utility()
	v = float("inf")
	for action in state.actions():
		v = min(v, max_value(succ(state, action)))
	return v
	
def max_value(state: State) -> float:
	"""Returns V_minmax from perspective of a MAX node."""
	assert isMaxNode(state)
	if isEnd(state):
		return state.utility()
	v = -float("inf")
	for action in state.actions():
		v = max(v, min_value(succ(state, action)))
	return v
\end{lstlisting}




\bluesec{Alpha-Beta Pruning} (5.3). General principle:
\begin{compactitem}
	\item Consider some node $n$ somewhere in the tree, such that $agent$ has a choice of moving to that node (i.e. $n$ is an \textit{immediate descendent} of an $agent$ (MAX) node\footnote{This also implies (for a two-player game) that $n$ is either a MIN node or a leaf.}). 
\end{compactitem}

Conceptual algorithm for the two-ply tree we've been using.
\begin{compactenum}
	\item Make your way down the left-most path of the tree until you hit the MIN node just before the leafs.
	\begin{compactenum}
		\item Collect each leaf utility. You can update the interval of possible values that the parent MIN node can have as you go, but it doesn't seem like we really do anything with those intermediate values (\red{right}?).
		\item You now know that the given MIN node will choose the smallest value. Let's call that value $m_1$ (the first min node value we've determined). 
	\end{compactenum}
	
	\item The parent of MIN (which is the root MAX node for our tree) sees that MIN has set its value/action choice as $m_1$. This means the root's value will be \textit{at least} $m_1$. 
\end{compactenum}

Key points to remember:
\begin{compactitem}
	\item When we speak about ``ranges'' or intervals, we are talking about the range of possible values $V(s)$ could be, given the information we've collected thus far.
\end{compactitem}





% ------------------------------------------------------------------------------
\lecture{Lectures}{Games II}{May 2, 2019}



\bluesec{Review: Evaluation Function}. New idea: can we \textit{learn} the eval function? We can redefine $\text{Eval}(s) = V(s; \vec w)$. Some possible models might be a linear model, a simple neural network:
\begin{align}
	V(s; \vec w) &= \vec w \dotp \phi(s) \\
	V(s; \vec w, \vec[1:k]{v}) &= \sum_{j=1}^k w_j \sigma (\vec[j]{v} \dotp \phi(s))
\end{align}


\bluesec{Temporal Difference Learning} (TD-Learning). In the learning scenario, we treat our estimate of $V$ as the \textit{prediction}, and the value of $r + \gamma V(s')$ as the \textit{label}. We then define an objective and perform gradient descent. If we use MSE objective, then TD-learning is defined as performing, for each $(s, a, r, s')$ \tstamp{25:00}.  
\graybox{
	\vec w
		&\leftarrow \vec w - \eta \left[
			V(s; \vec w) - (r + \gamma V(s'; \vec w))
		\right] \nabla_{\vec w} V(s; \vec w)
}

\myfig[0.5\textwidth]{figs/games2_td_vs_q.png}

\begin{table}{l l}
	Q-Learning & TD Learning \\ \midrule\midrule
	$\hat Q_{opt}(s, a; \vec w)$ & $\hat V_{\pi}(s; \vec w)$ \\ \midrule
	Off-policy & On-policy \\ \midrule
	Don't need $T(s, a, s')$ & Need $\text{Succ}(s, a)$ \\
\end{table}



\begin{example}[Two-Finger Morra \tstamp{40:00}]
	Players \red{A} and \blue{B} each show 1 or 2 fingers.
	\begin{compactitem}
		\item If both show 1, \blue{B} gives \red{A} 2 dollars. 
		\item If both show 2, \blue{B} gives \red{A} 4 dollars. 
		\item Otherwise, \red{A} gives \blue{B} 3 dollars
	\end{compactitem}
\end{example}


\myspace
\begin{itemdefinition}{Single-Move Simultaneous Game \tstamp{42:00}}
	\item Players = $\{A, B\}$
	\item $V(a, b)$: \red{A's utility} if A chooses action $a$, B chooses $b$. We call $V$ the \green{payoff matrix}.
	\item Evaluation: the value of the game if $A$ follows $\pi_A$ and $B$ follows $\pi_B$:
	\graybox{
		V(\pi_A, \pi_B)
			&= \sum_{a, b} \pi_A(a) \pi_B(b) V(a, b)
	}
where we seem to be using a horrifying overload of notation. In the above, $\pi: a \mapsto [0, 1]$. 
\end{itemdefinition}
Here we'll consider \green{pure strategies} (a single action) and \green{mixed strategies} (probability distribution of action).  Let's pretend for a moment that the players \textit{do} move sequentially (non-simultaneous). For \underline{\textbf{\textit{pure strategies}}}, who should go first?\footnote{It is crucial to remember that $V(a, b)$ is \textit{defined} to be the utility from player $A$'s perspective. In light of this, the proposition should not be surprising at all.} Proposition \tstamp{52:34}: going second is preferable to going first (pure strategies only). 
\begin{align}
	\underbrace{ \max_a \min_b V(a, b) }_\text{A goes 1st} &\le 
	\underbrace{ \min_b \max_a V(a, b) }_\text{A goes 2nd}
\end{align}
Note that the above is not specific to two-finger Morra. What if $A$ is playing a mixed strategy [and tells $B$ what it is]? Proposition: $B$ should always choose a pure strategy. For any mixed strategy $\pi_A$, \tstamp{56:56}
\begin{align}
	\min_{\pi_B} V(\pi_A, \pi_B)
\end{align}
can be attained by a pure strategy. \\

\Needspace{13\baselineskip}
What if $A$ only tells us that they're using some mixed strategy $\pi_A := [p, 1-p]$ (still on two-finger Morra example)?\marginnote{$A$ plays mixed; pretend $A$ goes first} Let's also pretend that player $A$ is going first. We should enumerate the values for all possible choices of $B$.
\begin{align}
	\mtgreen{[B plays 1]}\qquad&
		p \cdot (2) + (1 - p) \cdot (-3) = 5p - 3 \\
	\mtgreen{[B plays 2]}\qquad& 
		p \cdot (-3) + (1 - p) \cdot (4) = -7p + 4
\end{align}
remember that the numerical values (2, -3, 4) above are still from the perspective of $A$ (negative is good for $B$). $B$ will want to choose action $b \in \text{Actions}(b)$ such that
$$
	b = \argmin\{ 5p {-} 3,  -7p {+} 4  \}
$$
Notice that both options are linear functions of $p$, and that whatever the value of $p$, it is always the same for both of the linear functions\footnote{There is only one $p$. It is ``shared''. Don't overthink it.}. If we plot these linear functions, we can find the possible line segments (rays, technically) where this minimum lies\footnote{Red marker area at \tstamp{1:00:47}}. Since $B$ is trying to minimize, we (re: player $A$) know that $B$ will choose the smallest of these two options. Therefore, if player $A$ is going first (and wants to \textit{maximize}), while knowing that $B$ will minimize, it should choose $p$ that results in the largest value \textit{in the minimum region} of our two linear functions -- \textit{this turns out to be the point where the functions intersect}. Therefore, we can compute the optimal choice of $p$ for player $A$ by setting both linear equations equal to each other and solving for $p$. \\

How does the aforementioned analysis change if we pretend $B$ goes first \tstamp{1:02:00}?\marginnote{$A$ plays mixed; pretend $B$ goes first}[2em]























% ==================================================================================
% R E V I E W
% ==================================================================================
\mysection{Review}\label{Review}

\lecture{Review}{Discrete Math and Probability}{April 07, 2019}

\p \blue{Sequences and Summations}. 
\graybox{
	\sum_{k=0}^{n} a r^k ~ (r {\ne} 0) 
	&= \frac{ a r^{n + 1} - a }{ r - 1 }, ~ r \ne 1 \\
	\sum_{k = 1}^{\infty} k x^{k -1}, ~ |x|{<}1
	&= \inv{(1-x)^2}
}

\p \blue{Recurrence Relations}. 

\begin{example}[Bit Strings of Length Five]
	\question{Find number of bit strings of length $n$, denoted as $a_n$, that do NOT have two consecutive 0s.}
	
	\tcblower 
	
	Suggested thought process:
	\begin{compactenum}
		\item I can probably define $a_n$ as a \green{recurrence relation}. 
		\item I can think about this problem in terms of whether the last bit is a 1 or a 0:
		\begin{compactenum}
			\item Last bit is 1: there are $a_{n - 1}$ such bit strings satisfying the question. 
			\item Last bit is a 0: well then the n-2 bit can't be a zero, since that would violate the question. Therefore, there are $a_{n -2}$ such bit strings. 
		\end{compactenum}
		\item $\therefore a_n = a_{n - 1} + a_{n - 2}$. 
	\end{compactenum}
	
	\purple{Themes}:
	\begin{compactitem}
		\item Splitting the problem into two sets corresponding to 0 and 1 somehow.
		\item Using the \green{sum rule}. 
	\end{compactitem}
	
\end{example}


\p \blue{Patterns/Themes}.
\begin{compactitem}
	\item \textbf{Bit Strings}. 
	\begin{compactitem}
		\item Think in terms of cases: (1) the last bit is one, and (2) the last bit is zero. 
	\end{compactitem}
	
	\item \textbf{Number of times until something happens}.
	\begin{compactitem}
		\item Probably follows a \green{geometric distribution}: 
		\begin{align}
		p(X{=}k) 
		&= (1-p)^{k - 1} p \quad \text{for} \quad k = 1, 2, 3, \ldots \\
		\E{X} 
		&= \inv{p}
		\end{align}
	\end{compactitem}
\end{compactitem}





\lecture{Review}{Course Synthesis}{May 04, 2019}

A lot of the course topics have strange overlap and overloaded notation. Let's simplify.


\begin{itemdefinition}{Markov Decision Process}
	\item \textbf{Definition}:
	\begin{compactitem}[\green{\ding{224}}]
		\item $\text{States} \triangleq  \text{Set}[\text{State}]$
		\begin{compactitem}
			\item $s_{start} \in \text{States}$.
			\item $\text{IsEnd}(s)$
		\end{compactitem}
		\item $\text{Actions}(s) \mapsto \text{Set}[\text{Action}]$
		\item $T(s, a, s') \mapsto [0, 1]$
		\item $\text{Reward}(s, a, s')$
		\item $0 \le \gamma \le 1$. 
	\end{compactitem}

	\item \textbf{Representation}: graph with state nodes $s$ and chance nodes $c$.
	\begin{compactitem}[\green{\ding{224}}]
		\item $s{\rightarrow}c$ edges are actions.
		\item $c{\rightarrow}s$ edges are random outcomes. 
	\end{compactitem} 

	\item \textbf{Solution}: a policy $\pi$. 
	
	\item \textbf{Evaluation} of a policy $\pi$: 
	\begin{align}
		U(s_{1}, a_1, s_2, \ldots, a_{T - 1}, s_{T})
			&\triangleq  \sum_{t=1}^{T - 1} \gamma^{t - 1} R(s_t, a_t, s_{t+1}) \\
		V_{\pi}(s)
			&\triangleq \begin{cases}
			0 & \text{if IsEnd(s)} \\
			Q_{\pi}(s, \pi(s)) & \text{otherwise}
			\end{cases} \\
		Q_{\pi}(s, a)
			&\triangleq \sum_{s'} T(s, a, s')\left[
			R(s, a, s') + \gamma V_\pi(s')
			\right] 
	\end{align}
\end{itemdefinition}

Now we need some way of \textit{solving} the MDP: finding the optimal policy $\pi_{opt}$ that maximizes our expected utility.


\begin{itemdefinition}{Solving MDPs}
	\item Value Iteration: converges to correct answer ($\pi_{opt}$), provided that either $\gamma < 1$ or MDP graph acyclic. 
\end{itemdefinition}

\begin{algorithm}[Value Iteration]
	\begin{compactenum}
		\item Initialize $\V[0]{s} \leftarrow 0$ for all states $s$. 
		
		\item For each iteration $t = 1, \ldots, t_{VI}$ and for each state $s$:
		\graybox{
			V_{opt}^{(t)}(s)
			&\leftarrow  
			\mred{\max_{a \in \text{Actions(s)}   } }
			\underbrace{ \sum_{s'} T(s, a, s') \left[ 
				R(s, a, s') + \gamma V_{opt}^{(t-1)}(s')
				\right] }_{Q_{opt}^{(t-1)}(s, a) }
		}
	\end{compactenum}
	
	We want to choose $t_{PE}$ such that 
	\begin{align}
	\max_{s \in \text{States}} \left|
	\V{s} - \V[t -1]{s}
	\right| \le \epsilon 
	\end{align}
\end{algorithm}


\begin{algorithm}[Q-learning with function approximation]
	Define features $\vec \phi(s, a)$ and weights $\vec w$. 
	\begin{align}
	\hat Q_{opt}(s, a; \vec w) := \vec w \dotp \vec \phi(s, a)
	\end{align}
	
	On each $(s, a, r, s')$:
	\begin{align}
	\vec w
	&\leftarrow \vec w - \eta \left[ \underbrace{ \mred{  \hat Q_{opt}(s, a)    }   }_\text{prediction}
	- \underbrace{   \mgreen{  \left(
			r + \gamma \hat V_{opt}(s')
			\right) } }_\text{target} \right]  \vec \phi(s, a)
	\end{align}
	with implied objective $(\hat Q - (r + \gamma \hat V))^2$. 
	
\end{algorithm}






% ==================================================================================
% LEARNING FROM MISTAKES
% ==================================================================================
\mysection{Learning from Mistakes}\label{Learning from Mistakes}



% ------------------------------------------------------------------------------
\lecture{Learning from Mistakes}{Homework 1: Foundations}{April 21, 2019}

\myspace
\p \blue{Problem 1: Optimization and Probability}. 
\begin{compactitem}
	\item[(a)] \red{Mistake}: I didn't show the 2nd derivative was positive. \green{Lesson}: Always check 2nd deriv is positive when finding a minimum. 

	\item [(a)] \red{Mistake}: my interpretation of the case where $w_i$ not guaranteed positive was incorrect (?) \green{Lesson}: No idea (TODO: figure out why I was wrong). 
	
	\item [(b)] \red{Mistake}: I got it right, but my approach was way more complicated than theirs. They just pulled the $\max$ to the left of the summation and refactored both $f$ and $g$ to maximize over $s_1, \ldots, s_d$ and used a more elegant proof by intuition. I basically brute-forced it. \green{Lesson}: get more comfortable moving around summations with maxes, and understand that when comparing two quantities (like $f$ and $g$), you'll make it easier on yourself if you get them in the same form. 
	
	\item [(c)] \red{Mistake}: I got it right but, again, their solution was way simpler. Apparently, MDPs are relevant for solving these their way. They just used a simple expectation recursion relation, and I went full-on infinite series. \green{Lesson}: I need to get way more comfortable with recursive expectation value problems. 
\end{compactitem}








\begin{comment}

% ==================================================================================
% Final Project
% ==================================================================================
\mysection{Final Project}\label{Final Project}

% ------------------------------------------------------------------------------
\lecture{Final Project}{Guidelines}{April 20, 2019}

Condensed notes from \href{http://web.stanford.edu/class/cs221/project.html}{CS221 Final Project Guidelines}. 

\p \blue{Parts}.
\begin{compactitem}
\item \textbf{Task Definition}.

\item \textbf{Infrastructure}.

\item \textbf{Approach}. 

\item \textbf{Literature Review}. 

\item \textbf{Error Analysis}. 
\end{compactitem}

\myspace
\p \blue{Milestones}. 
\begin{compactitem}
\item \textbf{Proposal} (10\% points) (2 pages max). 
\end{compactitem}

\end{comment}

















\end{document}
