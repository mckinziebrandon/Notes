% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
% P  R  E  A  M  B  L  E
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =
\documentclass[11pt]{article}
\usepackage{amsbsy, amsmath, amssymb, authblk}

%\usepackage{array} 
%\usepackage{algorithm2e}

\usepackage{booktabs, bm}
\usepackage[small,labelfont=bf,up,singlelinecheck=false]{caption}
\usepackage{cancel}
\usepackage{comment}
%\usepackage{fancyhdr}
%\usepackage[default]{lato}
\usepackage[T1]{fontenc}
\usepackage[bottom]{footmisc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage[utf8]{inputenc}
%	\inputencoding{latin1}
%	\inputencoding{utf8}
%\usepackage{lettrine}
%\usepackage[sc]{mathpazo}
\usepackage{lmodern} % Nice fonts?
%\usepackage{mathrsfs}
\usepackage{mathtools} 
%\usepackage{marvosym} % silly bullet-point symbols (misc symbols)
%\usepackage{microtype}
\usepackage{minitoc}         % left in case it is needed elsewhere
\setcounter{secttocdepth}{5} % idem
\usepackage{etoc} % for toc before each section.
%\usepackage{multicol}
\usepackage{needspace}
\usepackage{paralist}
%\usepackage{polynom} 			% typesetting polynomial long division
%\usepackage{setspace}
%	\onehalfspacing 
\usepackage{tocloft}
\usepackage{xparse} % DeclareDocumentCommand
\usepackage[compact]{titlesec} 		% compact shrinks whitespace around section headings.
\usepackage{ulem} 				% for strikeout \sout command.
%\usepackage{verbatim}

% Muh packagez :)
\usepackage{../../Packages/MathCommands}
\usepackage{../../Packages/BrandonColors}
\usepackage{../../Packages/BrandonBoxes}
\usepackage{../../Packages/NoteTaker}
\usepackage{../../Packages/CS221}
%\usepackage{../Packages/MachineLearningUtils}


%\usepackage{program}
% DL BOOK CONVENTIONS
\renewcommand\vec[2][]{\bm{#2}_{#1}}

\DeclareDocumentCommand{\slice}
	{ O{t} O{1} m }
	{\vec[\langle #2 \ldots #1 \rangle]{#3}}

\newcommand\myfig[2][0.3\textwidth]{\begin{figure}[h!]\centering\includegraphics[width=#1]{#2}\end{figure}}
\newcommand\myspace[1][]{\vspace{#1\bigskipamount}}
\newcommand\p{\Needspace{10\baselineskip} \noindent}
\newcommand\tlab[1]{\tag{#1}\label{#1}}
\newcommand\Var[1]{\mathrm{Var}\left[#1\right]}


%\usepackage{program}

\usepackage{layout} % Type \layout() anywhere to see values of layout frame.
%\usepackage{showframe} % Displays layout frame on all pages
\usepackage{marginnote}
\renewcommand*{\marginfont}{\scriptsize}

\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\usepackage{tikz}
\usetikzlibrary{arrows, automata, shapes, snakes, positioning}
\usetikzlibrary{bayesnet}


\titleformat*{\subsubsection}{\small\scshape}
\newcommand\subsub[1]{\Needspace{15\baselineskip}\hrule\subsubsection{#1}\hrule}
\newcommand\matgrad[2]{\nabla_{\mathbf{#2}} #1}

% O{T} means "optional with default value of `T`"
% m means mandatory argument
\DeclareDocumentCommand{\vecseq}
	{ O{n} m }
	{ \{  \vec[1]{#2}, \ldots, \vec[#1]{#2}   \}  }
\DeclareDocumentCommand{\seq}
	{ O{n} m }
	{ \{ #2_1, \ldots #2_#1 \} }
\DeclareDocumentCommand{\dotseq}
	{ O{n} m }
	{ #2_1, \ldots #2_#1 }
	
\newcommand\QA[2]{\item \red{Q}: #1
	\begin{compactitem}
		\item \green{A}: #2
	\end{compactitem}}
	
\newcommand\myref[1]{\purple{[#1]}}

\definecolor{forgeblue}{HTML}{018C9F}
% Gray table borders
\makeatletter
\def\rulecolor#1#{\CT@arc{#1}}
\def\CT@arc#1#2{%
	\ifdim\baselineskip=\z@\noalign\fi
	{\gdef\CT@arc@{\color#1{#2}}}}
\let\CT@arc@\relax
\rulecolor{forgeblue}
\makeatother

%\setlength{\parskip}{1pt}
%\setlength{\columnseprule}{0.1pt}
%\setlength{\columnsep}{0.6cm}
%\setlength\tabcolsep{0.1cm}
\renewcommand{\arraystretch}{1.2}

\makeatletter
\newcommand*\dotp{\mathpalette\dotp@{.5}}
\newcommand*\dotp@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

% Title
\title{\vspace{-10mm}\fontsize{24pt}{8pt}\selectfont\textbf{Homework 7: Car Tracking}\vspace*{-4mm}}
% Author
\author{Brandon McKinzie}
% Date
\date{}

% --------------------------------------------------------------
% --------------------------------------------------------------



\renewcommand\cftsecfont{\small\bfseries}
\renewcommand\cftsubsecfont{\scriptsize}
\renewcommand\cftsubsubsecfont{\scriptsize}

\renewcommand\cftsecafterpnum{\vskip-5pt}
\renewcommand\cftsubsecafterpnum{\vskip-7pt}
\renewcommand\cftsubsubsecafterpnum{\vskip-7pt}

\begin{document}

\maketitle

\begin{comment}
Setup:
\begin{compactitem}
	\item We want to drive our car from start to finish (green box). 
	\item World is 2D grid with your car + K others. At each timestep t, you get noisy estimate of dist to other cars. 
	\item Variables: Assume we are only concerned with one other car.
	\begin{compactitem}
		\item $C_t \in \R^2$: actual location of the other car. Unobserved.
		\item $a_t \in \R^2$: your car's position. Observed and controlled by us.
		\item $D_t \sim \mathcal N (||a_t - C_t||, \sigma^2)$
	\end{compactitem}
	\item Goal: Compute $P(C_t \mid D_1, \ldots, D_t)$. 
\end{compactitem}
\end{comment}

\section*{Problem 1: Bayesian Network Basics}

\textbf{(a)} \question{Suppose we have a sensor reading for the second timestep, $D_2 = 0$. Compute the posterior distribution $\mathbb P(C_2 = 1 \mid D_2 = 0)$.}


Below is the Bayesian network, where we've observed $D_2=0$:
\begin{drawing}
	\node[blight] (h1) {$C_1$};
	\node[bquery, right=of h1] (h2) {$C_2$};
	\node[blight, right=of h2] (h3) {$C_3$};
	
	\node[blight, below=of h1] (e1) {$D_1$};
	\node[bdark, right=of e1] (e2) {$0$};
	\node[blight, right=of e2] (e3) {$D_3$};
	
	\diredge {h1} {e1, h2};
	\diredge {h2} {e2, h3};
	\diredge {h3} {e3};
\end{drawing}



\begin{align}
	\Prob{C_2=1 \mid D_2 = 0}
		&\propto \Prob{C_2=1, D_2=0} \\
		&= \sum_{c_1} \Prob{C_2=1, D_2=0, c_1} \\
		&= \sum_{c_1} \Prob{c_1} \Prob{C_2=1 \mid c_1} \Prob{D_2=0 \mid C_2=1} \\
		&= 0.5 \sum_{c_1} \Prob{C_2=1 \mid c_1} \Prob{D_2=0 \mid C_2=1} \\
		&= 0.5 \eta \sum_{c_1}  \Prob{C_2=1 \mid c_1}\\
		&= 0.5 \eta (\epsilon + (1 - \epsilon)) \\
		&=0.5 \eta \\
	\Prob{D_2  = 0}
		&= \sum_{c_2} \Prob{D_2=0, c_2} \\
		&= \sum_{c_2} \Prob{D_2=0 \mid c_2} \sum_{c_1} \Prob{c_2 \mid c_1} \Prob{c_1} \\
		&= (1 - \eta) \cdot \left( % d_2=0, c_2=0
			 		0.5 (1 - \epsilon) % c_1=0
			 		+ 0.5 \epsilon
		\right) + 
		\eta \cdot \left( % d_2=0, c_2=1
			0.5 \epsilon + 0.5 ( 1 - \epsilon)
		\right) \\
		&= 0.5 \\
	\therefore \Prob{C_2=1 \mid D_2=0} &= \frac{\Prob{C_2=1, D_2=0}}{\Prob{D_2=0}} = \eta 
\end{align}


\clearpage

\textbf{(b)} \question{Compute $\mathbb P(C_2=1 \mid D_2=0, D_3=1)$}

Now our Bayesian network looks like:

\begin{drawing}
	\node[blight] (h1) {$C_1$};
	\node[bquery, right=of h1] (h2) {$C_2$};
	\node[blight, right=of h2] (h3) {$C_3$};
	
	\node[blight, below=of h1] (e1) {$D_1$};
	\node[bdark, right=of e1] (e2) {$0$};
	\node[bdark, right=of e2] (e3) {$1$};
	
	\diredge {h1} {e1, h2};
	\diredge {h2} {e2, h3};
	\diredge {h3} {e3};
\end{drawing}

\begin{align}
	p(C_2, D_2, D_3) 
		&= \sum_{c_1} \sum_{c_3} p(c_1) p(c_2 \mid c_1) p(d_2 \mid c_2) p(c_3 \mid c_2) p(d_3 \mid c_3) \\
		&= 0.5 \sum_{c_1} \sum_{c_3} p(c_2 \mid c_1)  p(d_2 \mid c_2) p(c_3 \mid c_2) p(d_3 \mid c_3) \\
		&= 0.5 \cdot p(d_2 \mid c_2)  \sum_{c_3}  p(c_3 \mid c_2) p(d_3 \mid c_3) \\
	p(C_2=1, D_2=0, D_3=1)
		&= 0.5 \eta \left(
			\epsilon \eta + (1 - \epsilon) (1 - \eta)
		\right) \\
	p(D_2=0, D_3=1)
		&= \sum_{c_2} p(c_2, D_2=0, D_3=1) \\
		&= 0.5 \eta \left(
		\epsilon \eta + (1 - \epsilon) (1 - \eta)
		\right) + 0.5 (1 - \eta) \left(
			(1 - \epsilon) \eta + \epsilon (1 - \eta)
		\right)\\
	p(C_2=1 \mid D_2=0, D_3=1)
		&= \frac{0.5 \eta \left(
			\epsilon \eta + (1 - \epsilon) (1 - \eta)
			\right)}{0.5 \eta \left(
		\epsilon \eta + (1 - \epsilon) (1 - \eta)
	\right) + 0.5 (1 - \eta) \left(
(1 - \epsilon) \eta + \epsilon (1 - \eta)
\right)} \\
		&= \frac{\epsilon \eta^2 + \eta (1  - \epsilon) (1 - \eta)
		}{
			\epsilon \eta^2 +
			+2 (1-\eta)(1- \epsilon) \eta + \epsilon(1 - \eta)^2 
		}
\end{align}


\clearpage
\textbf{(c)}

\begin{itemize}
	\item[i.] 
	
	\begin{align}
		P(C_2=1 \mid D_2=0)
			&= \eta = 0.2 \\
		P(C_2 = 1 \mid D_2=0, D_3=1)
			&= \frac{
					(0.1) (0.2)^2 + 0.2 (0.9)(0.8)
			}{
			(0.1) (0.2)^2 + 2 (0.8)(0.9)(0.2) + (0.1)(0.8)^2
		}\\
	&\approx 0.4157
	\end{align}
	
	\item[ii.] The second reading ($D_3=1$) makes it more likely that $C_2=1$. Informally, after we observe $D_3=1$ while realizing that it's unlikely the car has actually moved (relative to the probability that our sensor was wrong), it becomes more probable that our previous reading of $D_2=0$ was just a bad reading. 
	
	
	\item[iii.] We can compute the value of $\epsilon$ by equating the two formulas and solving:
	\begin{align}
	\eta &= \frac{\epsilon \eta^2 + \eta (1  - \epsilon) (1 - \eta)
	}{
		\epsilon \eta^2 +
		+2 (1-\eta)(1- \epsilon) \eta + \epsilon(1 - \eta)^2 
	}\\
	1 &=  \frac{\epsilon \eta + (1  - \epsilon) (1 - \eta)
	}{
		\epsilon \eta^2 +
		+2 (1-\eta)(1- \epsilon) \eta + \epsilon(1 - \eta)^2 
	} \\
	 \epsilon \eta + (1  - \epsilon) (1 - \eta)
	&= \epsilon \eta^2 +
	+2 (1-\eta)(1- \epsilon) \eta + \epsilon(1 - \eta)^2 
	\\
	\epsilon\eta 
		&=  \epsilon \eta^2 +
		+ (2\eta - 1) (1-\eta)(1- \epsilon) + \epsilon(1 - \eta)^2 \\
	\epsilon (\eta - \eta^2 - (1-\eta)^2)
		&= (2\eta - 1) (1-\eta)(1- \epsilon) \\
	\epsilon( (1 - \eta) (\eta - (1 - \eta))  )
		&= (2\eta - 1) (1-\eta)(1- \epsilon) \\
		\epsilon (1 - \eta) (2\eta - 1)
		&= (2\eta - 1) (1-\eta)(1- \epsilon) \\
		\epsilon &= 1 - \epsilon \\
		\epsilon &= \frac{1}{2}
	\end{align}
	Intuitively, we'd have to set $\epsilon = \tfrac{1}{2}$ (car equally likely to be in 0 or 1 independent of previous location) since that would make additional observations essentially useless. 
\end{itemize}


\clearpage
\section*{Problem 5: Which car is it?}

\question{
	\begin{compactitem}
		\item $C_{ti} \in \R^2$: location of $i$th car at time $t$, where $1 \le i \le K$, and $1 \le t \le T$. 
		\item $D_t = \{D_{t1}, \ldots, D_{tK}\}$, where each $D_{ti} \in \R$ is noisy distance measurement of $i$th car at time $t$. 
	\end{compactitem}
}

\textbf{(a)} \question{Write an expression for $\Prob{C_{11}, C_{12} \mid E_1=e_1}$ as a function of $\mathcal{N}(v; \mu, \sigma^2)$ and the priors $p(c_{11})$ and $p(c_{12})$.}

 Since $T=1$, I'm going to drop the time index in the following calculations to avoid confusing myself. Then, since $K=2$, I'll denote $E_1 \equiv E \equiv (E_{1}, E_{2})$ (again, I've dropped the time index since it is always 1 for this problem). So we need an expression for $\Prob{C_{1}, C_{1} \mid E_{1}, E_{2}}$. \\
 
 First, we can rewrite
 \begin{align}
 	\Prob{C_{1}, C_{1} \mid E_{1}, E_{2}}
 		&\propto \Prob{E_{1}, E_{2} \mid C_{1}, C_{2}} \Prob{C_1, C_2} \\
 		&=  \Prob{E_{1}, E_{2} \mid C_{1}, C_{2}} \Prob{C_1} \Prob{C_2}
 \end{align}
 where we've taken advantage of the independence of the two cars. Now we need to break down $\Prob{E_{1}, E_{2} \mid C_1, C_2}$. Since $E$ is sampled uniformly at random from the possible permutations in $D$, we can think of a given instantiation $E = (e_1, e_2)$ as an event whose probability is proportional to the union of all possible readings $(d_1, d_2)$ that could result in $(e_1, e_2)$. Formally,
 \begin{small}
 \begin{align}
 	\Prob{E_1=e_1, E_2=e_2 \mid C_1, C_2} 
 		&\propto \Prob{D_1 = e_{1}, D_2 = e_2 \mid C_1, C_2} + \Prob{D_1=e_2, D_2=e_1 \mid C_1, C_2} \\
 		&= \Prob{D_1=e_1 \mid C_1}\Prob{D_2=e_2 \mid C_2} + \Prob{D_1=e_2 \mid C_1} \Prob{D_2=e_1 \mid C_2}
 \end{align}
 \end{small}
 where again I've used the independence of the two cars in the final line. We can replace the $D_i\mid C_i$ conditionals with the provided normal distribution to get the final expression (with time index included). Let $\mathcal{N}_{C_{ti}}(x) = \mathcal{N}(x; ||a_t - C_{ti}||; \sigma^2)$ (for brevity's sake).
 
 \begin{small}
 \graybox{
	\Prob{C_{11}, C_{12} \mid E_1 = e_1}
		&\propto \Prob{C_{11}}\Prob{C_{12}}\big[ 
			\mathcal{N}_{C_{11}}(e_{11}) \mathcal{N}_{C_{12}}(e_{12})  
		  +\mathcal{N}_{C_{11}}(e_{12})  \mathcal{N}_{C_{12}}(e_{11}) \big]
}
\end{small}



\clearpage
\textbf{(b)} \question{Assuming the prior $p(c_{1i})$ is the same for all i, show that the number of assignments for all K cars $(c_{11}, \dots, c_{1K})$ that obtain the maximum value of $\mathbb P(C_{11} = c_{11}, \dots, C_{1K} = c_{1K} \mid E_1 = e_1)$ is at least $K!$.}


Intuitively, by introducing the constraint that $p(c_{1i})$ is the same for all $i$, combined with the fact that we don't know which car each element of $E_1$ corresponds to, we can take any of the assignments that maximize $\Prob{C_{11}, \ldots, C_{1K} \mid E_1}$ and permute the locations of the cars without changing the probability. Since there are $K!$ such permutations, there are at least $K!$ assignments of the cars that obtain the maximum value. 





\clearpage
\textbf{(c)} \question{ For general K, what is the treewidth corresponding to the posterior distribution over all K car locations at all T time steps conditioned on all the sensor readings:
$$\mathbb P(C_{11} = c_{11}, \dots, C_{1K} = c_{1K}, \dots, C_{T1} = c_{T1}, \dots, C_{TK} = c_{TK} \mid E_1 = e_1, \dots, E_T = e_T)$$}


Given the factor graph, we'll first have to condition on the evidence $e_1, \ldots, e_T$. This will create factors $f_t$ for all timesteps $1 \le t \le T$, each with arity $K$. Since we want a variable ordering that starts with variables with the least amount of neighbors, we'll order along timesteps (from 1 to T). Since the Markov blanket of each variable we eliminate along timesteps has $K$ variables (corresponding to the $K-1$ other cars plus the transition), the resultant factors will have arities no larger than $K$. By definition, then, the treewidth is $K$. 






\clearpage
\textbf{(d)} \question{Now suppose you change your sensors so that at each time step t, they return the list of exact positions of the K cars, but shifted (with wrap around) by a random amount. For example, if the true car positions at time step 1 are $c_{11} = 1, c_{12} = 3, c_{13} = 8, c_{14} = 5$, then $e_1$ would be $[1, 3, 8, 5], [3, 8, 5, 1], [8, 5, 1, 3]$, or $[5, 1, 3, 8$, each with probability 1/4. Describe an efficient algorithm for computing $p(c_{ti} \mid e_1, \dots, e_T)$ for any time step t and car i. Your algorithm should not be exponential in K or T.}
















\end{document}